{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca20106c-728f-4313-be15-a53125ec675f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1758b9810064a7b9bd5a399013a4bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import re\n",
    "\n",
    "# Replace these with your actual model names or paths\n",
    "base_model_name = 'meta-llama/Meta-Llama-3-8B'      # e.g., 'gpt2'\n",
    "lora_model_name = 'fine-tuned-llama-lora'          # Path to your LoRA model\n",
    "jsonl_file_path = 'qa.train.synthetic.jsonl'       # Path to your input JSONL file\n",
    "\n",
    "# Configure 8-bit loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,    # Adjust as needed\n",
    "    llm_int8_has_fp16_weight=False,\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load the base model in 8-bit with device map for offloading\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',  # Automatically offloads layers to CPU/GPU\n",
    ")\n",
    "\n",
    "# Load the LoRA model on top of the base model\n",
    "model = PeftModel.from_pretrained(model, lora_model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ece8f51-bd99-4fe8-bbf4-565a80229be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_output(context, question, initial_output):\n",
    "    # Combine context, question, and initial output for the prompt\n",
    "    refinement_prompt = (\n",
    "        f\"Context: {context}\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Initial Answer: {initial_output}\\n\\n\"\n",
    "        \"Please refine the initial answer to better answer the question given the context. If the answer is incorrect then correct it. Only output the refined answer. \\nRefined answer:\"\n",
    "    )\n",
    "\n",
    "    # Prepare the input for the model\n",
    "    inputs = tokenizer(refinement_prompt, return_tensors='pt', truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate the refined output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            pad_token_id=tokenizer.eos_token_id \n",
    "        )\n",
    "\n",
    "    # Decode the output\n",
    "    refined_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return refined_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ff9a73-a312-44b4-b22c-2a3ffae764ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSONL lines: 100%|██████████| 2000/2000 [4:21:16<00:00,  7.84s/it]  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_jsonl_file(jsonl_file_path, output_jsonl_path):\n",
    "    refined_data = []\n",
    "    \n",
    "    with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in tqdm(lines, desc=\"Processing JSONL lines\"):\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        # Extract only the last context and question before the \"output\" field\n",
    "        fields = list(data.keys())\n",
    "        context, question = \"\", \"\"\n",
    "\n",
    "        # Extracting 'output' field details\n",
    "        output_field = data.get('output', '')\n",
    "\n",
    "        # Find the context and question in the input field\n",
    "        input_field = data.get('input', '')\n",
    "        matches = re.findall(r\"Context:(.*?)\\nQuestion:(.*?)\\n\", input_field, re.DOTALL)\n",
    "\n",
    "        # Extract the last context and question if matches found\n",
    "        if matches:\n",
    "            context, question = matches[-1]\n",
    "            context = context.strip()\n",
    "            question = question.strip()\n",
    "\n",
    "        # Refine the output\n",
    "        refined_output = refine_output(context.strip(), question.strip(), output_field.strip())\n",
    "\n",
    "        # Save the refined data\n",
    "        refined_data.append({\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"answer\": refined_output\n",
    "        })\n",
    "\n",
    "    # Write the refined data to a new JSONL file\n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as out_f:\n",
    "        for item in refined_data:\n",
    "            out_f.write(json.dumps(item) + '\\n')\n",
    "# Note: Replace 'jsonl_file_path' with the path to your actual JSONL file when running this function.\n",
    "# Example usage: process_jsonl_file('qa.train.synthetic.jsonl')\n",
    "process_jsonl_file(jsonl_file_path, \"refinedssr.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5f2ec2-029f-47bc-9b0d-d13c004e8cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSONL lines: 100%|██████████| 2000/2000 [00:00<00:00, 160926.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_jsonl_file(jsonl_file_path, output_jsonl_path):\n",
    "    refined_data_format= []\n",
    "    \n",
    "    # Read input JSONL file\n",
    "    with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in tqdm(lines, desc=\"Processing JSONL lines\"):\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        # Extract the \"context\", \"question\", and \"answer\" fields\n",
    "        context = data.get(\"context\", \"\").strip()\n",
    "        question = data.get(\"question\", \"\").strip()\n",
    "        answer = data.get(\"answer\", \"\").strip()\n",
    "\n",
    "        # Extract the refined answer from the \"answer\" field\n",
    "        match = re.search(r\"Refined answer: (.*)\", answer, re.DOTALL)\n",
    "        refined_answer = match.group(1).strip() if match else \"\"\n",
    "\n",
    "        # Save the refined data\n",
    "        refined_data_format.append({\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"refined_answer\": refined_answer\n",
    "        })\n",
    "\n",
    "    # Write the refined data to a new JSONL file\n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as out_f:\n",
    "        for item in refined_data_format:\n",
    "            out_f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Replace these paths with the actual file paths\n",
    "input_jsonl_path = \"refinedssr.jsonl\"  # Input JSONL path\n",
    "output_jsonl_path = \"refined_answers.jsonl\"    # Output JSONL path\n",
    "\n",
    "# Process the file\n",
    "process_jsonl_file(input_jsonl_path, output_jsonl_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d6265c3-c81c-4bcd-bc39-d244e931e6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0f6a0c3b7944c8aae8c9b14c126a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5892b618d0404c1fb65dff228905d492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a4552434364da4a6898717ae44fcb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4feafe32b05480baa13f625cc1fd978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dad1ee707d1480a96dad8bc60cfa886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f10125eccf2404197512d7a9ef505c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4498fb0944b4d7a8803a07f0e73166b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f87a90562e1430291c138b91d62447e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8694d21a34b41fb9615953977f8a290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638b4544c47b4c749a3aa069817a89cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7165fb163a442ea0c6606252eff6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 32/32 [00:01<00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled data saved to final_sampled.jsonl\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from datasets import utils\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def generate_embeddings(data, model_name='all-MiniLM-L6-v2', batch_size=64):\n",
    "    # Load the sentence transformer model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Combine context and question for embeddings\n",
    "    texts = [f\"{task['context']} {task['question']}\" for task in data]\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_numpy=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def cluster_and_sample_data(data, embeddings, sample_memory=200, n_clusters=20):\n",
    "    \"\"\"\n",
    "    Clusters data using K-means and samples from clusters proportionally.\n",
    "    \"\"\"\n",
    "    # Normalize embeddings\n",
    "    embeddings_norm = embeddings / np.linalg.norm(embeddings, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Perform k-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=0)\n",
    "    labels = kmeans.fit_predict(embeddings_norm)\n",
    "    \n",
    "    # Compute distances to cluster centers\n",
    "    centric_distances = np.linalg.norm(embeddings_norm - kmeans.cluster_centers_[labels], axis=1)\n",
    "    \n",
    "    # Count instances in each cluster\n",
    "    n_cluster_instances = np.bincount(labels, minlength=n_clusters)\n",
    "    \n",
    "    # Determine number of samples per cluster proportionally\n",
    "    total_instances = len(data)\n",
    "    clu_sample_num = [max(1, round(sample_memory * count / total_instances)) for count in n_cluster_instances]\n",
    "    \n",
    "    # Sample data points closest to cluster centers\n",
    "    sampled_indices = []\n",
    "    for clu_idx in range(n_clusters):\n",
    "        cluster_indices = np.where(labels == clu_idx)[0]\n",
    "        cluster_distances = centric_distances[cluster_indices]\n",
    "        num_samples = min(clu_sample_num[clu_idx], len(cluster_indices))\n",
    "        if num_samples > 0:\n",
    "            # Get indices of closest points\n",
    "            closest_indices = cluster_indices[np.argsort(cluster_distances)[:num_samples]]\n",
    "            sampled_indices.extend(closest_indices)\n",
    "    \n",
    "    # Get sampled data\n",
    "    sampled_data = [data[i] for i in sampled_indices]\n",
    "    \n",
    "    return sampled_data\n",
    "\n",
    "\n",
    "def save_sampled_data(sampled_data, output_path):\n",
    "    \"\"\"\n",
    "    Save sampled data to a JSONL file.\n",
    "    \"\"\"\n",
    "    with jsonlines.open(output_path, mode=\"w\") as writer:\n",
    "        writer.write_all(sampled_data)\n",
    "    print(f\"Sampled data saved to {output_path}\")\n",
    "\n",
    "\n",
    "# Paths\n",
    "refined_data_path = \"refined_answers.jsonl\"  # Replace with your refined synthetic JSONL path\n",
    "output_sampled_path = \"final_sampled.jsonl\"    # Path to save the sampled output\n",
    "\n",
    "# Parameters\n",
    "sample_memory = 200  # Adjust based on your requirement\n",
    "n_clusters = 20      # Number of clusters for K-means\n",
    "embedding_model_name = 'all-MiniLM-L6-v2'  # Model for embeddings\n",
    "\n",
    "# Step 1: Load refined data\n",
    "with jsonlines.open(refined_data_path, mode=\"r\") as reader:\n",
    "    refined_data = [item for item in reader]\n",
    "\n",
    "# Step 2: Generate embeddings\n",
    "embeddings = generate_embeddings(refined_data, model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 3: Cluster and sample data\n",
    "sampled_data = cluster_and_sample_data(refined_data, embeddings, sample_memory=sample_memory, n_clusters=n_clusters)\n",
    "\n",
    "# Step 4: Save the sampled data\n",
    "save_sampled_data(sampled_data, output_sampled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc0c363-13ce-42fd-997d-8d32709c0147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (0.15.1)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.31.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: accelerate in /home/vtyag/.local/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: filelock in /home/vtyag/.local/lib/python3.11/site-packages (from huggingface-hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/vtyag/.local/lib/python3.11/site-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/vtyag/.local/lib/python3.11/site-packages (from huggingface-hub) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vtyag/.local/lib/python3.11/site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vtyag/.local/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vtyag/.local/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/vtyag/.local/lib/python3.11/site-packages (from sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (1.11.3)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (10.1.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: sympy in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/vtyag/.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub) (2023.7.22)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vtyag/.local/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vtyag/.local/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.6/447.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.15.1\n",
      "    Uninstalling huggingface-hub-0.15.1:\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.31.0\n",
      "    Uninstalling transformers-4.31.0:\n",
      "      Successfully uninstalled transformers-4.31.0\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 2.2.2\n",
      "    Uninstalling sentence-transformers-2.2.2:\n",
      "      Successfully uninstalled sentence-transformers-2.2.2\n",
      "Successfully installed huggingface-hub-0.26.3 sentence-transformers-3.3.1 tokenizers-0.21.0 transformers-4.47.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade huggingface-hub transformers sentence-transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bef803-0814-469a-97aa-b199089353a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
