{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49dd23af-f4c5-46f1-8511-02d235503ac8",
   "metadata": {},
   "source": [
    "### QA EVCL (QA Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e4a3ccd-6f64-4fbd-bfc6-f28c1478118a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538fa84b540740938defbdf0b3a9ae5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions incrementally:\n",
      "Generating predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/tmp/ipykernel_929/1212530307.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/32 [00:06<03:20,  6.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 2/32 [00:12<03:13,  6.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 3/32 [00:19<03:05,  6.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 4/32 [00:25<02:56,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 5/32 [00:31<02:50,  6.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 6/32 [00:38<02:44,  6.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 7/32 [00:44<02:37,  6.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 8/32 [00:50<02:29,  6.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 9/32 [00:57<02:27,  6.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 10/32 [01:04<02:25,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 11/32 [01:10<02:16,  6.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 12/32 [01:16<02:07,  6.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 13/32 [01:23<02:01,  6.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 14/32 [01:29<01:54,  6.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 15/32 [01:35<01:47,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 16/32 [01:41<01:41,  6.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 17/32 [01:48<01:34,  6.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 18/32 [01:54<01:28,  6.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 19/32 [02:00<01:22,  6.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 20/32 [02:07<01:15,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 21/32 [02:13<01:09,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 22/32 [02:19<01:02,  6.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 23/32 [02:26<00:56,  6.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 24/32 [02:32<00:50,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 25/32 [02:38<00:44,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 26/32 [02:44<00:37,  6.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 27/32 [02:51<00:31,  6.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 28/32 [02:57<00:25,  6.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 29/32 [03:04<00:19,  6.36s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 30/32 [03:10<00:12,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 31/32 [03:16<00:06,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sample 496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [03:21<00:00,  6.31s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load the JSON file\n",
    "os.chdir('/home/pranav24/cs-546-project')\n",
    "file_path = \"/home/pranav24/cs-546-project/SSR/Latest_Weights/QA_Weights/task024_cosmosqa_answer_generation.json\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "instruction=\"\"\"\\nInstruction: Craft one correct answer to the question given in input. Be straight to the point and do not give incorrect answers. Only output the answer without restating the question or context. Be consistent with the context and use common sense.\"\"\"\n",
    "final_instruction=\"Now, ensure to only give the answer and not restate the question or context. \\nAnswer:\"\n",
    "# Extract input-output pairs from JSON\n",
    "instances = data[\"Instances\"][4500:5000]\n",
    "test_inputs = [instruction+instance[\"input\"]+final_instruction for instance in instances]\n",
    "test_outputs = [instance[\"output\"][0] for instance in instances]\n",
    "\n",
    "# print(test_inputs[499])\n",
    "# print()\n",
    "# print(test_outputs[499])\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer setup\n",
    "base_model_path = \"meta-llama/Meta-Llama-3-8B\"  # Replace with actual model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# Check if tokenizer has a padding token, if not, set the eos_token as padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16  # Adjust as needed\n",
    "\n",
    "\n",
    "# Define the model and load weights\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "fine_tuned_weights_path = \"/home/pranav24/cs-546-project/finetuned-weights-LoRA-EVCL-Correct-Task1_VCL_best\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, fine_tuned_weights_path)\n",
    "pyro.get_param_store().load('pyro_param_store_task1_evcl_best.pt')\n",
    "\n",
    "# Ensure compatibility with the unchanged part of the code\n",
    "DEVICE = model.device\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "references = []\n",
    "sampled_weights_log = []  # Store sampled weights\n",
    "\n",
    "output_file_path = \"/home/pranav24/cs-546-project/predictions_EVCL_Task1_FINAL_QA.json\"\n",
    "\n",
    "if os.path.exists(output_file_path):\n",
    "    with open(output_file_path, \"r\") as f:\n",
    "        saved_data = json.load(f)\n",
    "else:\n",
    "    saved_data = {\"predictions\": [], \"references\": []}\n",
    "\n",
    "print(\"Generating predictions incrementally:\")\n",
    "\n",
    "print(\"Generating predictions:\")\n",
    "\n",
    "for i in tqdm(range(0, len(test_inputs), batch_size)):  # Loop in batches\n",
    "    print(f'At sample {i}')\n",
    "    batch_inputs = test_inputs[i:i + batch_size]\n",
    "    batch_references = test_outputs[i:i + batch_size]\n",
    "\n",
    "    # Tokenize the inputs in a batch\n",
    "    inputs_tokenized = tokenizer(batch_inputs, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Apply Pyro parameters to LoRA layers\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, \"lora_A\"):\n",
    "                    for key in module.lora_A:\n",
    "                        loc = pyro.param(f\"{name}.lora_A.{key}_loc\")\n",
    "                        scale = pyro.param(f\"{name}.lora_A.{key}_scale\")\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            f\"{name}.lora_A.{key}\",\n",
    "                            dist.Normal(loc, scale).to_event(loc.dim())\n",
    "                        )\n",
    "                        sampled_weights_log.append(\n",
    "                            (name, key, sampled_weight.clone().cpu().numpy())\n",
    "                        )\n",
    "                        module.lora_A[key].weight.data.copy_(sampled_weight)\n",
    "\n",
    "                if hasattr(module, \"lora_B\"):\n",
    "                    for key in module.lora_B:\n",
    "                        loc = pyro.param(f\"{name}.lora_B.{key}_loc\")\n",
    "                        scale = pyro.param(f\"{name}.lora_B.{key}_scale\")\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            f\"{name}.lora_B.{key}\",\n",
    "                            dist.Normal(loc, scale).to_event(loc.dim())\n",
    "                        )\n",
    "                        sampled_weights_log.append(\n",
    "                            (name, key, sampled_weight.clone().cpu().numpy())\n",
    "                        )\n",
    "                        module.lora_B[key].weight.data.copy_(sampled_weight)\n",
    "\n",
    "            # Generate predictions using the tokenized inputs\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=inputs_tokenized[\"input_ids\"],\n",
    "                attention_mask=inputs_tokenized[\"attention_mask\"],\n",
    "                max_new_tokens=30,  \n",
    "                min_length=10,  \n",
    "                no_repeat_ngram_size=2,  \n",
    "                num_return_sequences=1,\n",
    "                top_p=0.9,  \n",
    "                temperature=0.7  \n",
    "            )\n",
    "\n",
    "        # Decode generated IDs\n",
    "        batch_predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        predictions.extend(batch_predictions)\n",
    "        references.extend(batch_references)\n",
    "\n",
    "        saved_data[\"predictions\"].extend(batch_predictions)\n",
    "        saved_data[\"references\"].extend(batch_references)\n",
    "\n",
    "        # Save incrementally to JSON\n",
    "        with open(output_file_path, \"w\") as json_file:\n",
    "            json.dump(saved_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c93f87da-0ee7-470b-bd90-7bacaf6f618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c436983-bfc6-4087-a81b-ae9be081e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROUGE Scores:\n",
      "{'rouge1': np.float64(0.052607719254781), 'rouge2': np.float64(0.012274753261103251), 'rougeL': np.float64(0.041798541501681344), 'rougeLsum': np.float64(0.0469195144838407)}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nROUGE Scores:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18506375-d37d-4505-a2a9-53d5a59d5557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5347b21-a494-4d16-b79e-e3aa9b67853d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba04cc4-b4ce-406c-8c1c-124ea28811f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce4f60-1e37-46f8-b8f1-97689a200325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ad314-0e60-45ec-a132-10cb07d17567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18350071-3a39-4188-a07a-4fdd75bf7ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71e3b0-95ed-4d14-8573-14da6139b4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a11663-8464-43b5-ae26-86f9ab14307c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3dc1b165-6ffb-425b-9370-9b616c36934b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eedae4f46dc43deb0b69043f3e64332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/tmp/ipykernel_929/1617821873.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.99s/it]\n"
     ]
    }
   ],
   "source": [
    "# test_inputs=[\"Context: Then I drove all around to the used DVD stores to get season 2. ( which I found for 20 bucks! ) I have been watching a disk a night, and not going to bed until 3 am because I knew that the new season was starting soon and I didn't want to be behind. I just finished season 2 and was on the edge of my seat the whole last episode. Yesterday I taped what was on tv. Question: What does the narrator think about the price of the DVD set purchased?\"]\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_inputs=[\"\\nInstruction: Craft one correct answer to the question given in input. Be straight to the point and do not give incorrect answers. Only output the answer without restating the question or context. Be consistent with the context and use common sense.\\n Context: He would just flash me this amazing smile. the funny thing was i kept getting hit on. A couple people came up to me when i was standing there, a couple friends were flirting with me when i was smoking, and i was talking to this other guy that was dancing. I know i gave him my number, and i think it was right in front of him. Question: Why did the man smile widely at the speaker and proceed to hit on her? Now, ensure to only give the answer\\nAnswer:\"]\n",
    "batch_size=1\n",
    "final_answer=[]\n",
    "\n",
    "base_model_path = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "fine_tuned_weights_path=\"/home/pranav24/cs-546-project/finetuned-weights-LoRA-EVCL-Correct-Task1_VCL_best\"\n",
    "pyro.get_param_store().load('pyro_param_store_task1_evcl_best.pt')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# Check if tokenizer has a padding token, if not, set the eos_token as padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "# pyro.clear_param_store()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, fine_tuned_weights_path)\n",
    "# pyro.get_param_store().load('pyro_param_store_task1_vcl_best.pt')\n",
    "# pyro.get_param_store().load('pyro_param_store_task1_vcl_best.pt')\n",
    "\n",
    "# Ensure compatibility with the unchanged part of the code\n",
    "DEVICE = model.device\n",
    "\n",
    "sampled_weights_log=[]\n",
    "for i in tqdm(range(0, len(test_inputs), batch_size)):  # Loop in batches\n",
    "    batch_inputs = test_inputs[i:i + batch_size]\n",
    "    # batch_references = test_outputs[i:i + batch_size]\n",
    "\n",
    "    # Tokenize the inputs in a batch\n",
    "    inputs_tokenized = tokenizer(batch_inputs, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Apply Pyro parameters to LoRA layers\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, \"lora_A\"):\n",
    "                    for key in module.lora_A:\n",
    "                        loc = pyro.param(f\"{name}.lora_A.{key}_loc\")\n",
    "                        scale = pyro.param(f\"{name}.lora_A.{key}_scale\")\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            f\"{name}.lora_A.{key}\",\n",
    "                            dist.Normal(loc, scale).to_event(loc.dim())\n",
    "                        )\n",
    "                        sampled_weights_log.append(\n",
    "                            (name, key, sampled_weight.clone().cpu().numpy())\n",
    "                        )\n",
    "                        module.lora_A[key].weight.data.copy_(sampled_weight)\n",
    "\n",
    "                if hasattr(module, \"lora_B\"):\n",
    "                    for key in module.lora_B:\n",
    "                        loc = pyro.param(f\"{name}.lora_B.{key}_loc\")\n",
    "                        scale = pyro.param(f\"{name}.lora_B.{key}_scale\")\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            f\"{name}.lora_B.{key}\",\n",
    "                            dist.Normal(loc, scale).to_event(loc.dim())\n",
    "                        )\n",
    "                        sampled_weights_log.append(\n",
    "                            (name, key, sampled_weight.clone().cpu().numpy())\n",
    "                        )\n",
    "                        module.lora_B[key].weight.data.copy_(sampled_weight)\n",
    "\n",
    "            # Generate predictions using the tokenized inputs\n",
    "            # print(sampled_weights_log)\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=inputs_tokenized[\"input_ids\"],\n",
    "                attention_mask=inputs_tokenized[\"attention_mask\"],\n",
    "                # max_length=512,  # Adjust as needed\n",
    "                max_new_tokens=30,  # Restrict to a smaller maximum length\n",
    "                min_length=10,  # Optional: Ensure a minimum length if needed\n",
    "                no_repeat_ngram_size=2,  # Prevent repetitive n-grams\n",
    "                num_return_sequences=1,\n",
    "                top_p=0.9,  # Nucleus sampling: consider only top 90% probability mass\n",
    "                temperature=0.7  # Control randomness (lower is more deterministic)\n",
    "            )\n",
    "\n",
    "        # Decode generated IDs\n",
    "        final_batch_predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        final_answer.extend(final_batch_predictions)\n",
    "        # predictions.extend(batch_predictions)\n",
    "        # references.extend(batch_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e32b06e-f87c-4dee-80a7-fbd0a3c67343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nInstruction: Craft one correct answer to the question given in input. Be straight to the point and do not give incorrect answers. Only output the answer without restating the question or context. Be consistent with the context and use common sense.\\n Context: He would just flash me this amazing smile. the funny thing was i kept getting hit on. A couple people came up to me when i was standing there, a couple friends were flirting with me when i was smoking, and i was talking to this other guy that was dancing. I know i gave him my number, and i think it was right in front of him. Question: Why did the man smile widely at the speaker and proceed to hit on her? Now, ensure to only give the answer\\nAnswer: The man was attracted to her because of her looks and personality. He wanted to get to know her better. \\n']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417f4c6-3d2c-44ea-b68b-5fe63ebdc7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c595385-1091-4c2a-b8a9-bf8d242083d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "534f4391-9b1c-4031-8ee1-bead885f4028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd8fba5373e436aba52ef815898117f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/tmp/ipykernel_929/2780875458.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# test_inputs=[\"Context: Then I drove all around to the used DVD stores to get season 2. ( which I found for 20 bucks! ) I have been watching a disk a night, and not going to bed until 3 am because I knew that the new season was starting soon and I didn't want to be behind. I just finished season 2 and was on the edge of my seat the whole last episode. Yesterday I taped what was on tv. Question: What does the narrator think about the price of the DVD set purchased?\"]\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_inputs=[\"Craft one correct answer to the question given in input. To make it more interesting, try to use non-stereotypical language if possible. Make sure your correct answer is reasonably long, consistent with the context, and requires common sense (instead of explicit extraction from the context.) In your answer, use as few words as possible from the given context. Use a response that is uncommon/non-stereotypical, so that it is less predictable. To be less repetitive, please vary your language for each question: Context: He would just flash me this amazing smile. the funny thing was i kept getting hit on. A couple people came up to me when i was standing there, a couple friends were flirting with me when i was smoking, and i was talking to this other guy that was dancing. I know i gave him my number, and i think it was right in front of him. Question: Why did the man smile widely at the speaker and proceed to hit on her?\"]\n",
    "batch_size=1\n",
    "final_answer=[]\n",
    "\n",
    "base_model_path = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "fine_tuned_weights_path=\"/home/pranav24/cs-546-project/SSR/Latest_Weights/QA_Weights/finetuned-weights/QA_Final\"\n",
    "# pyro.get_param_store().load('pyro_param_store_task1_evcl_best.pt')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# Check if tokenizer has a padding token, if not, set the eos_token as padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "# pyro.clear_param_store()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, fine_tuned_weights_path)\n",
    "# pyro.get_param_store().load('pyro_param_store_task1_vcl_best.pt')\n",
    "# pyro.get_param_store().load('pyro_param_store_task1_vcl_best.pt')\n",
    "\n",
    "# Ensure compatibility with the unchanged part of the code\n",
    "DEVICE = model.device\n",
    "\n",
    "sampled_weights_log=[]\n",
    "for i in tqdm(range(0, len(test_inputs), batch_size)):  # Loop in batches\n",
    "    batch_inputs = test_inputs[i:i + batch_size]\n",
    "    # batch_references = test_outputs[i:i + batch_size]\n",
    "\n",
    "    # Tokenize the inputs in a batch\n",
    "    inputs_tokenized = tokenizer(batch_inputs, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Generate predictions using the tokenized inputs\n",
    "            # print(sampled_weights_log)\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=inputs_tokenized[\"input_ids\"],\n",
    "                attention_mask=inputs_tokenized[\"attention_mask\"],\n",
    "                # max_length=512,  # Adjust as needed\n",
    "                max_new_tokens=30,  # Restrict to a smaller maximum length\n",
    "                min_length=10,  # Optional: Ensure a minimum length if needed\n",
    "                no_repeat_ngram_size=2,  # Prevent repetitive n-grams\n",
    "                num_return_sequences=1,\n",
    "                top_p=0.9,  # Nucleus sampling: consider only top 90% probability mass\n",
    "                temperature=0.7  # Control randomness (lower is more deterministic)\n",
    "            )\n",
    "\n",
    "        # Decode generated IDs\n",
    "        final_batch_predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        final_answer.extend(final_batch_predictions)\n",
    "        # predictions.extend(batch_predictions)\n",
    "        # references.extend(batch_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79d6506d-cf10-4311-9e7a-cd9232820855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Craft one correct answer to the question given in input. To make it more interesting, try to use non-stereotypical language if possible. Make sure your correct answer is reasonably long, consistent with the context, and requires common sense (instead of explicit extraction from the context.) In your answer, use as few words as possible from the given context. Use a response that is uncommon/non-stereotypical, so that it is less predictable. To be less repetitive, please vary your language for each question: Context: He would just flash me this amazing smile. the funny thing was i kept getting hit on. A couple people came up to me when i was standing there, a couple friends were flirting with me when i was smoking, and i was talking to this other guy that was dancing. I know i gave him my number, and i think it was right in front of him. Question: Why did the man smile widely at the speaker and proceed to hit on her? Answer: The man was being flirtatious. \\n']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803c097-99e5-4732-ad86-5057c2794e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_546)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
