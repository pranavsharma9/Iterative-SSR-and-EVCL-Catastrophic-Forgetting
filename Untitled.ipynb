{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39bc387e-8d68-4576-aa67-0add2b962f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from evaluate) (1.24.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from evaluate) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.11/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.9.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.26.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a91862c-bda2-4038-949f-7d66abaf0b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9377a073464f2dac5a45c4cb5f3792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3792e75b24174f33a7680c4d2c210524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1cbf69d91845908674fb4be885343f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/tmp/ipykernel_9238/2559409691.py:102: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|▎         | 1/32 [02:47<1:26:41, 167.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|▋         | 2/32 [05:59<1:30:54, 181.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|▉         | 3/32 [09:04<1:28:33, 183.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 12%|█▎        | 4/32 [12:03<1:24:41, 181.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 16%|█▌        | 5/32 [15:15<1:23:29, 185.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 19%|█▉        | 6/32 [18:45<1:23:53, 193.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 22%|██▏       | 7/32 [21:55<1:20:09, 192.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 25%|██▌       | 8/32 [25:02<1:16:19, 190.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 28%|██▊       | 9/32 [28:04<1:12:07, 188.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|███▏      | 10/32 [31:03<1:07:57, 185.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|███▍      | 11/32 [34:10<1:04:58, 185.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 38%|███▊      | 12/32 [37:06<1:00:54, 182.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 41%|████      | 13/32 [40:13<58:15, 184.00s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 44%|████▍     | 14/32 [43:15<55:04, 183.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 47%|████▋     | 15/32 [46:19<52:01, 183.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 50%|█████     | 16/32 [49:23<48:59, 183.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 53%|█████▎    | 17/32 [52:23<45:39, 182.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 56%|█████▋    | 18/32 [55:28<42:47, 183.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 59%|█████▉    | 19/32 [58:35<39:58, 184.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 62%|██████▎   | 20/32 [1:01:49<37:27, 187.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|██████▌   | 21/32 [1:04:53<34:10, 186.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|██████▉   | 22/32 [1:07:58<30:57, 185.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 72%|███████▏  | 23/32 [1:11:08<28:03, 187.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 75%|███████▌  | 24/32 [1:14:11<24:48, 186.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 78%|███████▊  | 25/32 [1:17:17<21:40, 185.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 81%|████████▏ | 26/32 [1:20:23<18:36, 186.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 84%|████████▍ | 27/32 [1:23:29<15:29, 185.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 88%|████████▊ | 28/32 [1:26:31<12:19, 184.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|█████████ | 29/32 [1:29:44<09:21, 187.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████▍| 30/32 [1:32:47<06:11, 186.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|█████████▋| 31/32 [1:36:04<03:09, 189.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████| 32/32 [1:39:02<00:00, 185.72s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the JSON file\n",
    "file_path = \"/home/pranav24/cs-546-project/SSR/Latest_Weights/QA_Weights/task024_cosmosqa_answer_generation.json\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract input-output pairs from JSON\n",
    "instances = data[\"Instances\"][2500:5000]\n",
    "inputs = [instance[\"input\"] for instance in instances]\n",
    "outputs = [instance[\"output\"][0] for instance in instances]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_inputs, test_inputs, train_outputs, test_outputs = train_test_split(\n",
    "    inputs, outputs, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert data to Hugging Face Dataset format\n",
    "train_ds = Dataset.from_dict({\"input\": train_inputs, \"output\": train_outputs})\n",
    "test_ds = Dataset.from_dict({\"input\": test_inputs, \"output\": test_outputs})\n",
    "\n",
    "# Tokenizer setup\n",
    "base_model_path = \"meta-llama/Meta-Llama-3-8B\"  # Replace with actual model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# Check if tokenizer has a padding token, if not, set the eos_token as padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train_ds = train_ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_test_ds = test_ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "\n",
    "# Convert datasets to PyTorch format\n",
    "tokenized_train_ds.set_format(\"torch\")\n",
    "tokenized_test_ds.set_format(\"torch\")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16  # Adjust as needed\n",
    "train_loader = DataLoader(tokenized_train_ds, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(tokenized_test_ds, batch_size=batch_size)\n",
    "\n",
    "# Define the model and load weights\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "fine_tuned_weights_path = \"/home/pranav24/cs-546-project/finetuned-weights-LoRA-EVCL-Final-Task2_EVCL_best\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, fine_tuned_weights_path)\n",
    "pyro.get_param_store().load('pyro_param_store_task2_vcl_best.pt')\n",
    "\n",
    "# Ensure compatibility with the unchanged part of the code\n",
    "DEVICE = model.device\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "references = []\n",
    "sampled_weights_log = []  # Store sampled weights\n",
    "\n",
    "print(\"Generating predictions:\")\n",
    "\n",
    "for i in tqdm(range(0, len(test_inputs), batch_size)):  # Loop in batches\n",
    "    batch_inputs = test_inputs[i:i + batch_size]\n",
    "    batch_references = test_outputs[i:i + batch_size]\n",
    "\n",
    "    # Tokenize the inputs in a batch\n",
    "    inputs_tokenized = tokenizer(batch_inputs, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Apply Pyro parameters to LoRA layers\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, \"lora_A\"):\n",
    "                    for key in module.lora_A:\n",
    "                        loc = pyro.param(f\"{name}.lora_A.{key}_loc\")\n",
    "                        scale = pyro.param(f\"{name}.lora_A.{key}_scale\")\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            f\"{name}.lora_A.{key}\",\n",
    "                            dist.Normal(loc, scale).to_event(loc.dim())\n",
    "                        )\n",
    "                        sampled_weights_log.append(\n",
    "                            (name, key, sampled_weight.clone().cpu().numpy())\n",
    "                        )\n",
    "                        module.lora_A[key].weight.data.copy_(sampled_weight)\n",
    "\n",
    "                if hasattr(module, \"lora_B\"):\n",
    "                    for key in module.lora_B:\n",
    "                        loc = pyro.param(f\"{name}.lora_B.{key}_loc\")\n",
    "                        scale = pyro.param(f\"{name}.lora_B.{key}_scale\")\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            f\"{name}.lora_B.{key}\",\n",
    "                            dist.Normal(loc, scale).to_event(loc.dim())\n",
    "                        )\n",
    "                        sampled_weights_log.append(\n",
    "                            (name, key, sampled_weight.clone().cpu().numpy())\n",
    "                        )\n",
    "                        module.lora_B[key].weight.data.copy_(sampled_weight)\n",
    "\n",
    "            # Generate predictions using the tokenized inputs\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=inputs_tokenized[\"input_ids\"],\n",
    "                attention_mask=inputs_tokenized[\"attention_mask\"],\n",
    "                max_length=1000,  # Adjust as needed\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True  # Optional: Sampling for diverse generations\n",
    "            )\n",
    "\n",
    "        # Decode generated IDs\n",
    "        batch_predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        predictions.extend(batch_predictions)\n",
    "        references.extend(batch_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34b43ec4-7e84-440c-bb40-c6bb5b271fe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the ROUGE metric\u001b[39;00m\n\u001b[1;32m      4\u001b[0m rouge \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load the ROUGE metric\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Compute ROUGE scores\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nROUGE Scores:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f616f54-5cb0-4ebb-9b05-838e89242508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_546)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
