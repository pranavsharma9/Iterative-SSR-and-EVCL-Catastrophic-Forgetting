{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d9d762a-2a01-4127-b303-b510cef6e435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/vtyag/.local/lib/python3.11/site-packages (4.47.0)\n",
      "Collecting peft\n",
      "  Using cached peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: datasets in /home/vtyag/.local/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: tqdm in /home/vtyag/.local/lib/python3.11/site-packages (4.67.0)\n",
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting rouge\n",
      "  Using cached rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: evaluate in /home/vtyag/.local/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/vtyag/.local/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vtyag/.local/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vtyag/.local/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/vtyag/.local/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vtyag/.local/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/vtyag/.local/lib/python3.11/site-packages (from peft) (2.5.1)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Using cached accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/vtyag/.local/lib/python3.11/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/vtyag/.local/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: xxhash in /home/vtyag/.local/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/vtyag/.local/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /home/vtyag/.local/lib/python3.11/site-packages (from datasets) (3.11.7)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vtyag/.local/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from rouge) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/vtyag/.local/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vtyag/.local/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vtyag/.local/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vtyag/.local/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vtyag/.local/lib/python3.11/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vtyag/.local/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: networkx in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vtyag/.local/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vtyag/.local/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Using cached peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Using cached huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Using cached rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Using cached accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: rouge, huggingface_hub, tokenizers, accelerate, peft\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "outlines 0.0.46 requires pyairports, which is not installed.\n",
      "outlines 0.0.46 requires pydantic>=2.0, which is not installed.\n",
      "vllm 0.6.4.post1 requires openai>=1.45.0, which is not installed.\n",
      "vllm 0.6.4.post1 requires pydantic>=2.9, which is not installed.\n",
      "vllm 0.6.4.post1 requires sentencepiece, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.2.1 huggingface_hub-0.27.0 peft-0.14.0 rouge-1.0.1 tokenizers-0.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft datasets scikit-learn tqdm huggingface_hub rouge evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18966ad6-8e55-422c-b93d-33041a61148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyro-ppl\n",
      "  Using cached pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: numpy>=1.7 in /home/vtyag/.local/lib/python3.11/site-packages (from pyro-ppl) (1.26.4)\n",
      "Collecting opt-einsum>=2.3.2 (from pyro-ppl)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting pyro-api>=0.1.1 (from pyro-ppl)\n",
      "  Using cached pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: torch>=2.0 in /home/vtyag/.local/lib/python3.11/site-packages (from pyro-ppl) (2.5.1)\n",
      "Requirement already satisfied: tqdm>=4.36 in /home/vtyag/.local/lib/python3.11/site-packages (from pyro-ppl) (4.67.0)\n",
      "Requirement already satisfied: filelock in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/vtyag/.local/lib/python3.11/site-packages (from torch>=2.0->pyro-ppl) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vtyag/.local/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0->pyro-ppl) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vtyag/.local/lib/python3.11/site-packages (from jinja2->torch>=2.0->pyro-ppl) (3.0.2)\n",
      "Using cached pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pyro-api, opt-einsum, pyro-ppl\n",
      "Successfully installed opt-einsum-3.4.0 pyro-api-0.1.2 pyro-ppl-1.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyro-ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e3fa0-e786-43b0-a181-d24676e99031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_device(2)  # Set to GPU 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    current_device = torch.cuda.current_device()  # Get the current CUDA device index\n",
    "    device_name = torch.cuda.get_device_name(current_device)  # Get the device name\n",
    "    print(f\"CUDA is currently using device {current_device}: {device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1f45e-10ac-4fea-87fc-31522fc0727c",
   "metadata": {},
   "source": [
    "### QA-SA EVCL (SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f92c362f-4992-43ea-81a8-e6b78023cf5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64082f83b50c4859a20e9f0e75ca6d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions incrementally:\n",
      "Generating predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/tmp/ipykernel_300/1145199426.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|▎         | 1/32 [00:02<01:14,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|▋         | 2/32 [00:04<01:12,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|▉         | 3/32 [00:07<01:06,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 12%|█▎        | 4/32 [00:09<01:01,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 16%|█▌        | 5/32 [00:12<01:09,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 19%|█▉        | 6/32 [00:14<01:03,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 22%|██▏       | 7/32 [00:16<01:00,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 25%|██▌       | 8/32 [00:20<01:03,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 28%|██▊       | 9/32 [00:22<00:59,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|███▏      | 10/32 [00:24<00:53,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|███▍      | 11/32 [00:26<00:48,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 38%|███▊      | 12/32 [00:29<00:51,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 41%|████      | 13/32 [00:32<00:47,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 44%|████▍     | 14/32 [00:34<00:43,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 47%|████▋     | 15/32 [00:36<00:41,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 50%|█████     | 16/32 [00:39<00:40,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 53%|█████▎    | 17/32 [00:41<00:35,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 56%|█████▋    | 18/32 [00:43<00:32,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 59%|█████▉    | 19/32 [00:47<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 62%|██████▎   | 20/32 [00:49<00:29,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|██████▌   | 21/32 [00:51<00:25,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|██████▉   | 22/32 [00:53<00:22,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 72%|███████▏  | 23/32 [00:57<00:24,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 75%|███████▌  | 24/32 [00:59<00:20,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 78%|███████▊  | 25/32 [01:01<00:16,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 81%|████████▏ | 26/32 [01:04<00:16,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 84%|████████▍ | 27/32 [01:07<00:12,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 88%|████████▊ | 28/32 [01:09<00:09,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|█████████ | 29/32 [01:11<00:06,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████▍| 30/32 [01:14<00:05,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|█████████▋| 31/32 [01:16<00:02,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████| 32/32 [01:18<00:00,  2.46s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load the JSON file\n",
    "\n",
    "\n",
    "login(\"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\")\n",
    "# Extract input-output pairs from JSON\n",
    "file_path = \"task1312_amazonreview_polarity_classification.json\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "instruct1=(\n",
    "    \"\\nInstruction: You will be given a sentence describing a review. \"\n",
    "    \"Classify its sentiment as either 'positive' or 'negative'. \"\n",
    "    \"Respond only with the sentiment label ('positive' or 'negative') without any additional information. \"\n",
    "    \"Ensure the output is a one word answer \"\n",
    "    \"\\nReview: \"\n",
    ")\n",
    "instruct2=\"\\nSentiment: \"\n",
    "\n",
    "# Extract input-output pairs from JSON\n",
    "instances = data[\"Instances\"][4500:5000]\n",
    "test_inputs = [instruct1+instance[\"input\"]+instruct2 for instance in instances]\n",
    "test_outputs = [instance[\"output\"][0] for instance in instances]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "\n",
    "# Convert data to Hugging Face Dataset format\n",
    "test_ds = Dataset.from_dict({\"input\": test_inputs, \"output\": test_outputs})\n",
    "\n",
    "# Tokenizer setup\n",
    "base_model_path = \"meta-llama/Meta-Llama-3-8B\"  # Replace with actual model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# Check if tokenizer has a padding token, if not, set the eos_token as padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16  # Adjust as needed\n",
    "\n",
    "# Define the model and load weights\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "fine_tuned_weights_path = \"finetuned_weights\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, fine_tuned_weights_path)\n",
    "pyro.get_param_store().load('pyro_param_store_task2_evcl_best.pt')\n",
    "DEVICE = model.device\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "output_file_path = \"predictions.json\"\n",
    "\n",
    "if os.path.exists(output_file_path):\n",
    "    with open(output_file_path, \"r\") as f:\n",
    "        saved_data = json.load(f)\n",
    "else:\n",
    "    saved_data = {\"predictions\": [], \"references\": []}\n",
    "\n",
    "print(\"Generating predictions incrementally:\")\n",
    "\n",
    "print(\"Generating predictions:\")\n",
    "\n",
    "for i in tqdm(range(0, len(test_inputs), batch_size)):  # Loop in batches\n",
    "    batch_inputs = test_inputs[i:i + batch_size]\n",
    "    batch_references = test_outputs[i:i + batch_size]\n",
    "\n",
    "    # Tokenize the inputs in a batch\n",
    "    inputs_tokenized = tokenizer(batch_inputs, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Apply Pyro parameters to LoRA layers\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, \"lora_A\"):\n",
    "                    for key in module.lora_A:\n",
    "                        loc = pyro.param(f\"{name}.lora_A.{key}_loc\")\n",
    "                        scale = pyro.param(f\"{name}.lora_A.{key}_scale\")\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            f\"{name}.lora_A.{key}\",\n",
    "                            dist.Normal(loc, scale).to_event(loc.dim())\n",
    "                        )\n",
    "            \n",
    "                        module.lora_A[key].weight.data.copy_(sampled_weight)\n",
    "\n",
    "                if hasattr(module, \"lora_B\"):\n",
    "                    for key in module.lora_B:\n",
    "                        loc = pyro.param(f\"{name}.lora_B.{key}_loc\")\n",
    "                        scale = pyro.param(f\"{name}.lora_B.{key}_scale\")\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            f\"{name}.lora_B.{key}\",\n",
    "                            dist.Normal(loc, scale).to_event(loc.dim())\n",
    "                        )\n",
    "                        \n",
    "                        module.lora_B[key].weight.data.copy_(sampled_weight)\n",
    "\n",
    "            # Generate predictions using the tokenized inputs\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=inputs_tokenized[\"input_ids\"],\n",
    "                attention_mask=inputs_tokenized[\"attention_mask\"],\n",
    "                max_new_tokens=10,  \n",
    "                min_length=10,  \n",
    "                no_repeat_ngram_size=2,  \n",
    "                num_return_sequences=1,\n",
    "                top_p=0.9,  \n",
    "                temperature=0.7  \n",
    "            )\n",
    "\n",
    "        # Decode generated IDs\n",
    "        batch_predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        predictions.extend(batch_predictions)\n",
    "        references.extend(batch_references)\n",
    "\n",
    "    saved_data[\"predictions\"].extend(batch_predictions)\n",
    "    saved_data[\"references\"].extend(batch_references)\n",
    "\n",
    "    # Save incrementally to JSON\n",
    "    with open(output_file_path, \"w\") as json_file:\n",
    "        json.dump(saved_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4939cd1-9203-483b-a9a9-f67b438a0c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.5907422799422799, 'rouge2': 0.0, 'rougeL': 0.5908138528138529, 'rougeLsum': 0.5892117604617604}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'sentiment_extraction.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "assert \"Sentiment\" in df.columns, \"Sentiment column is missing in the CSV.\"\n",
    "assert \"Reference\" in df.columns, \"Reference column is missing in the CSV.\"\n",
    "\n",
    "# Load ROUGE metric from evaluate package\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "results = rouge.compute(predictions=df[\"Sentiment\"].tolist(), references=df[\"Reference\"].tolist())\n",
    "\n",
    "print(\"ROUGE Scores:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f9eb5-6ae2-425b-8b5c-d7974c02f47d",
   "metadata": {},
   "source": [
    "### LORA only QA-SA (SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5b8ad0e-b8cf-4e62-aa89-543563dedc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3507fdc6d12241b0a6502200d13e4b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions incrementally:\n",
      "Generating predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|▎         | 1/32 [00:02<01:12,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|▋         | 2/32 [00:05<01:29,  2.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|▉         | 3/32 [00:07<01:13,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 12%|█▎        | 4/32 [00:09<01:04,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 16%|█▌        | 5/32 [00:11<00:59,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 19%|█▉        | 6/32 [00:13<00:54,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 22%|██▏       | 7/32 [00:15<00:52,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 25%|██▌       | 8/32 [00:17<00:50,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 28%|██▊       | 9/32 [00:20<00:55,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|███▏      | 10/32 [00:23<00:50,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|███▍      | 11/32 [00:25<00:46,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 38%|███▊      | 12/32 [00:27<00:43,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 41%|████      | 13/32 [00:29<00:40,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 44%|████▍     | 14/32 [00:31<00:39,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 47%|████▋     | 15/32 [00:34<00:43,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 50%|█████     | 16/32 [00:37<00:39,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 53%|█████▎    | 17/32 [00:39<00:34,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 56%|█████▋    | 18/32 [00:41<00:31,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 59%|█████▉    | 19/32 [00:43<00:28,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 62%|██████▎   | 20/32 [00:45<00:26,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|██████▌   | 21/32 [00:47<00:23,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|██████▉   | 22/32 [00:50<00:23,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 72%|███████▏  | 23/32 [00:52<00:21,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 75%|███████▌  | 24/32 [00:54<00:17,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 78%|███████▊  | 25/32 [00:56<00:15,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 81%|████████▏ | 26/32 [00:58<00:13,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 84%|████████▍ | 27/32 [01:01<00:11,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 88%|████████▊ | 28/32 [01:04<00:09,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|█████████ | 29/32 [01:06<00:06,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████▍| 30/32 [01:08<00:04,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|█████████▋| 31/32 [01:10<00:02,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████| 32/32 [01:11<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load the JSON file\n",
    "\n",
    "login(\"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\")\n",
    "# Extract input-output pairs from JSON\n",
    "file_path = \"task1312_amazonreview_polarity_classification.json\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "instruct1=(\n",
    "    \"\\nInstruction: You will be given a sentence describing a review. \"\n",
    "    \"Classify its sentiment as either 'positive' or 'negative'. \"\n",
    "    \"Respond only with the sentiment label ('positive' or 'negative') without any additional information. \"\n",
    "    \"Ensure the output is a one word answer \"\n",
    "    \"\\nReview: \"\n",
    ")\n",
    "instruct2=\"\\nSentiment: \"\n",
    "\n",
    "# Extract input-output pairs from JSON\n",
    "instances = data[\"Instances\"][4500:5000]\n",
    "test_inputs = [instruct1+instance[\"input\"]+instruct2 for instance in instances]\n",
    "test_outputs = [instance[\"output\"][0] for instance in instances]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "\n",
    "# Convert data to Hugging Face Dataset format\n",
    "test_ds = Dataset.from_dict({\"input\": test_inputs, \"output\": test_outputs})\n",
    "\n",
    "# Tokenizer setup\n",
    "base_model_path = \"meta-llama/Meta-Llama-3-8B\"  # Replace with actual model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# Check if tokenizer has a padding token, if not, set the eos_token as padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16  # Adjust as needed\n",
    "\n",
    "# Define the model and load weights\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "fine_tuned_weights_path = \"finetuned_weights_lora\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, fine_tuned_weights_path)\n",
    "DEVICE = model.device\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "output_file_path = \"predictions_SA.json\"\n",
    "\n",
    "if os.path.exists(output_file_path):\n",
    "    with open(output_file_path, \"r\") as f:\n",
    "        saved_data = json.load(f)\n",
    "else:\n",
    "    saved_data = {\"predictions\": [], \"references\": []}\n",
    "\n",
    "print(\"Generating predictions incrementally:\")\n",
    "\n",
    "print(\"Generating predictions:\")\n",
    "\n",
    "for i in tqdm(range(0, len(test_inputs), batch_size)):  # Loop in batches\n",
    "    batch_inputs = test_inputs[i:i + batch_size]\n",
    "    batch_references = test_outputs[i:i + batch_size]\n",
    "\n",
    "    # Tokenize the inputs in a batch\n",
    "    inputs_tokenized = tokenizer(batch_inputs, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():  # Standard torch.no_grad() without pyro\n",
    "        # Generate predictions using the tokenized inputs\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs_tokenized[\"input_ids\"],\n",
    "            attention_mask=inputs_tokenized[\"attention_mask\"],\n",
    "            max_new_tokens=10,  \n",
    "            min_length=10,  \n",
    "            no_repeat_ngram_size=2,  \n",
    "            num_return_sequences=1,\n",
    "            top_p=0.9,  \n",
    "            temperature=0.7  \n",
    "        )\n",
    "\n",
    "        # Decode generated IDs\n",
    "        batch_predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        predictions.extend(batch_predictions)\n",
    "        references.extend(batch_references)\n",
    "\n",
    "    saved_data[\"predictions\"].extend(batch_predictions)\n",
    "    saved_data[\"references\"].extend(batch_references)\n",
    "\n",
    "    # Save incrementally to JSON\n",
    "    with open(output_file_path, \"w\") as json_file:\n",
    "        json.dump(saved_data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ec9b862-919b-4fc9-a5a6-e160a635e819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.34111904761904766, 'rouge2': 0.0, 'rougeL': 0.34017142857142846, 'rougeLsum': 0.3394904761904761}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'sentiment_references_extraction.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "assert \"Sentiment\" in df.columns, \"Sentiment column is missing in the CSV.\"\n",
    "assert \"Reference\" in df.columns, \"Reference column is missing in the CSV.\"\n",
    "\n",
    "# Load ROUGE metric from evaluate package\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "results = rouge.compute(predictions=df[\"Sentiment\"].tolist(), references=df[\"Reference\"].tolist())\n",
    "\n",
    "print(\"ROUGE Scores:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a913db3-4b29-4caf-b045-64e455b5b2f8",
   "metadata": {},
   "source": [
    "### EVCL QA-SA (QA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f6be116-6d63-4944-95a6-7c1de4d4fe24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df233e5d23c4a28825fd86613c78e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pyro/params/param_store.py:334: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(input_file, map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions incrementally:\n",
      "Generating predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/tmp/ipykernel_300/323514384.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|▎         | 1/32 [00:03<01:48,  3.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|▋         | 2/32 [00:07<01:45,  3.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|▉         | 3/32 [00:10<01:36,  3.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 12%|█▎        | 4/32 [00:13<01:35,  3.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 16%|█▌        | 5/32 [00:17<01:37,  3.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 19%|█▉        | 6/32 [00:20<01:30,  3.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 22%|██▏       | 7/32 [00:24<01:24,  3.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 25%|██▌       | 8/32 [00:27<01:22,  3.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 28%|██▊       | 9/32 [00:31<01:21,  3.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|███▏      | 10/32 [00:34<01:15,  3.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|███▍      | 11/32 [00:37<01:10,  3.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 38%|███▊      | 12/32 [00:41<01:07,  3.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 41%|████      | 13/32 [00:45<01:06,  3.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 44%|████▍     | 14/32 [00:48<01:01,  3.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 47%|████▋     | 15/32 [00:51<00:57,  3.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 50%|█████     | 16/32 [00:55<00:54,  3.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 53%|█████▎    | 17/32 [00:58<00:53,  3.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 56%|█████▋    | 18/32 [01:02<00:48,  3.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 59%|█████▉    | 19/32 [01:05<00:45,  3.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 62%|██████▎   | 20/32 [01:09<00:43,  3.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|██████▌   | 21/32 [01:13<00:40,  3.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|██████▉   | 22/32 [01:16<00:35,  3.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 72%|███████▏  | 23/32 [01:20<00:31,  3.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 75%|███████▌  | 24/32 [01:24<00:29,  3.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 78%|███████▊  | 25/32 [01:28<00:26,  3.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 81%|████████▏ | 26/32 [01:31<00:21,  3.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 84%|████████▍ | 27/32 [01:35<00:18,  3.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 88%|████████▊ | 28/32 [01:38<00:14,  3.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|█████████ | 29/32 [01:42<00:11,  3.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████▍| 30/32 [01:46<00:07,  3.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|█████████▋| 31/32 [01:49<00:03,  3.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████| 32/32 [01:52<00:00,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load the JSON file\n",
    "\n",
    "\n",
    "login(\"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\")\n",
    "# Extract input-output pairs from JSON\n",
    "file_path = \"task024_cosmosqa_answer_generation.json\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "instruct1=(\n",
    "    \"Craft one correct answer to the question given in input.\"\n",
    "    \"To make it more interesting, try to use non-stereotypical language if possible. \"\n",
    "    \"Make sure your correct answer is reasonably long, consistent with the context, and requires common sense (instead of explicit extraction from the context) \"\n",
    "    \"Use a response that is uncommon/non-stereotypical, so that it is less predictable\"\n",
    "    \"To be less repetitive, please vary your language for each question.\"\n",
    "    \"\\nReview: \"\n",
    ")\n",
    "instruct2=\"\\nGenerate: \"\n",
    "\n",
    "# Extract input-output pairs from JSON\n",
    "instances = data[\"Instances\"][4500:5000]\n",
    "test_inputs = [instruct1+instance[\"input\"]+instruct2 for instance in instances]\n",
    "test_outputs = [instance[\"output\"][0] for instance in instances]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "\n",
    "# Convert data to Hugging Face Dataset format\n",
    "test_ds = Dataset.from_dict({\"input\": test_inputs, \"output\": test_outputs})\n",
    "\n",
    "# Tokenizer setup\n",
    "base_model_path = \"meta-llama/Meta-Llama-3-8B\"  # Replace with actual model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# Check if tokenizer has a padding token, if not, set the eos_token as padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16  # Adjust as needed\n",
    "\n",
    "# Define the model and load weights\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "fine_tuned_weights_path = \"finetuned_weights\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, fine_tuned_weights_path)\n",
    "pyro.get_param_store().load('pyro_param_store_task2_evcl_best.pt')\n",
    "DEVICE = model.device\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "output_file_path = \"predictions_QA.json\"\n",
    "\n",
    "if os.path.exists(output_file_path):\n",
    "    with open(output_file_path, \"r\") as f:\n",
    "        saved_data = json.load(f)\n",
    "else:\n",
    "    saved_data = {\"predictions\": [], \"references\": []}\n",
    "\n",
    "print(\"Generating predictions incrementally:\")\n",
    "\n",
    "print(\"Generating predictions:\")\n",
    "\n",
    "for i in tqdm(range(0, len(test_inputs), batch_size)):  # Loop in batches\n",
    "    batch_inputs = test_inputs[i:i + batch_size]\n",
    "    batch_references = test_outputs[i:i + batch_size]\n",
    "\n",
    "    # Tokenize the inputs in a batch\n",
    "    inputs_tokenized = tokenizer(batch_inputs, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Apply Pyro parameters to LoRA layers\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, \"lora_A\"):\n",
    "                    for key in module.lora_A:\n",
    "                        loc = pyro.param(f\"{name}.lora_A.{key}_loc\")\n",
    "                        scale = pyro.param(f\"{name}.lora_A.{key}_scale\")\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            f\"{name}.lora_A.{key}\",\n",
    "                            dist.Normal(loc, scale).to_event(loc.dim())\n",
    "                        )\n",
    "            \n",
    "                        module.lora_A[key].weight.data.copy_(sampled_weight)\n",
    "\n",
    "                if hasattr(module, \"lora_B\"):\n",
    "                    for key in module.lora_B:\n",
    "                        loc = pyro.param(f\"{name}.lora_B.{key}_loc\")\n",
    "                        scale = pyro.param(f\"{name}.lora_B.{key}_scale\")\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            f\"{name}.lora_B.{key}\",\n",
    "                            dist.Normal(loc, scale).to_event(loc.dim())\n",
    "                        )\n",
    "                        \n",
    "                        module.lora_B[key].weight.data.copy_(sampled_weight)\n",
    "\n",
    "            # Generate predictions using the tokenized inputs\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=inputs_tokenized[\"input_ids\"],\n",
    "                attention_mask=inputs_tokenized[\"attention_mask\"],\n",
    "                max_new_tokens=15,  \n",
    "                min_length=10,  \n",
    "                no_repeat_ngram_size=2,  \n",
    "                num_return_sequences=1,\n",
    "                top_p=0.9,  \n",
    "                temperature=0.7  \n",
    "            )\n",
    "\n",
    "        # Decode generated IDs\n",
    "        batch_predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        predictions.extend(batch_predictions)\n",
    "        references.extend(batch_references)\n",
    "\n",
    "    saved_data[\"predictions\"].extend(batch_predictions)\n",
    "    saved_data[\"references\"].extend(batch_references)\n",
    "\n",
    "    # Save incrementally to JSON\n",
    "    with open(output_file_path, \"w\") as json_file:\n",
    "        json.dump(saved_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2803eb92-cbd8-44fb-b16f-df802c2f7e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.15205543057270776, 'rouge2': 0.027119491820603664, 'rougeL': 0.1360292092279667, 'rougeLsum': 0.1366083126514292}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'extracted_answers.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "assert \"Answers\" in df.columns, \"Sentiment column is missing in the CSV.\"\n",
    "assert \"References\" in df.columns, \"Reference column is missing in the CSV.\"\n",
    "\n",
    "# Load ROUGE metric from evaluate package\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "results = rouge.compute(predictions=df[\"Answers\"].tolist(), references=df[\"References\"].tolist())\n",
    "\n",
    "print(\"ROUGE Scores:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b403d2dc-a940-43c7-a931-4381149d039e",
   "metadata": {},
   "source": [
    "### Lora Only QA-SA (QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b177566-80b2-445b-8d66-42c3141c598c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ac0c71e572420ea9d048fb11d18140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions incrementally:\n",
      "Generating predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|▎         | 1/32 [00:05<02:54,  5.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|▋         | 2/32 [00:08<02:06,  4.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|▉         | 3/32 [00:11<01:47,  3.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 12%|█▎        | 4/32 [00:15<01:37,  3.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 16%|█▌        | 5/32 [00:18<01:30,  3.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 19%|█▉        | 6/32 [00:21<01:25,  3.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 22%|██▏       | 7/32 [00:24<01:22,  3.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 25%|██▌       | 8/32 [00:28<01:22,  3.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 28%|██▊       | 9/32 [00:32<01:20,  3.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|███▏      | 10/32 [00:35<01:14,  3.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|███▍      | 11/32 [00:38<01:09,  3.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 38%|███▊      | 12/32 [00:41<01:04,  3.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 41%|████      | 13/32 [00:44<01:01,  3.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 44%|████▍     | 14/32 [00:47<00:57,  3.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 47%|████▋     | 15/32 [00:50<00:53,  3.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 50%|█████     | 16/32 [00:54<00:50,  3.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 53%|█████▎    | 17/32 [00:57<00:47,  3.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 56%|█████▋    | 18/32 [01:00<00:44,  3.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 59%|█████▉    | 19/32 [01:03<00:40,  3.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 62%|██████▎   | 20/32 [01:06<00:37,  3.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|██████▌   | 21/32 [01:09<00:34,  3.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|██████▉   | 22/32 [01:12<00:30,  3.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 72%|███████▏  | 23/32 [01:15<00:27,  3.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 75%|███████▌  | 24/32 [01:18<00:24,  3.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 78%|███████▊  | 25/32 [01:22<00:22,  3.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 81%|████████▏ | 26/32 [01:25<00:18,  3.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 84%|████████▍ | 27/32 [01:28<00:15,  3.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 88%|████████▊ | 28/32 [01:31<00:12,  3.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|█████████ | 29/32 [01:34<00:09,  3.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████▍| 30/32 [01:37<00:06,  3.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|█████████▋| 31/32 [01:40<00:03,  3.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████| 32/32 [01:43<00:00,  3.23s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load the JSON file\n",
    "\n",
    "login(\"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\")\n",
    "# Extract input-output pairs from JSON\n",
    "file_path = \"task024_cosmosqa_answer_generation.json\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "instruct1=(\n",
    "    \"Craft one correct answer to the question given in input.\"\n",
    "    \"To make it more interesting, try to use non-stereotypical language if possible. \"\n",
    "    \"Make sure your correct answer is reasonably long, consistent with the context, and requires common sense (instead of explicit extraction from the context) \"\n",
    "    \"Use a response that is uncommon/non-stereotypical, so that it is less predictable\"\n",
    "    \"To be less repetitive, please vary your language for each question.\"\n",
    "    \"\\nReview: \"\n",
    ")\n",
    "instruct2=\"\\nGenerate: \"\n",
    "\n",
    "# Extract input-output pairs from JSON\n",
    "instances = data[\"Instances\"][4500:5000]\n",
    "test_inputs = [instruct1+instance[\"input\"]+instruct2 for instance in instances]\n",
    "test_outputs = [instance[\"output\"][0] for instance in instances]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "\n",
    "# Convert data to Hugging Face Dataset format\n",
    "test_ds = Dataset.from_dict({\"input\": test_inputs, \"output\": test_outputs})\n",
    "\n",
    "# Tokenizer setup\n",
    "base_model_path = \"meta-llama/Meta-Llama-3-8B\"  # Replace with actual model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# Check if tokenizer has a padding token, if not, set the eos_token as padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16  # Adjust as needed\n",
    "\n",
    "# Define the model and load weights\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "fine_tuned_weights_path = \"finetuned_weights_lora\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, fine_tuned_weights_path)\n",
    "DEVICE = model.device\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "output_file_path = \"predictions_Lora_QA.json\"\n",
    "\n",
    "if os.path.exists(output_file_path):\n",
    "    with open(output_file_path, \"r\") as f:\n",
    "        saved_data = json.load(f)\n",
    "else:\n",
    "    saved_data = {\"predictions\": [], \"references\": []}\n",
    "\n",
    "print(\"Generating predictions incrementally:\")\n",
    "\n",
    "print(\"Generating predictions:\")\n",
    "\n",
    "for i in tqdm(range(0, len(test_inputs), batch_size)):  # Loop in batches\n",
    "    batch_inputs = test_inputs[i:i + batch_size]\n",
    "    batch_references = test_outputs[i:i + batch_size]\n",
    "\n",
    "    # Tokenize the inputs in a batch\n",
    "    inputs_tokenized = tokenizer(batch_inputs, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():  # Standard torch.no_grad() without pyro\n",
    "        # Generate predictions using the tokenized inputs\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs_tokenized[\"input_ids\"],\n",
    "            attention_mask=inputs_tokenized[\"attention_mask\"],\n",
    "            max_new_tokens=15,  \n",
    "            min_length=10,  \n",
    "            no_repeat_ngram_size=2,  \n",
    "            num_return_sequences=1,\n",
    "            top_p=0.9,  \n",
    "            temperature=0.7  \n",
    "        )\n",
    "\n",
    "        # Decode generated IDs\n",
    "        batch_predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        predictions.extend(batch_predictions)\n",
    "        references.extend(batch_references)\n",
    "\n",
    "    saved_data[\"predictions\"].extend(batch_predictions)\n",
    "    saved_data[\"references\"].extend(batch_references)\n",
    "\n",
    "    # Save incrementally to JSON\n",
    "    with open(output_file_path, \"w\") as json_file:\n",
    "        json.dump(saved_data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae852cde-9627-4eda-a070-d5425fd65540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.10703696677478106, 'rouge2': 0.019555833881274846, 'rougeL': 0.09630928934111785, 'rougeLsum': 0.0965816343461406}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'Extracted_Answers_with_References.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "assert \"Generate_Answer\" in df.columns, \"Sentiment column is missing in the CSV.\"\n",
    "assert \"Reference_Answer\" in df.columns, \"Reference column is missing in the CSV.\"\n",
    "\n",
    "# Load ROUGE metric from evaluate package\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "results = rouge.compute(predictions=df[\"Generate_Answer\"].tolist(), references=df[\"Reference_Answer\"].tolist())\n",
    "\n",
    "print(\"ROUGE Scores:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544a694-83d1-403e-b0c2-32e182a93134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
