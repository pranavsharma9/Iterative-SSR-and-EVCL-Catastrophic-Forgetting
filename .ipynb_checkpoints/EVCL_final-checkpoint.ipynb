{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3df9f10-1efb-4437-b04e-e6c2020be1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import tyxe\n",
    "\n",
    "import random\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO, Trace_ELBO\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, TensorDataset\n",
    "\n",
    "from datasets import load_dataset  # Added to load SuperNI dataset\n",
    "\n",
    "from typing import Optional, List\n",
    "from model.mle_prior import MLEPrior\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaff032f-ae3f-4be6-a016-d5ee7237f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current Device Index: 0\n",
      "Current Device Name: NVIDIA A100-SXM4-80GB\n",
      "Number of GPUs: 1\n",
      "Device 0: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "current_device = torch.cuda.current_device()\n",
    "print(\"Current Device Index:\", current_device)\n",
    "\n",
    "device_name = torch.cuda.get_device_name(current_device)\n",
    "print(\"Current Device Name:\", device_name)\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "for device_id in range(num_gpus):\n",
    "    print(f\"Device {device_id}: {torch.cuda.get_device_name(device_id)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3192f-96f4-44da-9eb4-aea2856aab4f",
   "metadata": {},
   "source": [
    "### Task1 -QA LoRA+EVCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d43227-5365-4537-a585-1f370b504d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_info(\n",
    "    model, \n",
    "    data_loader, \n",
    "    prev_fisher_info=None, \n",
    "    ewc_gamma=1.0, \n",
    "    num_epochs=1, \n",
    "    head_modules=None, \n",
    "    n_samples=None\n",
    "):\n",
    "\n",
    "    fisher = {}\n",
    "    \n",
    "    # Initialize Fisher matrix for LoRA parameters, excluding head modules if provided\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name and (head_modules is None or not any(name.startswith(head) for head in head_modules)):\n",
    "            fisher[name] = torch.zeros_like(param).to(DEVICE)\n",
    "    \n",
    "    # Save the model's current training state and set to eval\n",
    "    old_training_state = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    batch_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            if n_samples is not None and batch_count >= n_samples:\n",
    "                break\n",
    "\n",
    "            print(f\"Processing batch {batch_count + 1}\")\n",
    "            model.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            try:\n",
    "                # with autocast(device_type='cuda'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "            # scaler.scale(loss).backward()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {batch_count + 1}: {e}\")\n",
    "                break\n",
    "\n",
    "            # Accumulate Fisher information for LoRA parameters\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'lora' in name and param.grad is not None and (head_modules is None or not any(name.startswith(head) for head in head_modules)):\n",
    "                    fisher[name] += param.grad.data ** 2\n",
    "\n",
    "            print(f\"Completed batch {batch_count + 1}\")\n",
    "            batch_count += 1\n",
    "\n",
    "    # Normalize Fisher information by the number of processed batches or samples\n",
    "    normalization_factor = batch_count if n_samples is None else min(batch_count, n_samples)\n",
    "    for name in fisher:\n",
    "        fisher[name] = fisher[name] / normalization_factor\n",
    "\n",
    "    # Integrate previous Fisher information with EWC scaling\n",
    "    if prev_fisher_info is not None:\n",
    "        for name in fisher:\n",
    "            if name in prev_fisher_info:\n",
    "                fisher[name] += ewc_gamma * prev_fisher_info[name]\n",
    "\n",
    "    # Restore the model's original training state\n",
    "    model.train(old_training_state)\n",
    "    \n",
    "    return fisher\n",
    "\n",
    "# Function to get variational posterior means\n",
    "def get_variational_posterior_means(model):\n",
    "    posterior_means = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'lora_A'):\n",
    "            # print('yes')\n",
    "            for key in module.lora_A:\n",
    "                param_name = f\"{name}.lora_A.{key}\"\n",
    "                loc_name = f\"{param_name}_loc\"\n",
    "                if loc_name in pyro.get_param_store():\n",
    "                    lora_A_loc = pyro.param(loc_name).detach().clone()\n",
    "                    # Add '.weight' to the parameter name\n",
    "                    posterior_means[f\"{param_name}.weight\"] = lora_A_loc\n",
    "        if hasattr(module, 'lora_B'):\n",
    "            # print('yes')\n",
    "            for key in module.lora_B:\n",
    "                param_name = f\"{name}.lora_B.{key}\"\n",
    "                loc_name = f\"{param_name}_loc\"\n",
    "                if loc_name in pyro.get_param_store():\n",
    "                    lora_B_loc = pyro.param(loc_name).detach().clone()\n",
    "                    # Add '.weight' to the parameter name\n",
    "                    posterior_means[f\"{param_name}.weight\"] = lora_B_loc\n",
    "    return posterior_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4265f3da-cf93-4c14-b872-82c7f2817cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import LoraLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc12ed2-8b42-4e1e-87f5-ed00d1eb2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling,BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import PeftConfig, PeftModel\n",
    "from accelerate import init_empty_weights\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "import bitsandbytes\n",
    "\n",
    "def deterministic_lora_task():\n",
    "    login(\"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\")\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    base_model_repo_id = \"meta-llama/Meta-Llama-3-8B\"  \n",
    "    adapter_model_dir = r\"/home/pranav24/cs-546-project/SSR/Latest_Weights/QA_Weights/finetuned-weights/QA_Final\"\n",
    "\n",
    "    os.chdir(r'/home/pranav24/cs-546-project')\n",
    "    \n",
    "   \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,  \n",
    "        device_map=\"auto\",  \n",
    "        offload_folder=\"offload\",  \n",
    "        offload_state_dict=True,  \n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_repo_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_repo_id,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.float16, \n",
    "    )\n",
    "    model.config.reduction = \"mean\" \n",
    "    \n",
    "\n",
    "    peft_config = PeftConfig.from_pretrained(adapter_model_dir)\n",
    "    model = PeftModel.from_pretrained(model, adapter_model_dir, config=peft_config)\n",
    "        \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            print(name)\n",
    "    return model,tokenizer\n",
    "\n",
    "\n",
    "# def initialize_lora():\n",
    "#     login(\"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\")\n",
    "#     # Set environment variable to manage memory fragmentation\n",
    "#     os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    \n",
    "     \n",
    "#     # Specify directories and the path to the zip file\n",
    "#     offload_dir = os.path.expanduser(\"llama_offload_evcl/\")\n",
    "     \n",
    "#     os.makedirs(offload_dir, exist_ok=True)\n",
    "     \n",
    "#     # Extract only the specified JSON file from the zip archive\n",
    "#     os.chdir('/home/pranav24/cs-546-project/SSR/Latest_Weights/QA_Weights')\n",
    "#     target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "     \n",
    "#     # Load tokenizer from Hugging Face\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "#     # Load the model with accelerate's offloading and device map auto-setup\n",
    "#     with init_empty_weights():\n",
    "#         model = AutoModelForCausalLM.from_pretrained(\n",
    "#             \"meta-llama/Meta-Llama-3-8B\",\n",
    "#             device_map=\"auto\",\n",
    "#             # max_memory=max_memory,\n",
    "#             offload_folder=offload_dir,\n",
    "#             load_in_8bit=True,\n",
    "#             llm_int8_enable_fp32_cpu_offload=True\n",
    "#         )\n",
    "     \n",
    "#     # Configure LoRA with reduced rank\n",
    "#     lora_config = LoraConfig(\n",
    "#         r=8,\n",
    "#         lora_alpha=16,\n",
    "#         lora_dropout=0.1,\n",
    "#         bias=\"none\",\n",
    "#         task_type=\"CAUSAL_LM\",\n",
    "#     )\n",
    "#     model = get_peft_model(model, lora_config)\n",
    "\n",
    "#     #printing the trainable parameters\n",
    "#     model.print_trainable_parameters()\n",
    "\n",
    "#     # for name, param in model.named_parameters():\n",
    "#     #     if 'lora' in name:\n",
    "#     #         print(name)\n",
    "\n",
    "#     return model, tokenizer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62ff472-4fa4-47c2-b2bf-f39fd4fbfbea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['device_map', 'offload_folder', 'offload_state_dict']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ac90589f4846e1a15a2131968ad609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading base model...\")\n",
    "model,tokenizer=deterministic_lora_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57385901-11a1-491b-abb3-1b924f4d6cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237eacfc9c514b08911eb5d8d8fca9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import init_empty_weights\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "os.chdir('/home/pranav24/cs-546-project/SSR/Latest_Weights/QA_Weights')\n",
    "target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "instances = json_data['Instances'][0:2500]\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "\n",
    "# Tokenize the dataset\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"input\"], examples[\"output\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization and set format\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Split dataset into train and eval\n",
    "train_size = int(0.8 * len(tokenized_datasets))\n",
    "train_dataset = tokenized_datasets.select(range(train_size))\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6015f59-7b73-46b6-940a-9f1ec22c3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(model, tokenizer, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93cecb98-c125-4a32-a87f-05e4addea699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    sampled_weights_log = []  # To store sampled weights for verification\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # Log sampled weights for LoRA layers during the forward pass\n",
    "                for name, module in model.named_modules():\n",
    "                    if hasattr(module, \"lora_A\"):\n",
    "                        for key in module.lora_A:\n",
    "                            loc = pyro.param(f\"{name}.lora_A.{key}_loc\")\n",
    "                            scale = pyro.param(f\"{name}.lora_A.{key}_scale\")\n",
    "                            sampled_weight = pyro.sample(\n",
    "                                f\"{name}.lora_A.{key}\",\n",
    "                                dist.Normal(loc, scale).to_event(loc.dim())\n",
    "                            )\n",
    "                            # Log sampled weight for debugging\n",
    "                            sampled_weights_log.append(\n",
    "                                (name, key, sampled_weight.clone().cpu().numpy())\n",
    "                            )\n",
    "                            # Ensure the sampled weight is used in the model\n",
    "                            module.lora_A[key].weight.data.copy_(sampled_weight)\n",
    "\n",
    "                    if hasattr(module, \"lora_B\"):\n",
    "                        for key in module.lora_B:\n",
    "                            loc = pyro.param(f\"{name}.lora_B.{key}_loc\")\n",
    "                            scale = pyro.param(f\"{name}.lora_B.{key}_scale\")\n",
    "                            sampled_weight = pyro.sample(\n",
    "                                f\"{name}.lora_B.{key}\",\n",
    "                                dist.Normal(loc, scale).to_event(loc.dim())\n",
    "                            )\n",
    "                            # Log sampled weight for debugging\n",
    "                            sampled_weights_log.append(\n",
    "                                (name, key, sampled_weight.clone().cpu().numpy())\n",
    "                            )\n",
    "                            # Ensure the sampled weight is used in the model\n",
    "                            module.lora_B[key].weight.data.copy_(sampled_weight)\n",
    "\n",
    "                # Perform forward pass\n",
    "                outputs = model(input_ids, labels=labels, attention_mask=attention_mask)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    # Log the sampled weights (optional, for debugging)\n",
    "    print(\"Sampled Weights Log:\")\n",
    "    for layer_name, key, weight in sampled_weights_log[:5]:  # Show only the first 5 entries\n",
    "        print(f\"Layer: {layer_name}, Key: {key}, Sampled Weight (snippet): {weight.flatten()[:5]}\")\n",
    "\n",
    "    print(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "479be571-c608-4e8b-9f9b-2c09915106a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model(model,eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae06200-7093-45ff-bec3-abc1e93834af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from torch.optim import AdamW\n",
    "import torch.cuda.amp as amp\n",
    "from transformers import get_scheduler\n",
    "from pyro.optim import ExponentialLR\n",
    "evaluation_loss=[]\n",
    "\n",
    "\n",
    "def run_lora_evcl_1(\n",
    "    train_loader,\n",
    "    eval_loader,\n",
    "    num_epochs: int = 100,\n",
    "    model: str = \"meta-llama/Meta-Llama-3-8B\",\n",
    "    batch_size: int = 2,\n",
    "    learning_rate: float = 1e-5,\n",
    "    logging_steps: int = 100,\n",
    "    eval_steps: int = 200,\n",
    "    save_steps: int = 500,\n",
    "    output_dir: str = \"finetuned-weights-LoRA-EVCL\",\n",
    "    load_pyro: bool = False,\n",
    "    best_output_dir=\"finetuned-weights-LoRA-EVCL-Final-Task1_VCL_best\"\n",
    "):\n",
    "\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False  # Freeze non-LoRA parameters\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    def bayesian_guide(input_ids, attention_mask, labels, epoch, warmup_epochs=10, min_scale_factor=0.1):\n",
    "\n",
    "        annealing_factor = max(1.0 - (epoch / warmup_epochs), min_scale_factor)\n",
    "        \n",
    "        # Define variational distributions over the LoRA parameters\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, 'lora_A'):\n",
    "                for key in module.lora_A:\n",
    "                    param_name = f\"{name}.lora_A.{key}\"\n",
    "                    lora_A_param = module.lora_A[key].weight\n",
    "                    device = lora_A_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_A_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.01 * torch.ones_like(lora_A_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    \n",
    "                    adjusted_scale = scale * annealing_factor\n",
    "                    \n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, adjusted_scale).to_event(lora_A_param.dim())\n",
    "                    )\n",
    "            if hasattr(module, 'lora_B'):\n",
    "                for key in module.lora_B:\n",
    "                    param_name = f\"{name}.lora_B.{key}\"\n",
    "                    lora_B_param = module.lora_B[key].weight\n",
    "                    device = lora_B_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_B_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.01 * torch.ones_like(lora_B_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    \n",
    "                    adjusted_scale = scale * annealing_factor\n",
    "                    \n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, adjusted_scale).to_event(lora_B_param.dim())\n",
    "                    )\n",
    "                    \n",
    "    def bayesian_model(input_ids, attention_mask, labels):\n",
    "        # Define a function to sample and substitute LoRA parameters\n",
    "        def model_with_sampled_lora():\n",
    "            # Sample LoRA parameters and set them in the model\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, 'lora_A'):\n",
    "                    for key in module.lora_A:\n",
    "                        param_name = f\"{name}.lora_A.{key}\"\n",
    "                        lora_A_module = module.lora_A[key]\n",
    "                        device = lora_A_module.weight.device\n",
    "    \n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                lora_A_module.weight.detach().to(device),\n",
    "                                (0.1 * torch.ones_like(lora_A_module.weight)).to(device)\n",
    "                            ).to_event(lora_A_module.weight.dim())\n",
    "                        )\n",
    "    \n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_A_module.weight.copy_(sampled_weight)\n",
    "    \n",
    "                if hasattr(module, 'lora_B'):\n",
    "                    for key in module.lora_B:\n",
    "                        param_name = f\"{name}.lora_B.{key}\"\n",
    "                        lora_B_module = module.lora_B[key]\n",
    "                        device = lora_B_module.weight.device\n",
    "    \n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                lora_B_module.weight.detach().to(device),\n",
    "                                (0.1 * torch.ones_like(lora_B_module.weight)).to(device)\n",
    "                            ).to_event(lora_B_module.weight.dim())\n",
    "                        )\n",
    "    \n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_B_module.weight.copy_(sampled_weight)\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            return loss\n",
    "    \n",
    "        # Use the modified model with sampled LoRA parameters\n",
    "        return model_with_sampled_lora()\n",
    "\n",
    "\n",
    "    # Set up SVI\n",
    "    if load_pyro:\n",
    "        print('using previous pyro params')\n",
    "        pyro.get_param_store().load('pyro_param_store_task1.pt')\n",
    "    else:\n",
    "        print('not using previous pyro params')\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "    optim = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "    optim = pyro.optim.PyroOptim(AdamW, {\"lr\": learning_rate, \"weight_decay\": 1e-5})\n",
    "  \n",
    "    scheduler = ExponentialLR({'optimizer': AdamW, 'optim_args': {'lr': learning_rate}, 'gamma': 0.1})\n",
    "    elbo = TraceMeanField_ELBO()\n",
    "    # svi = SVI(bayesian_model, bayesian_guide, scheduler, loss=elbo)\n",
    "    # svi = SVI(bayesian_model, lambda *args, **kwargs: bayesian_guide(*args, **kwargs, epoch=epoch), scheduler, loss=elbo)\n",
    "\n",
    "    print(f\"Training on Task 1...\")\n",
    "    max_wait=20\n",
    "    best_eval_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        svi = SVI(bayesian_model, lambda *args, **kwargs: bayesian_guide(*args, **kwargs, epoch=epoch), scheduler, loss=elbo)\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for num_batches, batch in enumerate(train_loader, 1):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            if epoch==0 and num_batches==1:\n",
    "                generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=512,  # Adjust as needed\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "                \n",
    "                batch_predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                print(batch_predictions)\n",
    "    \n",
    "                data = {\n",
    "                            \"batch_predictions\": batch_predictions,\n",
    "                        }\n",
    "\n",
    "\n",
    "                with open(f\"/home/pranav24/cs-546-project/Testing/predictions_EVCL_1_epoch_{epoch}_{num_batches}.json\", \"w\") as json_file:\n",
    "                    json.dump(data, json_file, indent=4)\n",
    "\n",
    "            # generated_ids = model.generate(\n",
    "            #     input_ids=input_ids,\n",
    "            #     attention_mask=attention_mask,\n",
    "            #     max_new_tokens=512,  # Adjust as needed\n",
    "            #     num_return_sequences=1,\n",
    "            # )\n",
    "\n",
    "            # batch_predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            # print(batch_predictions)\n",
    "\n",
    "            # data = {\n",
    "            #             \"batch_predictions\": batch_predictions,\n",
    "            #         }\n",
    "\n",
    "\n",
    "            # with open(f\"/home/pranav24/cs-546-project/Testing/predictions_EVCL_1_{num_batches}.json\", \"w\") as json_file:\n",
    "            #     json.dump(data, json_file, indent=4)\n",
    "            \n",
    "            loss = svi.step(input_ids, attention_mask, labels)\n",
    "            total_loss += loss\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "            # Logging\n",
    "            if num_batches % logging_steps == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch {epoch}, Step {num_batches}, Loss: {avg_loss}\")\n",
    "\n",
    "            # Evaluation\n",
    "            if num_batches % eval_steps == 0:\n",
    "                eval_loss=evaluate_model(model, eval_loader)\n",
    "                evaluation_loss.append(eval_loss)\n",
    "                \n",
    "\n",
    "\n",
    "        avg_epoch_loss = total_loss / num_batches\n",
    "        print(f\"Task 1 Epoch {epoch} completed. Average Loss: {avg_epoch_loss}\")\n",
    "\n",
    "        if epoch%10 ==0:\n",
    "            save_trained_model(model, tokenizer, output_dir)\n",
    "            pyro.get_param_store().save('pyro_param_store_task1_vcl.pt')\n",
    "\n",
    "        if epoch%5==0:\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=512,  # Adjust as needed\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "            batch_predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            print(batch_predictions)\n",
    "\n",
    "            data = {\n",
    "                        \"batch_predictions\": batch_predictions,\n",
    "                    }\n",
    "\n",
    "\n",
    "            with open(f\"/home/pranav24/cs-546-project/Testing/predictions_EVCL_1_epoch_{epoch}_{num_batches}.json\", \"w\") as json_file:\n",
    "                json.dump(data, json_file, indent=4)\n",
    "            \n",
    "                \n",
    "        if eval_loss<best_eval_loss:\n",
    "            best_eval_loss=eval_loss\n",
    "            no_improvement=0\n",
    "            save_trained_model(model, tokenizer, best_output_dir)\n",
    "            pyro.get_param_store().save('pyro_param_store_task1_vcl_best.pt')\n",
    "        else:\n",
    "            no_improvement+=1\n",
    "\n",
    "        if no_improvement>=max_wait:\n",
    "            print(f'early stopping at epoch: {epoch}')\n",
    "            break\n",
    "            \n",
    "    \n",
    "    save_trained_model(model, tokenizer, output_dir)\n",
    "    pyro.get_param_store().save('pyro_param_store_task1_vcl.pt') \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a88408ab-cd21-43bf-a8bb-6c9f4e8e3b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pranav24/cs-546-project/SSR/Latest_Weights/QA_Weights\n",
      "/home/pranav24/cs-546-project\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('/home/pranav24/cs-546-project/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed87466-51c2-43f4-bccc-e0710e0b8fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n",
      "not using previous pyro params\n",
      "Training on Task 1...\n",
      "[\"Context: I was bored, so I thought I 'd take apart a guitar humbucker I had lying around, and test its magnetic polarity. To do this, I took a fridge magnet, and test which end would repel it. Turned out, there was no end of the pickup magnet that repelled the fridge one. I tried everywhere on all six sides, and I didn't even notice any difference in the degree of attraction. \\nQuestion: What may happen after they noticed the magnet would not repel?The magnet would be thrown out.They may have noticed the magnet was faulty. \\nFact: The narrator is taking apart a guitar humbucker.They are taking apart the guitar humbucker to test the magnetic polarity.. \\nReason: They are testing the magnetic polarity because they are bored. They are bored so they are testing the magnetic polarity.They are bored so they are testing the magnetic polarity. \\nThe narrator is bored and they are testing the magnetic polarity. They are bored and they are testing the magnetic polarity. \\n\", \"Context: After we 'd cruised around for an hour or so, I got hot. I realized I was surrounded by water ( duh ) but would have to let go of one of my random oddities of never touching foot in the lake. We were far enough out that there was none of the usual slime skin on the water, or random trash floating by. I quickly jumped off the back of the jet ski without thinking. \\nQuestion: What's a possible reason the writer got hot?: They were sweating from the heat.They were sweating from the exercise. They were sweating from the water. They were sweating from the sun. \\nFact: They were sweating from the exercise. They were sweating from the exercise because they were exercising. They were exercising because they were on a jet ski. \\n. \\n\", 'Context: Actually it was all thanks to * cough Rave cough * that I was reminded about our little weekend getaway to Cameron Highlands last Friday. It was some serious fun we had there. Just the three of us. Kenny, Rave and yours truly. \\nQuestion: Did the narrator enjoy the trip?nter. The narrator enjoyed the trip because it was fun. \\nFact: The narrator enjoyed the trip because they were with friends.. \\nQuestion: What may happen after the trip?The narrator may want to do another trip. \\nFact: The narrator may want to do another trip because they enjoyed the trip. \\n', \"Context: I am thinkin about purchasing a 04'Infiniti G35 for $ 12,000, & it has 70k miles on it, the car looks and drives beautiful, but is it worth buying?. \\nQuestion: Why might I feel ambivalent about buying this car?The price is too high. \\nFact: I'm looking to buy a car.The car is too expensive. \\nClaim: I'm trying to decide whether or not to buy it. \\nRationale: I'm trying to decide whether or not to buy it. \\n\", \"Context: And to all those who stuck around for me thankyou and I appreciate everything You've done for me. ON THE REAL you know who you are. This year I've grown a stronger bond with each of my family members showing that I ca nt talk to them about anything. Junior year was a blast!. \\nQuestion: Why does the narrator look at Junior year with fondness?The narrator was able to bond with their family..The narrator was able to bond with their family.What happened to the narrator that made them appreciate their family more? They were able to bond with their family. \\nThey were able to bond with their family. \\nThe narrator was able to bond with their family. \\nThey were able to bond with their family. \\nThe narrator was able to bond with their family.The narrator was able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family. \\nThey were able to bond with their family\", 'Context: Good food too. It was so nice to just be were it was quiet, surrounded by trees and a lake. Helped clear my mind from this awful go go go of the city. It seems like my life is flashing past me at a million miles an hour. \\nQuestion: what made me clear my mind?.I went to a quiet place. \\nFact: I went to a quiet place to clear my mind.I went to a quiet place to clear my mind. \\n', \"Context: Just to say that my sister is OK. She's home now - they couldn't find out what was causing her stomach pains and the high white blood cell count. She stayed overnight and for the next day and then they said she could go home and see how she went. If she continues to get the stomach pains we are going to take her back to the GP and see if he will refer her to a specialist or something. Went to Brighton today to meet Louisa. \\nQuestion: What's a possible reason the writer's sister is home now?She was released from the hospital.They decided to go home. \\nFact: They decided to go home.The writer's sister was released from the hospital.They decided to go home. \\nFact: They decided to go home. They decided to go home. \\n\", 'Context: Exhaustion had rooted itself deep in her bones and the last few days, no matter how much she slept, she couldn\\'t shake it. She \\'d woken up that morning with a headache and struggled with it most of the day. She couldn\\'t take anything because no one knew how any kind of medicine would affect her. Besides, taking something meant admitting to someone in her family that she didn\\'t feel good. \\nQuestion: What may be the reason why she can\\'t take medicine for the headache?They don\\'t know how it would affect her.. \\nFact: They are not sure how the medicine would affect them.... \\nFact: They are not sure how the medicine would affect them. \\nQuote: \" They can\\'t take anything because no one knew how any kind of medicine would affect her. \" \\nQuote: \" They can\\'t take anything because no one knew how any kind of medicine would affect her. \" \\nExplanation: They don\\'t want to take the medicine because they don\\'t know how it will affect them. \\nExplanation: They don\\'t want to take the medicine because they don\\'t know how it will affect them. \\n']\n",
      "Epoch 0, Step 100, Loss: 6176289.785\n",
      "Epoch 0, Step 200, Loss: 6176286.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11961/1970798832.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Weights Log:\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [ 0.00744212  0.01787096 -0.00340345  0.02338447 -0.00342143]\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.01358767 -0.01266908 -0.00178339 -0.00520429  0.00176825]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.00067367 -0.00840912  0.00555926 -0.01680095  0.00778805]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [ 0.00232528  0.01133147  0.00329883  0.00396356 -0.00098567]\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.02270155 -0.00788337 -0.03990851 -0.00589622 -0.0139312 ]\n",
      "Evaluation Loss: 11.2919\n",
      "Task 1 Epoch 0 completed. Average Loss: 6176286.134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Test-Task1_VCL\n",
      "[\"Context: I have sustained many injuries from the outside world. My house isn't much safer. I run into walls, i trip over toys. I once tripped over one of Brady's toys and slammed into the doors that hide my washer and dryer. \\nQuestion: Why isn't my house safer?.\\n\\n\\nThe outside world is more dangerous than my house. \\n\", \"Context: Nate and Melissa are really cool, I've hung out with Nate before but never Melissa, but she was pretty nice. I usually don't get along with girls. Kristen and Diah were kind of empty - headed, but sweet overall, so I can't complain about them other than that they're too nice, but genuinely so. \\nQuestion: Why was the narrator hanging out with Nate and Melissa.?The narrator was hanging out with Nate and Melissa because they were really cool. \\nFact: The narrator was hanging out with Nate and Melissa because they were really cool. \\n\", 'Context: I woke up yesterday feeling a little risky in the kitchen and I decided to try something I had been seeing all over blog land. Savory oats, I was hesitant, but I think it went well. I made some hot grain cereal with water, smart balance, salt and pepper. I topped it with an over easy egg and a little fiesta cheese. \\nQuestion: Why might I have been hesitant to try the new dish?or What might be the reason I decided to try the new dish?I was hesitant because I had never tried a savory oatmeal before. \\nFact: I had never tried a savory oatmeal before. \\n', 'Context: Mostly though the children work on their own and are self - directed. They come to me for help if needed, but I encourage them to look it up as best they can. The little ones help me throughout the day with the daily chores ( even Nathaniel can fold cloth napkins and hand towels : ) ). It makes my load a lot lighter. \\nQuestion: What is a valid fact about the children?They help with the daily chores.They are self - directed. \\n', \"Context: Thought about going to the Bar in the Castro. On Mondays they have an 80's night / happy hour. On Muni some cute guy chats me up about my Jack Skellington hoodie. Then he shows off his Jack Skellington tattoo. \\nQuestion: WhaT MAY HAPPEN AT THE BAR AGAIN?a HAPPY HOUR. \\nFact: They May Be Happy And Enjoying Themselves. \\n\", 'Context: The whole time I was searching for \" real food \" like a burger, chicken fingers or pulled pork ( hey, it was a splurge day! ), but I never found any. I probably did not eat enough food to counteract the whiskey - never a good idea. \\nQuestion: Why is whiskey an important part of a splurge day?nIt is a way to relax and enjoy the day. \\n', \"Context: My reasoning was that she would then be sooooo tired that she would sleep through most of the 6 hour drive down. Ok, it wasn't a brilliant idea. It was actually not very well thought out at all. \\nQuestion: Did she sleep all the way through?ing through?. \\nThe answer is that she slept through the whole drive. \\n\", 'Context: She loves Hannah Montana, Jonas Brothers ( especially Joe ), and Sharpay & Troy of High School Musical. She already picks what to wear, asks me to buy her a pda phone so she can play bubble breaker and usually drops odd comments as if she\\'s a 20 yr old already. in short, tao na siya talaga!. \\nQuestion: What may be the reason speaker feels the way they do? \"She feels the way she does because she is a teenager who is starting to act like a teenager. \". \\nThe speaker may feel this way because they are a teenager and are starting to act like a teenager. \\nFact: She is starting to act like a teenager because she is one. \\n']\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Test-Task1_VCL_best\n",
      "Epoch 1, Step 100, Loss: 6528859.18\n",
      "Epoch 1, Step 200, Loss: 6528859.4825\n",
      "Sampled Weights Log:\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [ 0.00988038  0.01056841 -0.00244639  0.0122807  -0.0116655 ]\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [ 0.01242075  0.00284991 -0.01676497 -0.01391206  0.00914984]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.01034538 -0.00979094  0.01539778 -0.01315099  0.02376485]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.01601379  0.01533959 -0.00918084 -0.00597449  0.00233258]\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.03688769  0.01068242 -0.03513305  0.00844186 -0.00593068]\n",
      "Evaluation Loss: 11.1936\n",
      "Task 1 Epoch 1 completed. Average Loss: 6528859.034\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Test-Task1_VCL_best\n",
      "Epoch 2, Step 100, Loss: 6924452.17\n",
      "Epoch 2, Step 200, Loss: 6924453.095\n",
      "Sampled Weights Log:\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [ 0.00119215  0.01006813  0.00877164 -0.00334267 -0.02642598]\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.00563065 -0.005839    0.02025504 -0.00346591 -0.02134699]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.00875077 -0.00053856  0.00536021  0.00526438 -0.01042238]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.01223146  0.00493423  0.00830225 -0.00332014 -0.00420562]\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.01102006  0.00946065  0.0046041   0.00579148  0.00052408]\n",
      "Evaluation Loss: 11.1667\n",
      "Task 1 Epoch 2 completed. Average Loss: 6924453.528\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Test-Task1_VCL_best\n",
      "Epoch 3, Step 100, Loss: 7374397.145\n",
      "Epoch 3, Step 200, Loss: 7374396.7475\n",
      "Sampled Weights Log:\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [ 0.01244468  0.01628341 -0.00076958  0.00649337 -0.01966034]\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.00543369 -0.00380703  0.01082889  0.01293132 -0.0007501 ]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.01432351  0.00147233  0.00063932 -0.01296948 -0.01058194]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.01404465  0.02480661 -0.0064912   0.01017288 -0.00299145]\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj, Key: default, Sampled Weight (snippet): [ 0.00049971 -0.00138184 -0.02239466 -0.01190942 -0.00111435]\n",
      "Evaluation Loss: 11.3335\n",
      "Task 1 Epoch 3 completed. Average Loss: 7374396.758\n",
      "Epoch 4, Step 100, Loss: 7895289.94\n",
      "Epoch 4, Step 200, Loss: 7895290.2075\n",
      "Sampled Weights Log:\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [ 0.00200591  0.00862405 -0.0129512   0.02347852 -0.01730396]\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.00382923 -0.00868028 -0.00573778 -0.01081867 -0.00399627]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.0135329  -0.0113346   0.01406822 -0.0075291  -0.00972265]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [ 0.00982234  0.00467971  0.02575643  0.00599867 -0.01784812]\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.00545625  0.0077262  -0.0051365   0.00905472 -0.0117772 ]\n",
      "Evaluation Loss: 11.2676\n",
      "Task 1 Epoch 4 completed. Average Loss: 7895290.266\n",
      "Epoch 5, Step 100, Loss: 8512868.29\n",
      "Epoch 5, Step 200, Loss: 8512868.545\n",
      "Sampled Weights Log:\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [ 0.0143389   0.00684922  0.01061055 -0.00349406 -0.00802315]\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.00844948  0.00789448 -0.0032613  -0.00188046  0.00557048]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.00582351 -0.0085518   0.00288246 -0.00632176  0.01200915]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.00964957  0.00277163 -0.00784896 -0.00894753  0.00626384]\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.01617454 -0.00439391  0.00115631 -0.00533924 -0.01573328]\n",
      "Evaluation Loss: 11.2753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Epoch 5 completed. Average Loss: 8512868.408\n",
      "['Context: Two things happened today in Beijing. First off, incoming journalists were amazed to find China had successfully lifted the brown haze in city. Skies were crystal blue and the air felt noticeably lighter. \\nQuestion: Why did the sky appear clearer?nThey were able to clean the air.The air was cleaned. \\nFact: The air was cleaner.The air was cleaner because they cleaned it.The air was cleaner because they cleaned it. \\nThe air was cleaner because they cleaned it. \\nThe air was cleaner because they cleaned it. \\nThe air was cleaner because they cleaned it. \\nThey cleaned the air. \\nThey cleaned the air. \\nThey cleaned the air. \\n', \"Context: I'm not really sure why, but it's just good to see her and to know she's doing well. I really dislike having to avoid someone or keep some sort of hatred in my heart. It's just too hard for me to do. \\nQuestion: Why would I have hatred in my heart towards her?; I don't like her...It's just too hard for me to do. I don't like her. I don't like her. \\nFact: I am not a very good person. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I don't like her. I\", 'Context: \" Patrick, we\\'re concerned. \" he said. \" Why not just tell us you weren\\'t eating? \" he asked. \" Why let it get this bad? \" he asked. Patrick sighed. \\nQuestion: What is going to happen to Patrick after being confronted?\" Patrick is going to be forced to eat... \\nThe narrator is Patrick\\'s father. \\nHe is concerned about Patrick because he is not eating. \\nThe narrator wants Patrick to eat because he is worried that Patrick is starving.The narrator wants Patrick to eat because he is worried that Patrick is starving. \\nThe narrator wants Patrick to eat because he is worried that Patrick is starving. \\nThe narrator wants Patrick to eat because he is worried that Patrick is starving. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he is concerned about Patrick because he is not eating. \\nI think the narrator is Patrick\\'s father because he', \"Context: Our camera battery died ( and we forgot to pack the charger ) right around the time of Commencement, plus it's crappy to begin with. Unfortunately, the professional pics of me walking turned out terrible, partially because I was wearing no makeup ( time crunch ) and thus looked freakishly pale and glowing. I actually didn't look much better in this pic, till I messed with it a bunch in Picasa. I wish I was adept at Photoshop, so I could have customized the shape of the colored area. \\nQuestion: What may be a fact about this person?They don't know how to use Photoshop...The person is a college graduate. They are attending a graduation ceremony. \\nI have a crappy camera. I didn't have time to get ready for the graduation ceremony. I didn't wear make up. I don't know how to use Photoshop. \\nThe person is a college graduate. They are attending a graduation ceremony. They have a crappy camera. I didn't have time to get ready for the graduation ceremony. I didn't wear make up. I don't know how to use Photoshop. I have a crappy camera. I didn't have time to get ready for the graduation ceremony. I didn't wear make up. I don't know how to use Photoshop. \\nI have a crappy camera. I didn't have time to get ready for the graduation ceremony. I didn't wear make up. I don't know how to use Photoshop. \\nI have a crappy camera. I didn't have time to get ready for the graduation ceremony. I didn't wear make up. I don't know how to use Photoshop. \\nI have a crappy camera. I didn't have time to get ready for the graduation ceremony. I didn't wear make up. I don't know how to use Photoshop. I have a crappy camera. I didn't have time to get ready for the graduation ceremony. I didn't wear make up. I don't know how to use Photoshop. \\nI have a crappy camera. I didn't have time to get ready for the graduation ceremony. I didn't wear make up. I don't know how to use Photoshop. I have a crappy camera. I didn't have time to get ready for the graduation ceremony. I didn't wear make up. I don't know how to use Photoshop. \\nI have a crappy camera. I didn't have time to get ready for the graduation ceremony. I didn't wear make up. I don't know how to use Photoshop. I have a crappy camera. I didn't have time to get ready for the graduation ceremony. I didn't wear make up. I don't know how to use Photoshop. \\nI have a crappy camera. I didn't have time to get ready for the graduation ceremony. I didn't wear make up. I don't know how to use Photoshop. I have\", \"Context: It wasn't in his nature to refuse what took such effort to give. It was perhaps the trait that got him into most of the trouble that found him. But the fact still remained that he couldn't deny Aral his request. \\nQuestion: What might have he done for Aral?He might have given him a gift..The narrator is talking about a gift that is given to Aral. He might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nThe narrator is talking about a gift that is given to Aral. He might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nThe narrator is talking about a gift that is given to Aral. He might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give. \\nHe might have given him a gift. It is not in his nature to refuse what takes such effort to give\", 'Context: She really liked this guy -- as a friend... and when I went over to see her yesterday, she told me this. Now, my girlfriend has been through a lot for a 22-year - old and I am only 18. But she told me she was going to pick me... and she did this with tears in her eyes and everything... I really felt bad about this, and kind of pissed off, because her \" friend \" had made her sucidal when he said that to her --- that he liked her. \\nQuestion: What did I do after she picked me?My girlfriend picked me and I was sad. \\nFact: I was sad because I was not picked by her friend.I was sad because I was not picked by her friend. \\nShe picked me because I was a friend. \\nI was sad because she was crying. \\nI was sad because I was not picked by her friend. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying. \\nI was sad because she was crying.', \"Context: Did you know that Glamour Models Gone Bad has over 1024 different beautiful babes in their collection? These guys have a penchant for beautiful babes and it shows. They add new models all the time, and they aren't just any skank off the street. They find the beautiful babes so you have the best eye candy. \\nQuestion: Why is the speaker speaking so positively about the company, Glamour Models Gone Bad?iThey are impressed by their quality.The speaker is impressed by the models.They are impressed by the way the company is run.They are impressed by the models.The speaker is impressed by the models. \\nThey are impressed by the models. \\nThe speaker is impressed by the models. \\nThe speaker is impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by the models. \\nThey are impressed by\", 'Context: I had a really good time, a bunch of people came out and we all had a really good time. I was even more happy that Faye was able to tag along. \\nQuestion: What would the narrator say made her social night fun?entertaining people..The narrator would say that they were able to talk with people and make them laugh. \\nThe narrator would say that they were able to talk with people and make them laugh. \\nThe narrator would say that they were able to talk with people and make them laugh. \\nThe narrator would say that they were able to talk with people and make them laugh. \\nI had a really good time, a bunch of people came out and we all had a really good time. I was even more happy that Faye was able to tag along. \\nWhat would the narrator say made them happy?Faye was able to come along. \\nWhat would the narrator say made them happy?Faye was able to come along. \\nWhat would the narrator say made them happy?Faye was able to come along. \\nI had a really good time, a bunch of people came out and we all had a really good time. I was even more happy that Faye was able to tag along. \\nWhat would the narrator say made them happy?Faye was able to come along. \\nWhat would the narrator say made them happy?Faye was able to come along. \\nI had a really good time, a bunch of people came out and we all had a really good time. I was even more happy that Faye was able to tag along. \\nWhat would the narrator say made them happy?Faye was able to come along. \\nWhat would the narrator say made them happy?Faye was able to come along. \\nI had a really good time, a bunch of people came out and we all had a really good time. I was even more happy that Faye was able to tag along. \\nWhat would the narrator say made them happy?Faye was able to come along. \\nWhat would the narrator say made them happy?Faye was able to come along. \\nI had a really good time, a bunch of people came out and we all had a really good time. I was even more happy that Faye was able to tag along. \\nWhat would the narrator say made them happy?Faye was able to come along. \\nWhat would the narrator say made them happy?Faye was able to come along. \\nI had a really good time, a bunch of people came out and we all had a really good time. I was even more happy that Faye was able to tag along. \\nWhat would the narrator say made them happy?']\n",
      "Epoch 6, Step 100, Loss: 9270244.29\n",
      "Epoch 6, Step 200, Loss: 9270244.205\n",
      "Sampled Weights Log:\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.00140491  0.01791046 -0.00818407  0.00117286 -0.02623224]\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.00053619 -0.01472756  0.00194065  0.01939145 -0.00392035]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.01811506 -0.01238847 -0.00054552 -0.00943422  0.00678711]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.00811864  0.02700258  0.00134557  0.00487266  0.01514686]\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj, Key: default, Sampled Weight (snippet): [ 0.00324793 -0.00824989 -0.01693904 -0.0017273  -0.00824578]\n",
      "Evaluation Loss: 11.8516\n",
      "Task 1 Epoch 6 completed. Average Loss: 9270244.316\n",
      "Epoch 7, Step 100, Loss: 10248243.76\n",
      "Epoch 7, Step 200, Loss: 10248243.74\n",
      "Sampled Weights Log:\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [ 0.01007441  0.0144137   0.00744043  0.01151773 -0.01103398]\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.00275327 -0.00116283 -0.00475354  0.00189917 -0.00013548]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.0159134  -0.0087621   0.00557326 -0.00493642 -0.00748828]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [ 0.01275423  0.01020894 -0.00833957  0.00789546  0.00179113]\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.01877456  0.01056144 -0.0130429   0.0013463  -0.01588917]\n",
      "Evaluation Loss: 11.1685\n",
      "Task 1 Epoch 7 completed. Average Loss: 10248243.672\n",
      "Epoch 8, Step 100, Loss: 11628315.93\n",
      "Epoch 8, Step 200, Loss: 11628315.995\n",
      "Sampled Weights Log:\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [ 0.00567209 -0.01031177  0.01551429  0.00169041 -0.02248464]\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.01212521 -0.01440192 -0.01502896  0.00390749  0.00825992]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.00443591 -0.01568245 -0.0001089   0.00992753  0.00582154]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.0145009  -0.00487164 -0.00405522  0.00222645 -0.01408386]\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.00748078 -0.01652836 -0.01221776 -0.01314355  0.00184923]\n",
      "Evaluation Loss: 11.3594\n",
      "Task 1 Epoch 8 completed. Average Loss: 11628315.92\n",
      "Epoch 9, Step 100, Loss: 13989446.94\n",
      "Epoch 9, Step 200, Loss: 13989446.945\n",
      "Sampled Weights Log:\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [ 0.00115906  0.00093758  0.00083951 -0.00816154 -0.01143699]\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.00982613  0.00620109 -0.01158424  0.00340665 -0.00082003]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.00912601  0.00823844  0.00390811  0.01685861  0.0051352 ]\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj, Key: default, Sampled Weight (snippet): [-0.00576805  0.02748742 -0.00702635  0.01263972 -0.03570302]\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj, Key: default, Sampled Weight (snippet): [-0.01374749  0.01025211 -0.01270071 -0.01044605 -0.01140854]\n",
      "Evaluation Loss: 11.4907\n",
      "Task 1 Epoch 9 completed. Average Loss: 13989446.932\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Test-Task1_VCL\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model=run_lora_evcl_1(\n",
    "        train_loader=train_loader,\n",
    "        eval_loader=eval_loader,\n",
    "        num_epochs=10,\n",
    "        model=model,\n",
    "        batch_size=8,\n",
    "        # learning_rate=1e-5,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=100,\n",
    "        eval_steps=200,\n",
    "        save_steps=500,\n",
    "        output_dir=\"finetuned-weights-LoRA-EVCL-Test-Task1_VCL\",\n",
    "        load_pyro=False,\n",
    "        best_output_dir=\"finetuned-weights-LoRA-EVCL-Test-Task1_VCL_best\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64aee245-7e13-4698-8b75-7d72f09421c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['device_map', 'offload_folder', 'offload_state_dict']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddcd18897bd40e28b28cc208326eafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "os.chdir(r'/home/pranav24/cs-546-project')\n",
    "pyro.get_param_store().load('pyro_param_store_task1_vcl_best.pt')\n",
    "login(\"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\")\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "base_model_repo_id = \"meta-llama/Meta-Llama-3-8B\"  \n",
    "adapter_model_dir = r\"/home/pranav24/cs-546-project/finetuned-weights-LoRA-EVCL-Final-Task1_VCL_best\"\n",
    "\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  \n",
    "    device_map=\"auto\",  \n",
    "    offload_folder=\"offload\",  \n",
    "    offload_state_dict=True,  \n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_repo_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_repo_id,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16, \n",
    ")\n",
    "model.config.reduction = \"mean\" \n",
    "\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(adapter_model_dir)\n",
    "model = PeftModel.from_pretrained(model, adapter_model_dir, config=peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "602a70a2-af6d-4078-98bf-473da225ec18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4891f08-2d31-43d5-9231-a0c14332831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f3a69c3-3a02-4521-a882-a55b1fcca627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/1\n",
      "Processing batch 1\n",
      "Completed batch 1\n",
      "Processing batch 2\n",
      "Completed batch 2\n",
      "Processing batch 3\n",
      "Completed batch 3\n",
      "Processing batch 4\n",
      "Completed batch 4\n",
      "Processing batch 5\n",
      "Completed batch 5\n",
      "Processing batch 6\n",
      "Completed batch 6\n",
      "Processing batch 7\n",
      "Completed batch 7\n",
      "Processing batch 8\n",
      "Completed batch 8\n",
      "Processing batch 9\n",
      "Completed batch 9\n",
      "Processing batch 10\n",
      "Completed batch 10\n",
      "Processing batch 11\n",
      "Completed batch 11\n",
      "Processing batch 12\n",
      "Completed batch 12\n",
      "Processing batch 13\n",
      "Completed batch 13\n",
      "Processing batch 14\n",
      "Completed batch 14\n",
      "Processing batch 15\n",
      "Completed batch 15\n",
      "Processing batch 16\n",
      "Completed batch 16\n",
      "Processing batch 17\n",
      "Completed batch 17\n",
      "Processing batch 18\n",
      "Completed batch 18\n",
      "Processing batch 19\n",
      "Completed batch 19\n",
      "Processing batch 20\n",
      "Completed batch 20\n",
      "Processing batch 21\n",
      "Completed batch 21\n",
      "Processing batch 22\n",
      "Completed batch 22\n",
      "Processing batch 23\n",
      "Completed batch 23\n",
      "Processing batch 24\n",
      "Completed batch 24\n",
      "Processing batch 25\n",
      "Completed batch 25\n",
      "Processing batch 26\n",
      "Completed batch 26\n",
      "Processing batch 27\n",
      "Completed batch 27\n",
      "Processing batch 28\n",
      "Completed batch 28\n",
      "Processing batch 29\n",
      "Completed batch 29\n",
      "Processing batch 30\n",
      "Completed batch 30\n",
      "Processing batch 31\n",
      "Completed batch 31\n",
      "Processing batch 32\n",
      "Completed batch 32\n",
      "Processing batch 33\n",
      "Completed batch 33\n",
      "Processing batch 34\n",
      "Completed batch 34\n",
      "Processing batch 35\n",
      "Completed batch 35\n",
      "Processing batch 36\n",
      "Completed batch 36\n",
      "Processing batch 37\n",
      "Completed batch 37\n",
      "Processing batch 38\n",
      "Completed batch 38\n",
      "Processing batch 39\n",
      "Completed batch 39\n",
      "Processing batch 40\n",
      "Completed batch 40\n",
      "Processing batch 41\n",
      "Completed batch 41\n",
      "Processing batch 42\n",
      "Completed batch 42\n",
      "Processing batch 43\n",
      "Completed batch 43\n",
      "Processing batch 44\n",
      "Completed batch 44\n",
      "Processing batch 45\n",
      "Completed batch 45\n",
      "Processing batch 46\n",
      "Completed batch 46\n",
      "Processing batch 47\n",
      "Completed batch 47\n",
      "Processing batch 48\n",
      "Completed batch 48\n",
      "Processing batch 49\n",
      "Completed batch 49\n",
      "Processing batch 50\n",
      "Completed batch 50\n",
      "Processing batch 51\n",
      "Completed batch 51\n",
      "Processing batch 52\n",
      "Completed batch 52\n",
      "Processing batch 53\n",
      "Completed batch 53\n",
      "Processing batch 54\n",
      "Completed batch 54\n",
      "Processing batch 55\n",
      "Completed batch 55\n",
      "Processing batch 56\n",
      "Completed batch 56\n",
      "Processing batch 57\n",
      "Completed batch 57\n",
      "Processing batch 58\n",
      "Completed batch 58\n",
      "Processing batch 59\n",
      "Completed batch 59\n",
      "Processing batch 60\n",
      "Completed batch 60\n",
      "Processing batch 61\n",
      "Completed batch 61\n",
      "Processing batch 62\n",
      "Completed batch 62\n",
      "Processing batch 63\n",
      "Completed batch 63\n",
      "Processing batch 64\n",
      "Completed batch 64\n",
      "Processing batch 65\n",
      "Completed batch 65\n",
      "Processing batch 66\n",
      "Completed batch 66\n",
      "Processing batch 67\n",
      "Completed batch 67\n",
      "Processing batch 68\n",
      "Completed batch 68\n",
      "Processing batch 69\n",
      "Completed batch 69\n",
      "Processing batch 70\n",
      "Completed batch 70\n",
      "Processing batch 71\n",
      "Completed batch 71\n",
      "Processing batch 72\n",
      "Completed batch 72\n",
      "Processing batch 73\n",
      "Completed batch 73\n",
      "Processing batch 74\n",
      "Completed batch 74\n",
      "Processing batch 75\n",
      "Completed batch 75\n",
      "Processing batch 76\n",
      "Completed batch 76\n",
      "Processing batch 77\n",
      "Completed batch 77\n",
      "Processing batch 78\n",
      "Completed batch 78\n",
      "Processing batch 79\n",
      "Completed batch 79\n",
      "Processing batch 80\n",
      "Completed batch 80\n",
      "Processing batch 81\n",
      "Completed batch 81\n",
      "Processing batch 82\n",
      "Completed batch 82\n",
      "Processing batch 83\n",
      "Completed batch 83\n",
      "Processing batch 84\n",
      "Completed batch 84\n",
      "Processing batch 85\n",
      "Completed batch 85\n",
      "Processing batch 86\n",
      "Completed batch 86\n",
      "Processing batch 87\n",
      "Completed batch 87\n",
      "Processing batch 88\n",
      "Completed batch 88\n",
      "Processing batch 89\n",
      "Completed batch 89\n",
      "Processing batch 90\n",
      "Completed batch 90\n",
      "Processing batch 91\n",
      "Completed batch 91\n",
      "Processing batch 92\n",
      "Completed batch 92\n",
      "Processing batch 93\n",
      "Completed batch 93\n",
      "Processing batch 94\n",
      "Completed batch 94\n",
      "Processing batch 95\n",
      "Completed batch 95\n",
      "Processing batch 96\n",
      "Completed batch 96\n",
      "Processing batch 97\n",
      "Completed batch 97\n",
      "Processing batch 98\n",
      "Completed batch 98\n",
      "Processing batch 99\n",
      "Completed batch 99\n",
      "Processing batch 100\n",
      "Completed batch 100\n",
      "Processing batch 101\n",
      "Completed batch 101\n",
      "Processing batch 102\n",
      "Completed batch 102\n",
      "Processing batch 103\n",
      "Completed batch 103\n",
      "Processing batch 104\n",
      "Completed batch 104\n",
      "Processing batch 105\n",
      "Completed batch 105\n",
      "Processing batch 106\n",
      "Completed batch 106\n",
      "Processing batch 107\n",
      "Completed batch 107\n",
      "Processing batch 108\n",
      "Completed batch 108\n",
      "Processing batch 109\n",
      "Completed batch 109\n",
      "Processing batch 110\n",
      "Completed batch 110\n",
      "Processing batch 111\n",
      "Completed batch 111\n",
      "Processing batch 112\n",
      "Completed batch 112\n",
      "Processing batch 113\n",
      "Completed batch 113\n",
      "Processing batch 114\n",
      "Completed batch 114\n",
      "Processing batch 115\n",
      "Completed batch 115\n",
      "Processing batch 116\n",
      "Completed batch 116\n",
      "Processing batch 117\n",
      "Completed batch 117\n",
      "Processing batch 118\n",
      "Completed batch 118\n",
      "Processing batch 119\n",
      "Completed batch 119\n",
      "Processing batch 120\n",
      "Completed batch 120\n",
      "Processing batch 121\n",
      "Completed batch 121\n",
      "Processing batch 122\n",
      "Completed batch 122\n",
      "Processing batch 123\n",
      "Completed batch 123\n",
      "Processing batch 124\n",
      "Completed batch 124\n",
      "Processing batch 125\n",
      "Completed batch 125\n",
      "Processing batch 126\n",
      "Completed batch 126\n",
      "Processing batch 127\n",
      "Completed batch 127\n",
      "Processing batch 128\n",
      "Completed batch 128\n",
      "Processing batch 129\n",
      "Completed batch 129\n",
      "Processing batch 130\n",
      "Completed batch 130\n",
      "Processing batch 131\n",
      "Completed batch 131\n",
      "Processing batch 132\n",
      "Completed batch 132\n",
      "Processing batch 133\n",
      "Completed batch 133\n",
      "Processing batch 134\n",
      "Completed batch 134\n",
      "Processing batch 135\n",
      "Completed batch 135\n",
      "Processing batch 136\n",
      "Completed batch 136\n",
      "Processing batch 137\n",
      "Completed batch 137\n",
      "Processing batch 138\n",
      "Completed batch 138\n",
      "Processing batch 139\n",
      "Completed batch 139\n",
      "Processing batch 140\n",
      "Completed batch 140\n",
      "Processing batch 141\n",
      "Completed batch 141\n",
      "Processing batch 142\n",
      "Completed batch 142\n",
      "Processing batch 143\n",
      "Completed batch 143\n",
      "Processing batch 144\n",
      "Completed batch 144\n",
      "Processing batch 145\n",
      "Completed batch 145\n",
      "Processing batch 146\n",
      "Completed batch 146\n",
      "Processing batch 147\n",
      "Completed batch 147\n",
      "Processing batch 148\n",
      "Completed batch 148\n",
      "Processing batch 149\n",
      "Completed batch 149\n",
      "Processing batch 150\n",
      "Completed batch 150\n",
      "Processing batch 151\n",
      "Completed batch 151\n",
      "Processing batch 152\n",
      "Completed batch 152\n",
      "Processing batch 153\n",
      "Completed batch 153\n",
      "Processing batch 154\n",
      "Completed batch 154\n",
      "Processing batch 155\n",
      "Completed batch 155\n",
      "Processing batch 156\n",
      "Completed batch 156\n",
      "Processing batch 157\n",
      "Completed batch 157\n",
      "Processing batch 158\n",
      "Completed batch 158\n",
      "Processing batch 159\n",
      "Completed batch 159\n",
      "Processing batch 160\n",
      "Completed batch 160\n",
      "Processing batch 161\n",
      "Completed batch 161\n",
      "Processing batch 162\n",
      "Completed batch 162\n",
      "Processing batch 163\n",
      "Completed batch 163\n",
      "Processing batch 164\n",
      "Completed batch 164\n",
      "Processing batch 165\n",
      "Completed batch 165\n",
      "Processing batch 166\n",
      "Completed batch 166\n",
      "Processing batch 167\n",
      "Completed batch 167\n",
      "Processing batch 168\n",
      "Completed batch 168\n",
      "Processing batch 169\n",
      "Completed batch 169\n",
      "Processing batch 170\n",
      "Completed batch 170\n",
      "Processing batch 171\n",
      "Completed batch 171\n",
      "Processing batch 172\n",
      "Completed batch 172\n",
      "Processing batch 173\n",
      "Completed batch 173\n",
      "Processing batch 174\n",
      "Completed batch 174\n",
      "Processing batch 175\n",
      "Completed batch 175\n",
      "Processing batch 176\n",
      "Completed batch 176\n",
      "Processing batch 177\n",
      "Completed batch 177\n",
      "Processing batch 178\n",
      "Completed batch 178\n",
      "Processing batch 179\n",
      "Completed batch 179\n",
      "Processing batch 180\n",
      "Completed batch 180\n",
      "Processing batch 181\n",
      "Completed batch 181\n",
      "Processing batch 182\n",
      "Completed batch 182\n",
      "Processing batch 183\n",
      "Completed batch 183\n",
      "Processing batch 184\n",
      "Completed batch 184\n",
      "Processing batch 185\n",
      "Completed batch 185\n",
      "Processing batch 186\n",
      "Completed batch 186\n",
      "Processing batch 187\n",
      "Completed batch 187\n",
      "Processing batch 188\n",
      "Completed batch 188\n",
      "Processing batch 189\n",
      "Completed batch 189\n",
      "Processing batch 190\n",
      "Completed batch 190\n",
      "Processing batch 191\n",
      "Completed batch 191\n",
      "Processing batch 192\n",
      "Completed batch 192\n",
      "Processing batch 193\n",
      "Completed batch 193\n",
      "Processing batch 194\n",
      "Completed batch 194\n",
      "Processing batch 195\n",
      "Completed batch 195\n",
      "Processing batch 196\n",
      "Completed batch 196\n",
      "Processing batch 197\n",
      "Completed batch 197\n",
      "Processing batch 198\n",
      "Completed batch 198\n",
      "Processing batch 199\n",
      "Completed batch 199\n",
      "Processing batch 200\n",
      "Completed batch 200\n",
      "Processing batch 201\n",
      "Completed batch 201\n",
      "Processing batch 202\n",
      "Completed batch 202\n",
      "Processing batch 203\n",
      "Completed batch 203\n",
      "Processing batch 204\n",
      "Completed batch 204\n",
      "Processing batch 205\n",
      "Completed batch 205\n",
      "Processing batch 206\n",
      "Completed batch 206\n",
      "Processing batch 207\n",
      "Completed batch 207\n",
      "Processing batch 208\n",
      "Completed batch 208\n",
      "Processing batch 209\n",
      "Completed batch 209\n",
      "Processing batch 210\n",
      "Completed batch 210\n",
      "Processing batch 211\n",
      "Completed batch 211\n",
      "Processing batch 212\n",
      "Completed batch 212\n",
      "Processing batch 213\n",
      "Completed batch 213\n",
      "Processing batch 214\n",
      "Completed batch 214\n",
      "Processing batch 215\n",
      "Completed batch 215\n",
      "Processing batch 216\n",
      "Completed batch 216\n",
      "Processing batch 217\n",
      "Completed batch 217\n",
      "Processing batch 218\n",
      "Completed batch 218\n",
      "Processing batch 219\n",
      "Completed batch 219\n",
      "Processing batch 220\n",
      "Completed batch 220\n",
      "Processing batch 221\n",
      "Completed batch 221\n",
      "Processing batch 222\n",
      "Completed batch 222\n",
      "Processing batch 223\n",
      "Completed batch 223\n",
      "Processing batch 224\n",
      "Completed batch 224\n",
      "Processing batch 225\n",
      "Completed batch 225\n",
      "Processing batch 226\n",
      "Completed batch 226\n",
      "Processing batch 227\n",
      "Completed batch 227\n",
      "Processing batch 228\n",
      "Completed batch 228\n",
      "Processing batch 229\n",
      "Completed batch 229\n",
      "Processing batch 230\n",
      "Completed batch 230\n",
      "Processing batch 231\n",
      "Completed batch 231\n",
      "Processing batch 232\n",
      "Completed batch 232\n",
      "Processing batch 233\n",
      "Completed batch 233\n",
      "Processing batch 234\n",
      "Completed batch 234\n",
      "Processing batch 235\n",
      "Completed batch 235\n",
      "Processing batch 236\n",
      "Completed batch 236\n",
      "Processing batch 237\n",
      "Completed batch 237\n",
      "Processing batch 238\n",
      "Completed batch 238\n",
      "Processing batch 239\n",
      "Completed batch 239\n",
      "Processing batch 240\n",
      "Completed batch 240\n",
      "Processing batch 241\n",
      "Completed batch 241\n",
      "Processing batch 242\n",
      "Completed batch 242\n",
      "Processing batch 243\n",
      "Completed batch 243\n",
      "Processing batch 244\n",
      "Completed batch 244\n",
      "Processing batch 245\n",
      "Completed batch 245\n",
      "Processing batch 246\n",
      "Completed batch 246\n",
      "Processing batch 247\n",
      "Completed batch 247\n",
      "Processing batch 248\n",
      "Completed batch 248\n",
      "Processing batch 249\n",
      "Completed batch 249\n",
      "Processing batch 250\n",
      "Completed batch 250\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "prev_fisher_info = None\n",
    "prev_params = None\n",
    "ewc_gamma = 1.0  \n",
    "\n",
    "fisher_info = compute_fisher_info(\n",
    "    model=model,\n",
    "    data_loader=train_loader,\n",
    "    prev_fisher_info=prev_fisher_info,\n",
    "    ewc_gamma=ewc_gamma,\n",
    "    num_epochs=1,  \n",
    "    head_modules=None,  \n",
    "    n_samples=None  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb848a4b-e88c-4417-a87b-ce963878e251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher Information saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('fisher_info_task1.pkl', 'wb') as f:\n",
    "#     pickle.dump(fisher_info, f)\n",
    "# print(\"Fisher Information saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cf70e2a-5795-483a-b114-7b454c7d500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher Information loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "os.chdir('/home/pranav24/cs-546-project/')\n",
    "with open('fisher_info_task1.pkl', 'rb') as f:\n",
    "    fisher_info = pickle.load(f)\n",
    "print(\"Fisher Information loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7e157fa-3d0a-4f37-a264-8c6ada0f30aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 5.602710371022113e-05\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 7.817161531420425e-05\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.007847669534385204\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.025193164125084877\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 8.951361451181583e-06\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.1536309102666564e-05\n",
      "Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.005907014012336731\n",
      "Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.01295658852905035\n",
      "Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 9.04551416169852e-06\n",
      "Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.4987615941208787e-05\n",
      "Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.003545289160683751\n",
      "Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.009925187565386295\n",
      "Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 7.720158464508131e-06\n",
      "Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.0687756912375335e-05\n",
      "Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0015507894568145275\n",
      "Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.009180009365081787\n",
      "Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.9817803149635438e-06\n",
      "Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.869487045449205e-06\n",
      "Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.000911958864890039\n",
      "Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.011568509042263031\n",
      "Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 9.92708237390616e-07\n",
      "Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 6.515025461339974e-07\n",
      "Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0014279063325375319\n",
      "Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0060573602095246315\n",
      "Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.304444140259875e-06\n",
      "Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.467240508645773e-06\n",
      "Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0005496571538969874\n",
      "Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.00611923960968852\n",
      "Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 6.100791551943985e-07\n",
      "Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 4.82420148273377e-07\n",
      "Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0008777786861173809\n",
      "Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.00463161151856184\n",
      "Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.037290300904715e-07\n",
      "Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 5.57008149826288e-07\n",
      "Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0009283266263082623\n",
      "Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0038367626257240772\n",
      "Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.275565916283085e-07\n",
      "Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 7.377510087280825e-07\n",
      "Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0005091616185382009\n",
      "Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0036248862743377686\n",
      "Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.7264097702282015e-07\n",
      "Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.2561823104515497e-07\n",
      "Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0004791053361259401\n",
      "Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0017680996097624302\n",
      "Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 5.286869395604299e-07\n",
      "Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 5.159589591130498e-07\n",
      "Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0004583375994116068\n",
      "Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.004151042550802231\n",
      "Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 9.309759718689747e-08\n",
      "Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.426912999453634e-07\n",
      "Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0003805401502177119\n",
      "Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0027110306546092033\n",
      "Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.2107012632186525e-07\n",
      "Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.0037990111632098e-07\n",
      "Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0005757558392360806\n",
      "Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0027679845225065947\n",
      "Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 6.621618240387761e-07\n",
      "Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.2094757266822853e-06\n",
      "Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0004118913202546537\n",
      "Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0007903020596131682\n",
      "Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.3438419677622733e-07\n",
      "Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.4049422247808252e-07\n",
      "Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0007126223063096404\n",
      "Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.005605771206319332\n",
      "Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 7.110581634606206e-08\n",
      "Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 8.730012268642895e-08\n",
      "Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.001028817379847169\n",
      "Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.006190232466906309\n",
      "Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.082461776353739e-07\n",
      "Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.554903855729208e-07\n",
      "Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0008599888533353806\n",
      "Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0040125660598278046\n",
      "Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 9.300624697061721e-07\n",
      "Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.2110563097376144e-07\n",
      "Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0007742070010863245\n",
      "Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0017857361817732453\n",
      "Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 6.80050220580597e-08\n",
      "Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 6.154661491564184e-08\n",
      "Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00045243240310810506\n",
      "Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.002208318328484893\n",
      "Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.6068949548753153e-07\n",
      "Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.4287340377450164e-07\n",
      "Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0002987942425534129\n",
      "Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.000938900513574481\n",
      "Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.4671883263872587e-07\n",
      "Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.352617573502357e-07\n",
      "Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0002158806601073593\n",
      "Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.004138489253818989\n",
      "Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.367433123457886e-07\n",
      "Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.6147636706118647e-07\n",
      "Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00012181845522718504\n",
      "Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.002362028928473592\n",
      "Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.454178395008057e-07\n",
      "Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 4.410966312207165e-07\n",
      "Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0010522203519940376\n",
      "Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.003519802587106824\n",
      "Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 8.226785439546802e-07\n",
      "Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.748121517015534e-07\n",
      "Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0006585731171071529\n",
      "Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0030556712299585342\n",
      "Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.1688063611691177e-07\n",
      "Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.2256566606083652e-07\n",
      "Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0010496082250028849\n",
      "Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0029637054540216923\n",
      "Layer: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.0876833534421166e-08\n",
      "Layer: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.1112017972436661e-07\n",
      "Layer: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00041014113230630755\n",
      "Layer: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0059659164398908615\n",
      "Layer: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 5.203928594710305e-07\n",
      "Layer: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 5.210148401602055e-07\n",
      "Layer: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00048644491471350193\n",
      "Layer: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.00907720997929573\n",
      "Layer: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 7.586106107737578e-07\n",
      "Layer: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 9.956706890079658e-07\n",
      "Layer: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0005515313241630793\n",
      "Layer: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.003926522098481655\n",
      "Layer: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.236096463548165e-07\n",
      "Layer: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 9.52613135041247e-08\n",
      "Layer: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0012665746035054326\n",
      "Layer: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0023176786489784718\n",
      "Layer: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.0508733200585993e-07\n",
      "Layer: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.6707215877431736e-07\n",
      "Layer: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0007399377645924687\n",
      "Layer: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.004381331615149975\n",
      "Layer: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 5.01052888068898e-09\n",
      "Layer: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 4.881406390211396e-09\n",
      "Layer: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0013715435052290559\n",
      "Layer: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.005403806921094656\n"
     ]
    }
   ],
   "source": [
    "for name, fisher_matrix in fisher_info.items():\n",
    "    print(f\"Layer: {name}, Fisher Info Mean: {fisher_matrix.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aedf2ad5-9045-4dd3-8250-b791e69e4e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for name, fisher_matrix in fisher_info_check.items():\n",
    "#     print(f\"Layer: {name}, Fisher Info Mean: {fisher_matrix.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d1b4cfa-ef40-47e7-9f88-1940c51e0c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prev_posterior_means = get_variational_posterior_means(model)\n",
    "# torch.save(prev_posterior_means, f'posterior_means_task_{1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb386a2b-56f0-4528-85ac-8161cc0e75ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8118/4242824579.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  prev_posterior_means = torch.load('posterior_means_task_1.pt')\n"
     ]
    }
   ],
   "source": [
    "prev_posterior_means = torch.load('posterior_means_task_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf1d6a1f-9a1c-4f64-923b-3a02a008e407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0089,  0.0050,  0.0051,  ..., -0.0123, -0.0097, -0.0027],\n",
       "         [ 0.0048,  0.0048,  0.0027,  ..., -0.0147,  0.0005, -0.0033],\n",
       "         [-0.0054,  0.0072,  0.0114,  ..., -0.0055,  0.0123, -0.0172],\n",
       "         ...,\n",
       "         [-0.0124,  0.0095, -0.0006,  ...,  0.0063,  0.0076, -0.0081],\n",
       "         [-0.0139, -0.0162,  0.0034,  ...,  0.0069,  0.0097, -0.0077],\n",
       "         [-0.0072,  0.0038, -0.0073,  ..., -0.0069, -0.0005, -0.0084]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight': tensor([[-1.5358e-03, -8.9430e-04,  5.7832e-04,  ...,  3.9116e-04,\n",
       "           2.1487e-03, -6.4525e-04],\n",
       "         [-2.2344e-03, -3.1120e-03, -4.8351e-04,  ..., -1.0736e-03,\n",
       "           3.9612e-03, -1.8255e-03],\n",
       "         [-9.6764e-04, -3.1381e-03, -1.3185e-03,  ..., -4.2025e-04,\n",
       "           4.3880e-03, -1.0377e-06],\n",
       "         ...,\n",
       "         [-1.6329e-02,  1.4821e-02, -1.4626e-02,  ..., -1.5436e-02,\n",
       "          -1.5029e-02, -1.6553e-02],\n",
       "         [-1.6291e-02,  1.4324e-02, -1.5011e-02,  ..., -1.5377e-02,\n",
       "          -1.4993e-02, -1.6510e-02],\n",
       "         [-1.6348e-02,  1.4857e-02, -1.5539e-02,  ..., -1.5462e-02,\n",
       "          -1.5899e-02, -1.6970e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0016, -0.0029,  0.0032,  ..., -0.0018,  0.0051,  0.0217],\n",
       "         [-0.0073, -0.0092, -0.0055,  ...,  0.0108, -0.0045, -0.0017],\n",
       "         [-0.0078,  0.0202,  0.0089,  ..., -0.0121,  0.0038, -0.0054],\n",
       "         ...,\n",
       "         [ 0.0091,  0.0162,  0.0121,  ...,  0.0043,  0.0070, -0.0087],\n",
       "         [-0.0004,  0.0033, -0.0086,  ..., -0.0130,  0.0098,  0.0110],\n",
       "         [-0.0069, -0.0128, -0.0009,  ...,  0.0076,  0.0132, -0.0098]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0060,  0.0072, -0.0010,  ..., -0.0033,  0.0072, -0.0056],\n",
       "         [-0.0062,  0.0022,  0.0016,  ...,  0.0015,  0.0020, -0.0052],\n",
       "         [-0.0036,  0.0057, -0.0033,  ...,  0.0032,  0.0053, -0.0020],\n",
       "         ...,\n",
       "         [ 0.0004,  0.0051, -0.0059,  ..., -0.0091,  0.0056, -0.0011],\n",
       "         [ 0.0020, -0.0012, -0.0003,  ..., -0.0028, -0.0008,  0.0008],\n",
       "         [ 0.0005, -0.0042,  0.0061,  ...,  0.0097, -0.0040,  0.0026]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0133,  0.0002, -0.0122,  ...,  0.0177, -0.0124,  0.0095],\n",
       "         [ 0.0014,  0.0054,  0.0041,  ...,  0.0092,  0.0132, -0.0093],\n",
       "         [-0.0117,  0.0102, -0.0068,  ..., -0.0202,  0.0093,  0.0097],\n",
       "         ...,\n",
       "         [ 0.0058, -0.0088, -0.0054,  ..., -0.0034, -0.0144, -0.0115],\n",
       "         [ 0.0003, -0.0105, -0.0018,  ..., -0.0201, -0.0068, -0.0049],\n",
       "         [-0.0023, -0.0067, -0.0059,  ...,  0.0039,  0.0120, -0.0180]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight': tensor([[ 1.4959e-03, -1.8449e-03,  1.8808e-03,  ...,  4.1223e-03,\n",
       "          -3.8710e-04, -4.6174e-04],\n",
       "         [-6.4446e-04, -1.8164e-03,  3.3968e-04,  ...,  4.3895e-04,\n",
       "          -4.5958e-04,  8.4424e-04],\n",
       "         [ 2.4243e-03, -2.2095e-03,  2.0900e-03,  ...,  4.6726e-03,\n",
       "          -1.6904e-03,  1.6126e-03],\n",
       "         ...,\n",
       "         [-3.4638e-03, -3.8131e-03, -8.6051e-04,  ..., -8.4639e-04,\n",
       "          -2.3339e-03, -1.2604e-03],\n",
       "         [-2.5443e-03, -2.9330e-03,  2.5191e-04,  ..., -1.7866e-03,\n",
       "           6.5895e-04, -4.4030e-04],\n",
       "         [ 1.3505e-05, -4.5097e-03, -2.3336e-03,  ...,  4.0561e-04,\n",
       "          -2.2107e-03,  2.4323e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight': tensor([[-6.4297e-03,  6.9379e-03,  1.6957e-02,  ...,  1.5516e-02,\n",
       "           5.7661e-03,  1.4463e-02],\n",
       "         [ 2.2166e-03, -1.1090e-02, -5.7583e-03,  ...,  8.0538e-03,\n",
       "          -7.9524e-03,  1.8239e-04],\n",
       "         [ 1.0434e-02,  9.6507e-03,  8.8435e-03,  ...,  3.9591e-03,\n",
       "           4.0760e-03,  1.7149e-03],\n",
       "         ...,\n",
       "         [-1.7433e-02, -5.0512e-03, -8.1169e-04,  ...,  1.2532e-02,\n",
       "           2.3474e-04,  1.8045e-03],\n",
       "         [ 3.5798e-03, -8.3153e-05,  9.5227e-03,  ...,  6.7654e-03,\n",
       "          -1.3798e-02, -6.8993e-04],\n",
       "         [ 5.3826e-03,  3.9912e-03, -8.5205e-03,  ...,  1.0974e-02,\n",
       "           1.1900e-02,  7.1198e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight': tensor([[ 1.7984e-03, -1.6829e-03,  2.1065e-03,  ..., -2.9948e-03,\n",
       "          -1.6102e-03, -3.2758e-03],\n",
       "         [-1.6952e-03, -4.4840e-03, -2.3535e-03,  ...,  4.3802e-05,\n",
       "          -2.1842e-03, -4.3042e-04],\n",
       "         [ 1.1486e-03, -2.4644e-03, -1.5968e-03,  ..., -1.1216e-03,\n",
       "          -3.5215e-03, -1.0920e-03],\n",
       "         ...,\n",
       "         [-2.5602e-03,  1.0929e-04, -3.2616e-03,  ...,  2.6756e-03,\n",
       "          -1.1138e-03,  2.9469e-03],\n",
       "         [ 3.9303e-03, -1.2443e-03,  1.2664e-03,  ..., -2.2148e-03,\n",
       "           4.5217e-03, -7.6446e-04],\n",
       "         [-4.8158e-04, -4.1384e-03, -1.3554e-03,  ..., -8.4853e-05,\n",
       "          -1.4902e-03,  5.5574e-04]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0141,  0.0051, -0.0176,  ...,  0.0095,  0.0109,  0.0092],\n",
       "         [-0.0069, -0.0017, -0.0005,  ..., -0.0048,  0.0112,  0.0139],\n",
       "         [-0.0045, -0.0012, -0.0037,  ...,  0.0070, -0.0014, -0.0067],\n",
       "         ...,\n",
       "         [-0.0004, -0.0136,  0.0110,  ..., -0.0077, -0.0013,  0.0057],\n",
       "         [ 0.0100, -0.0068, -0.0093,  ..., -0.0034, -0.0100,  0.0003],\n",
       "         [ 0.0018,  0.0099,  0.0025,  ..., -0.0040,  0.0077,  0.0014]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight': tensor([[ 5.2506e-03,  4.6292e-03,  1.9086e-04,  ...,  5.3810e-03,\n",
       "          -5.0409e-04, -3.3392e-03],\n",
       "         [ 6.8772e-03,  5.0560e-03,  3.1625e-03,  ...,  4.3207e-03,\n",
       "          -4.2221e-03, -2.8184e-03],\n",
       "         [ 3.0731e-03,  2.8609e-03,  1.3572e-03,  ...,  3.5974e-03,\n",
       "          -4.8355e-05, -3.7308e-03],\n",
       "         ...,\n",
       "         [ 1.3357e-04,  8.6451e-04,  3.1462e-04,  ...,  4.1721e-03,\n",
       "           9.8394e-04,  4.2845e-03],\n",
       "         [-4.5775e-03, -3.5933e-03, -2.6194e-03,  ...,  5.4174e-04,\n",
       "           1.3012e-03,  1.2059e-03],\n",
       "         [ 1.5052e-03,  1.4103e-03,  1.5047e-03,  ...,  3.9656e-04,\n",
       "           1.5868e-03,  1.0285e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight': tensor([[ 7.0402e-03, -1.5130e-02,  1.0537e-02,  ..., -1.9347e-02,\n",
       "           6.3073e-03,  1.9050e-02],\n",
       "         [-8.1884e-03,  8.6998e-03,  1.4658e-02,  ..., -1.5389e-02,\n",
       "           8.4218e-03, -2.9263e-03],\n",
       "         [ 1.2834e-02,  1.7013e-02,  1.7252e-02,  ..., -4.2697e-03,\n",
       "          -1.5564e-02, -1.9754e-02],\n",
       "         ...,\n",
       "         [ 3.3740e-04,  9.3335e-04,  5.9897e-03,  ...,  2.1864e-02,\n",
       "           1.5764e-02, -1.3487e-02],\n",
       "         [-9.3996e-05, -1.3090e-03, -6.2711e-03,  ..., -1.7496e-02,\n",
       "           6.1434e-03,  1.8953e-02],\n",
       "         [ 6.9559e-05, -5.6223e-03,  1.3746e-02,  ...,  1.1863e-02,\n",
       "          -1.1660e-02, -1.0394e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight': tensor([[-1.4405e-03,  2.5551e-03,  4.1042e-03,  ..., -2.1338e-03,\n",
       "          -4.5383e-03,  4.9003e-03],\n",
       "         [ 2.8222e-04, -1.0643e-03,  3.9614e-05,  ..., -1.0619e-03,\n",
       "           1.9801e-04,  6.9881e-04],\n",
       "         [ 2.5323e-03,  4.4296e-04, -1.4089e-03,  ..., -1.6793e-03,\n",
       "           6.4118e-04,  1.1294e-03],\n",
       "         ...,\n",
       "         [-1.2317e-03, -7.7785e-04,  7.5529e-04,  ...,  1.6461e-03,\n",
       "          -1.0107e-03,  7.2967e-04],\n",
       "         [-2.4459e-03, -3.0400e-03, -1.7178e-03,  ...,  3.2815e-03,\n",
       "           2.0066e-03, -1.0896e-03],\n",
       "         [ 1.4630e-03,  2.9856e-03,  1.7284e-03,  ..., -2.2936e-03,\n",
       "          -1.2327e-03,  1.7409e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0119, -0.0097,  0.0028,  ...,  0.0118,  0.0024,  0.0126],\n",
       "         [ 0.0127, -0.0046, -0.0123,  ...,  0.0110, -0.0071,  0.0019],\n",
       "         [-0.0067, -0.0094,  0.0143,  ...,  0.0091,  0.0090,  0.0018],\n",
       "         ...,\n",
       "         [-0.0110,  0.0030, -0.0063,  ...,  0.0068, -0.0140,  0.0039],\n",
       "         [-0.0115, -0.0037, -0.0146,  ...,  0.0039,  0.0023, -0.0058],\n",
       "         [ 0.0025, -0.0046, -0.0088,  ...,  0.0046,  0.0032,  0.0030]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight': tensor([[-4.2650e-03,  2.3622e-03, -2.2003e-03,  ...,  7.4991e-05,\n",
       "           3.5393e-03,  1.7220e-04],\n",
       "         [-3.3942e-03, -1.3914e-03, -2.4167e-03,  ...,  2.2348e-03,\n",
       "           1.4898e-03, -2.4217e-03],\n",
       "         [-6.3562e-04,  8.3063e-04,  1.5149e-03,  ...,  9.2351e-04,\n",
       "          -1.7928e-03,  5.8831e-05],\n",
       "         ...,\n",
       "         [ 5.7869e-03, -3.7911e-03,  4.5821e-04,  ..., -6.0757e-03,\n",
       "          -6.2379e-03,  5.0816e-03],\n",
       "         [ 9.1146e-04,  1.7511e-03,  4.6030e-03,  ..., -2.6990e-03,\n",
       "          -3.7121e-03,  4.2930e-03],\n",
       "         [-1.8241e-03,  3.0909e-03,  2.3097e-04,  ...,  4.7819e-03,\n",
       "           5.6137e-03, -5.1524e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0074, -0.0020,  0.0100,  ...,  0.0199,  0.0036,  0.0127],\n",
       "         [-0.0044, -0.0094,  0.0017,  ...,  0.0070, -0.0170, -0.0096],\n",
       "         [ 0.0007, -0.0020,  0.0028,  ..., -0.0008,  0.0149, -0.0017],\n",
       "         ...,\n",
       "         [ 0.0045,  0.0077, -0.0063,  ..., -0.0068,  0.0082,  0.0044],\n",
       "         [-0.0026,  0.0128, -0.0060,  ..., -0.0148, -0.0021,  0.0183],\n",
       "         [ 0.0095, -0.0081,  0.0016,  ...,  0.0040,  0.0118,  0.0003]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0060, -0.0016,  0.0023,  ..., -0.0037,  0.0008,  0.0031],\n",
       "         [-0.0021,  0.0051,  0.0014,  ..., -0.0031,  0.0051, -0.0043],\n",
       "         [ 0.0014, -0.0010, -0.0002,  ..., -0.0007, -0.0008,  0.0025],\n",
       "         ...,\n",
       "         [ 0.0071, -0.0029,  0.0061,  ..., -0.0055,  0.0037,  0.0020],\n",
       "         [-0.0034,  0.0020, -0.0014,  ..., -0.0007,  0.0025,  0.0005],\n",
       "         [ 0.0004,  0.0010,  0.0014,  ..., -0.0008,  0.0014, -0.0015]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0154,  0.0150, -0.0067,  ...,  0.0124, -0.0108, -0.0144],\n",
       "         [ 0.0008,  0.0094, -0.0135,  ..., -0.0105, -0.0019,  0.0138],\n",
       "         [ 0.0049, -0.0049,  0.0006,  ..., -0.0022,  0.0080,  0.0008],\n",
       "         ...,\n",
       "         [-0.0066,  0.0115, -0.0124,  ..., -0.0005,  0.0145, -0.0156],\n",
       "         [ 0.0102,  0.0043,  0.0102,  ..., -0.0113, -0.0073,  0.0149],\n",
       "         [ 0.0038,  0.0047, -0.0072,  ..., -0.0022,  0.0101,  0.0059]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.0020, -0.0027, -0.0003,  ..., -0.0022,  0.0003, -0.0028],\n",
       "         [-0.0029,  0.0015,  0.0036,  ...,  0.0045, -0.0029,  0.0045],\n",
       "         [-0.0032,  0.0042,  0.0027,  ...,  0.0014, -0.0022,  0.0045],\n",
       "         ...,\n",
       "         [ 0.0035,  0.0019, -0.0025,  ..., -0.0024,  0.0026, -0.0025],\n",
       "         [ 0.0027,  0.0022, -0.0026,  ..., -0.0028,  0.0030, -0.0019],\n",
       "         [ 0.0040,  0.0006, -0.0022,  ..., -0.0035,  0.0039, -0.0031]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0086, -0.0087, -0.0052,  ...,  0.0006,  0.0113, -0.0077],\n",
       "         [ 0.0110,  0.0046,  0.0029,  ..., -0.0048, -0.0054,  0.0111],\n",
       "         [-0.0173, -0.0157, -0.0053,  ...,  0.0131, -0.0113,  0.0094],\n",
       "         ...,\n",
       "         [ 0.0050, -0.0090, -0.0093,  ...,  0.0079, -0.0161,  0.0137],\n",
       "         [ 0.0060, -0.0127, -0.0034,  ...,  0.0099,  0.0071,  0.0105],\n",
       "         [-0.0012, -0.0191, -0.0067,  ...,  0.0029, -0.0091, -0.0151]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0011, -0.0026,  0.0044,  ...,  0.0013,  0.0040,  0.0016],\n",
       "         [ 0.0032,  0.0020, -0.0022,  ...,  0.0008, -0.0030,  0.0009],\n",
       "         [ 0.0039,  0.0044,  0.0032,  ...,  0.0044,  0.0033,  0.0041],\n",
       "         ...,\n",
       "         [ 0.0017,  0.0051, -0.0036,  ...,  0.0022, -0.0047,  0.0021],\n",
       "         [-0.0050, -0.0016, -0.0033,  ..., -0.0056, -0.0036, -0.0049],\n",
       "         [ 0.0032, -0.0009,  0.0024,  ...,  0.0027,  0.0019,  0.0026]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight': tensor([[ 1.2991e-02, -2.1402e-03,  1.5410e-03,  ...,  6.8061e-03,\n",
       "          -1.2703e-02, -1.1516e-03],\n",
       "         [ 7.1351e-04, -1.6544e-02, -9.8010e-03,  ...,  6.5142e-03,\n",
       "           5.6518e-03, -7.5533e-03],\n",
       "         [-2.2174e-03, -2.7957e-03,  3.7373e-03,  ...,  7.9065e-03,\n",
       "           2.9201e-03,  1.2606e-02],\n",
       "         ...,\n",
       "         [ 1.6877e-02,  1.3336e-02,  1.8336e-02,  ...,  1.2345e-02,\n",
       "           5.6472e-03,  4.9848e-03],\n",
       "         [ 1.5246e-04, -8.1357e-05, -2.0891e-02,  ...,  1.1069e-02,\n",
       "          -1.3578e-02,  1.0463e-02],\n",
       "         [-7.7417e-03,  3.7733e-03, -1.8069e-02,  ...,  7.9225e-03,\n",
       "           2.3274e-03,  1.0208e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight': tensor([[-2.2528e-03,  3.3160e-04,  2.1221e-03,  ..., -2.2217e-03,\n",
       "           2.8770e-03,  1.5057e-03],\n",
       "         [-6.7046e-04,  1.3522e-03,  2.1217e-03,  ..., -5.9492e-05,\n",
       "           6.9273e-04,  4.4635e-04],\n",
       "         [ 1.9853e-03,  1.1357e-03,  4.3195e-04,  ...,  4.9623e-05,\n",
       "          -2.0214e-03,  1.1808e-04],\n",
       "         ...,\n",
       "         [-3.9420e-04, -2.7334e-03,  1.5768e-03,  ..., -1.9997e-03,\n",
       "          -1.3831e-03,  1.7261e-03],\n",
       "         [-1.2611e-03,  1.2580e-03, -1.2149e-03,  ..., -5.7489e-04,\n",
       "           6.1404e-03, -2.5448e-04],\n",
       "         [-1.0778e-03, -4.9518e-03,  1.7950e-03,  ...,  1.3247e-03,\n",
       "          -4.4937e-03, -1.7280e-04]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0167, -0.0094,  0.0014,  ..., -0.0028, -0.0133, -0.0085],\n",
       "         [-0.0033, -0.0028, -0.0167,  ...,  0.0084, -0.0058,  0.0151],\n",
       "         [-0.0010, -0.0014,  0.0071,  ...,  0.0041,  0.0080, -0.0130],\n",
       "         ...,\n",
       "         [-0.0007,  0.0076, -0.0002,  ..., -0.0023, -0.0014,  0.0036],\n",
       "         [ 0.0091, -0.0026,  0.0099,  ..., -0.0054, -0.0059,  0.0068],\n",
       "         [ 0.0016, -0.0120,  0.0035,  ...,  0.0159, -0.0048,  0.0139]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight': tensor([[-3.6034e-04, -1.1642e-03,  1.0744e-03,  ..., -2.5095e-03,\n",
       "           1.2816e-03, -7.6728e-05],\n",
       "         [-9.6311e-04,  5.3903e-04, -1.1504e-03,  ..., -1.6010e-03,\n",
       "          -4.7015e-05,  5.0086e-06],\n",
       "         [ 2.0127e-03,  2.8419e-03, -3.7696e-03,  ...,  4.5400e-03,\n",
       "          -1.0888e-03, -3.0064e-03],\n",
       "         ...,\n",
       "         [ 4.0978e-03,  4.4059e-03, -3.8297e-03,  ...,  3.4509e-03,\n",
       "          -3.6868e-03,  1.8997e-04],\n",
       "         [-4.1376e-03, -5.1898e-03,  5.7618e-03,  ..., -3.0934e-03,\n",
       "           4.4152e-03, -4.1194e-04],\n",
       "         [ 6.9978e-04, -2.0109e-04, -5.0092e-04,  ..., -1.1490e-05,\n",
       "          -6.6957e-04,  1.5044e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0148,  0.0010, -0.0143,  ..., -0.0060, -0.0068, -0.0098],\n",
       "         [-0.0057,  0.0047, -0.0042,  ..., -0.0153, -0.0142,  0.0017],\n",
       "         [ 0.0048,  0.0112, -0.0189,  ..., -0.0058,  0.0132, -0.0089],\n",
       "         ...,\n",
       "         [ 0.0025,  0.0128,  0.0053,  ...,  0.0138, -0.0122, -0.0149],\n",
       "         [ 0.0058, -0.0034,  0.0064,  ..., -0.0029,  0.0095,  0.0028],\n",
       "         [-0.0079, -0.0086,  0.0064,  ...,  0.0142,  0.0142,  0.0159]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight': tensor([[ 5.9981e-04,  7.0383e-04,  1.7958e-03,  ..., -3.2742e-03,\n",
       "           1.8548e-03, -6.9649e-03],\n",
       "         [-4.0520e-03, -2.1844e-03, -3.2849e-03,  ..., -6.1240e-05,\n",
       "           1.3241e-03,  3.9196e-03],\n",
       "         [-4.9080e-04, -1.7906e-03, -1.1753e-03,  ...,  6.0113e-03,\n",
       "           3.6334e-03,  1.7645e-03],\n",
       "         ...,\n",
       "         [ 1.3231e-03, -2.4802e-03, -2.2586e-03,  ..., -5.6388e-03,\n",
       "           3.4239e-03,  3.5066e-03],\n",
       "         [ 1.4465e-03, -1.4418e-03, -1.8687e-04,  ...,  4.4104e-03,\n",
       "           3.2462e-04, -4.7747e-04],\n",
       "         [ 2.0483e-03, -1.8106e-03,  7.8376e-04,  ...,  3.0550e-03,\n",
       "           1.3708e-05,  3.7487e-05]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0129, -0.0047, -0.0018,  ...,  0.0034,  0.0069, -0.0096],\n",
       "         [ 0.0075,  0.0080,  0.0120,  ...,  0.0137,  0.0039, -0.0113],\n",
       "         [ 0.0038, -0.0189, -0.0047,  ...,  0.0042,  0.0017,  0.0056],\n",
       "         ...,\n",
       "         [-0.0064,  0.0039,  0.0124,  ...,  0.0190, -0.0073,  0.0119],\n",
       "         [ 0.0039,  0.0082, -0.0102,  ...,  0.0075,  0.0042,  0.0002],\n",
       "         [ 0.0163, -0.0022, -0.0102,  ..., -0.0056, -0.0003, -0.0052]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0029, -0.0028,  0.0042,  ...,  0.0046,  0.0013, -0.0014],\n",
       "         [ 0.0008, -0.0007,  0.0010,  ...,  0.0012, -0.0015, -0.0004],\n",
       "         [ 0.0002,  0.0014, -0.0022,  ..., -0.0025, -0.0001, -0.0009],\n",
       "         ...,\n",
       "         [ 0.0034,  0.0019, -0.0014,  ..., -0.0017, -0.0031,  0.0025],\n",
       "         [-0.0004,  0.0022, -0.0016,  ..., -0.0020,  0.0008, -0.0004],\n",
       "         [ 0.0018,  0.0012, -0.0027,  ..., -0.0014, -0.0014,  0.0041]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight': tensor([[ 5.7548e-03,  1.6151e-03, -1.1263e-03,  ..., -6.9063e-05,\n",
       "           2.1063e-03,  1.9361e-03],\n",
       "         [ 1.3866e-03, -7.0616e-03, -7.5005e-03,  ..., -1.5335e-02,\n",
       "           2.8789e-03,  7.2661e-03],\n",
       "         [ 1.2092e-03, -1.4714e-02, -1.7922e-02,  ...,  5.5763e-03,\n",
       "          -1.5451e-02, -1.1729e-02],\n",
       "         ...,\n",
       "         [-1.6135e-02, -1.1769e-02,  4.1343e-03,  ..., -1.0455e-02,\n",
       "          -2.8770e-03,  1.2541e-02],\n",
       "         [ 1.5133e-02,  1.0545e-02,  1.1824e-02,  ..., -2.1957e-03,\n",
       "           7.3928e-03,  2.3641e-03],\n",
       "         [-6.8840e-04, -4.6604e-03,  1.5006e-02,  ...,  7.0522e-04,\n",
       "          -5.0636e-03, -5.3051e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight': tensor([[ 1.4237e-03, -6.6504e-04,  2.7199e-03,  ...,  7.5833e-04,\n",
       "          -1.4304e-03, -1.7898e-03],\n",
       "         [ 1.7245e-03,  7.7776e-04,  2.9131e-03,  ...,  8.3338e-04,\n",
       "          -7.6862e-04, -1.2036e-03],\n",
       "         [ 9.8872e-04,  7.8098e-04,  1.2066e-03,  ...,  1.9698e-04,\n",
       "          -2.3717e-03,  1.2571e-03],\n",
       "         ...,\n",
       "         [-5.2894e-03, -5.4752e-03,  4.0110e-05,  ..., -2.0228e-04,\n",
       "          -2.2624e-03,  9.5453e-04],\n",
       "         [ 2.2193e-03,  3.3195e-03,  2.5969e-03,  ..., -1.6133e-04,\n",
       "           8.5544e-05, -4.9192e-03],\n",
       "         [ 1.1276e-03,  3.7915e-03,  2.5054e-03,  ...,  6.9365e-04,\n",
       "          -2.1203e-03, -5.6360e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight': tensor([[ 2.7252e-04, -2.8528e-03,  7.3746e-05,  ...,  8.0353e-03,\n",
       "          -6.4268e-03, -6.9873e-03],\n",
       "         [-3.0041e-03,  6.6954e-03, -1.0691e-02,  ..., -1.9547e-02,\n",
       "           8.7832e-03, -1.1595e-02],\n",
       "         [ 1.8103e-02, -1.8101e-02,  9.1939e-03,  ..., -2.0317e-03,\n",
       "          -1.2084e-02,  1.5560e-03],\n",
       "         ...,\n",
       "         [ 9.8981e-03,  6.9977e-03, -1.6160e-02,  ..., -9.6354e-04,\n",
       "          -7.3072e-03,  4.8297e-03],\n",
       "         [-6.5339e-03, -1.5006e-02,  1.5932e-02,  ...,  9.1788e-03,\n",
       "          -6.2357e-03,  8.0868e-05],\n",
       "         [ 1.6900e-02,  6.6775e-03, -5.1470e-03,  ..., -8.6698e-04,\n",
       "          -1.6148e-02, -9.6515e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight': tensor([[ 3.0816e-03, -3.2382e-03, -3.3285e-03,  ..., -2.6998e-03,\n",
       "          -1.0381e-03, -1.4624e-04],\n",
       "         [ 1.2005e-03,  1.7147e-03,  5.0574e-04,  ...,  2.1153e-03,\n",
       "          -4.5667e-03,  3.4629e-03],\n",
       "         [ 3.0346e-03,  1.5191e-03,  1.2130e-03,  ...,  1.7992e-03,\n",
       "          -6.8721e-03,  2.3022e-03],\n",
       "         ...,\n",
       "         [-2.8631e-04, -8.8907e-05, -1.3302e-03,  ..., -2.6475e-03,\n",
       "           5.1855e-03, -9.5367e-04],\n",
       "         [-2.2450e-03,  1.5809e-03,  1.1282e-03,  ...,  1.3904e-03,\n",
       "          -1.2824e-03, -2.2742e-04],\n",
       "         [-1.9733e-03,  2.4690e-03,  3.4366e-03,  ...,  3.1509e-03,\n",
       "          -2.7805e-03,  2.8638e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0012, -0.0098,  0.0128,  ...,  0.0087,  0.0112, -0.0031],\n",
       "         [ 0.0167,  0.0093,  0.0091,  ..., -0.0045, -0.0017, -0.0053],\n",
       "         [ 0.0033,  0.0076,  0.0048,  ...,  0.0033, -0.0001, -0.0146],\n",
       "         ...,\n",
       "         [ 0.0126,  0.0060, -0.0106,  ..., -0.0112, -0.0045,  0.0030],\n",
       "         [ 0.0162, -0.0113,  0.0063,  ..., -0.0137, -0.0123,  0.0061],\n",
       "         [ 0.0094,  0.0093,  0.0062,  ..., -0.0035, -0.0105,  0.0108]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight': tensor([[ 1.2607e-03,  1.4154e-03,  4.0635e-04,  ..., -6.6057e-05,\n",
       "           5.2973e-04,  1.6299e-03],\n",
       "         [-7.3013e-04, -1.1021e-03,  2.4493e-03,  ..., -2.7810e-04,\n",
       "          -4.1848e-04,  1.7372e-03],\n",
       "         [-1.6084e-03, -2.3522e-03,  3.2754e-03,  ...,  1.5441e-03,\n",
       "          -2.0585e-03,  1.4579e-03],\n",
       "         ...,\n",
       "         [ 1.5987e-03,  2.7040e-03, -2.4416e-03,  ...,  4.1427e-03,\n",
       "           1.0479e-03, -2.0477e-03],\n",
       "         [-2.7386e-03, -3.8306e-03,  3.9003e-03,  ..., -3.4411e-03,\n",
       "          -3.8480e-03,  2.7973e-03],\n",
       "         [-3.4446e-03, -1.4001e-03, -4.8745e-03,  ..., -3.2477e-03,\n",
       "           6.0765e-04,  3.3441e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0088,  0.0082, -0.0030,  ...,  0.0090, -0.0029, -0.0020],\n",
       "         [-0.0151,  0.0116,  0.0021,  ..., -0.0058,  0.0088, -0.0054],\n",
       "         [ 0.0057, -0.0040,  0.0107,  ..., -0.0001,  0.0137, -0.0081],\n",
       "         ...,\n",
       "         [-0.0092, -0.0045, -0.0011,  ..., -0.0115,  0.0060, -0.0107],\n",
       "         [-0.0059,  0.0081,  0.0136,  ...,  0.0067, -0.0092, -0.0004],\n",
       "         [ 0.0192, -0.0143,  0.0128,  ...,  0.0009,  0.0092, -0.0060]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0013,  0.0002,  0.0016,  ..., -0.0008, -0.0012,  0.0001],\n",
       "         [-0.0006,  0.0002,  0.0006,  ..., -0.0002, -0.0004,  0.0012],\n",
       "         [-0.0013,  0.0023,  0.0023,  ..., -0.0002, -0.0023, -0.0016],\n",
       "         ...,\n",
       "         [ 0.0022, -0.0030, -0.0031,  ...,  0.0033,  0.0023,  0.0026],\n",
       "         [ 0.0027, -0.0044, -0.0058,  ...,  0.0061,  0.0039,  0.0050],\n",
       "         [ 0.0020, -0.0031, -0.0027,  ...,  0.0020,  0.0038,  0.0024]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0088, -0.0013, -0.0021,  ..., -0.0060,  0.0162, -0.0141],\n",
       "         [ 0.0062, -0.0102, -0.0018,  ..., -0.0038, -0.0040,  0.0066],\n",
       "         [-0.0094, -0.0136,  0.0019,  ...,  0.0068, -0.0075,  0.0114],\n",
       "         ...,\n",
       "         [ 0.0183, -0.0142,  0.0079,  ..., -0.0016,  0.0067,  0.0036],\n",
       "         [-0.0088,  0.0004,  0.0074,  ...,  0.0121,  0.0024,  0.0070],\n",
       "         [ 0.0061, -0.0069,  0.0091,  ..., -0.0126,  0.0135, -0.0009]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0004, -0.0010,  0.0019,  ...,  0.0002,  0.0024,  0.0011],\n",
       "         [ 0.0002, -0.0016,  0.0020,  ...,  0.0012,  0.0019,  0.0032],\n",
       "         [-0.0044,  0.0015, -0.0030,  ..., -0.0026, -0.0022,  0.0013],\n",
       "         ...,\n",
       "         [-0.0037, -0.0008,  0.0005,  ..., -0.0023,  0.0012,  0.0004],\n",
       "         [ 0.0007,  0.0027, -0.0022,  ..., -0.0014, -0.0036, -0.0016],\n",
       "         [-0.0013, -0.0002,  0.0015,  ...,  0.0008,  0.0010,  0.0014]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0174, -0.0181, -0.0026,  ..., -0.0012,  0.0090,  0.0155],\n",
       "         [ 0.0099, -0.0059, -0.0112,  ..., -0.0116, -0.0055,  0.0067],\n",
       "         [ 0.0103,  0.0111, -0.0056,  ..., -0.0043,  0.0090, -0.0100],\n",
       "         ...,\n",
       "         [ 0.0015,  0.0130, -0.0003,  ..., -0.0034,  0.0114,  0.0086],\n",
       "         [ 0.0140, -0.0182,  0.0043,  ..., -0.0181, -0.0168, -0.0066],\n",
       "         [ 0.0038, -0.0141,  0.0054,  ...,  0.0022,  0.0043, -0.0023]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight': tensor([[ 8.8401e-03,  1.8101e-03, -7.0048e-04,  ..., -3.4103e-03,\n",
       "           5.1748e-03, -4.8329e-03],\n",
       "         [ 6.6870e-03, -1.8024e-03,  1.8055e-03,  ..., -6.3183e-04,\n",
       "           3.0343e-03,  2.6978e-03],\n",
       "         [ 2.4607e-04, -1.8300e-03,  7.5788e-05,  ...,  2.8726e-03,\n",
       "          -2.3849e-03, -4.7957e-04],\n",
       "         ...,\n",
       "         [ 4.3928e-03, -2.7983e-03,  3.0309e-03,  ...,  5.8483e-04,\n",
       "           2.3373e-03,  4.5863e-03],\n",
       "         [ 2.1320e-03, -2.7151e-03,  3.2365e-03,  ...,  1.4576e-04,\n",
       "           5.8451e-04,  3.6758e-03],\n",
       "         [-1.7479e-04,  2.6481e-03, -2.0510e-03,  ..., -2.3659e-03,\n",
       "           3.4513e-04,  2.6434e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0075, -0.0186,  0.0037,  ...,  0.0064, -0.0118, -0.0102],\n",
       "         [ 0.0023,  0.0096, -0.0029,  ..., -0.0084,  0.0157,  0.0011],\n",
       "         [ 0.0147,  0.0090,  0.0097,  ..., -0.0095, -0.0082,  0.0057],\n",
       "         ...,\n",
       "         [ 0.0097,  0.0112,  0.0078,  ...,  0.0003, -0.0092, -0.0202],\n",
       "         [ 0.0117,  0.0028, -0.0023,  ..., -0.0183, -0.0122, -0.0049],\n",
       "         [ 0.0011, -0.0133,  0.0052,  ...,  0.0052,  0.0010,  0.0137]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0031,  0.0024,  0.0038,  ...,  0.0035, -0.0037, -0.0046],\n",
       "         [ 0.0006, -0.0013, -0.0011,  ..., -0.0015,  0.0017,  0.0025],\n",
       "         [-0.0032,  0.0018,  0.0028,  ...,  0.0022, -0.0016, -0.0026],\n",
       "         ...,\n",
       "         [ 0.0008,  0.0014, -0.0002,  ...,  0.0010, -0.0020, -0.0001],\n",
       "         [-0.0014,  0.0033,  0.0005,  ...,  0.0043, -0.0040, -0.0039],\n",
       "         [ 0.0024, -0.0030, -0.0022,  ..., -0.0036,  0.0011,  0.0047]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0171,  0.0085, -0.0096,  ..., -0.0010, -0.0225, -0.0118],\n",
       "         [-0.0065, -0.0092,  0.0008,  ...,  0.0137, -0.0111, -0.0150],\n",
       "         [-0.0104, -0.0089,  0.0073,  ..., -0.0074,  0.0097,  0.0045],\n",
       "         ...,\n",
       "         [ 0.0036, -0.0064,  0.0102,  ..., -0.0084,  0.0054, -0.0135],\n",
       "         [ 0.0088, -0.0057,  0.0025,  ...,  0.0043,  0.0046, -0.0184],\n",
       "         [-0.0061,  0.0171,  0.0061,  ..., -0.0072, -0.0064,  0.0048]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight': tensor([[ 6.9722e-03, -2.2048e-03,  4.1582e-03,  ...,  1.5088e-03,\n",
       "           6.0647e-03,  3.5317e-03],\n",
       "         [-3.6093e-03, -1.8004e-05, -2.9253e-03,  ..., -1.3690e-03,\n",
       "          -7.0767e-03, -2.6895e-03],\n",
       "         [-1.0014e-03, -2.8212e-03, -3.4671e-04,  ..., -2.7990e-03,\n",
       "           6.8892e-04, -1.1864e-03],\n",
       "         ...,\n",
       "         [ 8.8218e-04,  2.5101e-03,  4.4808e-03,  ...,  3.7650e-03,\n",
       "           8.0766e-04,  3.2762e-03],\n",
       "         [ 1.8583e-05,  3.1575e-03,  2.3487e-03,  ...,  1.1248e-03,\n",
       "           2.4197e-03,  1.9437e-03],\n",
       "         [-3.8768e-03, -5.1327e-03, -4.5006e-03,  ..., -5.7654e-03,\n",
       "          -1.6722e-03, -5.1615e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0158,  0.0106,  0.0170,  ...,  0.0055,  0.0006,  0.0018],\n",
       "         [ 0.0151,  0.0082,  0.0112,  ..., -0.0169,  0.0189,  0.0045],\n",
       "         [ 0.0036,  0.0009,  0.0132,  ...,  0.0171,  0.0099, -0.0109],\n",
       "         ...,\n",
       "         [ 0.0074,  0.0118,  0.0165,  ...,  0.0093,  0.0076,  0.0133],\n",
       "         [-0.0068,  0.0009,  0.0122,  ...,  0.0032,  0.0041, -0.0147],\n",
       "         [-0.0201,  0.0089,  0.0131,  ..., -0.0007,  0.0133, -0.0179]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight': tensor([[-3.3427e-04,  1.1099e-03, -2.0122e-03,  ...,  4.4785e-04,\n",
       "          -3.1699e-03, -1.7789e-04],\n",
       "         [ 4.3876e-03, -1.1912e-04, -4.7012e-04,  ..., -6.5515e-05,\n",
       "          -1.6090e-03, -5.5530e-04],\n",
       "         [ 4.8586e-03, -1.7174e-03, -2.5996e-04,  ...,  1.6781e-04,\n",
       "           1.8427e-03, -1.6521e-03],\n",
       "         ...,\n",
       "         [-3.9078e-03,  8.6582e-04, -2.6720e-03,  ..., -1.9257e-03,\n",
       "           3.5477e-04, -3.5153e-04],\n",
       "         [-3.8960e-03, -2.5142e-03,  3.0308e-04,  ..., -1.8293e-03,\n",
       "           1.0137e-03,  1.3059e-04],\n",
       "         [ 1.4101e-03,  1.2691e-03, -5.6992e-03,  ...,  3.0571e-03,\n",
       "          -3.0449e-03,  1.9589e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0138, -0.0080,  0.0072,  ...,  0.0135,  0.0069, -0.0007],\n",
       "         [-0.0017,  0.0136, -0.0040,  ...,  0.0182,  0.0031, -0.0082],\n",
       "         [ 0.0003,  0.0077, -0.0118,  ...,  0.0028, -0.0014,  0.0079],\n",
       "         ...,\n",
       "         [-0.0029,  0.0093,  0.0111,  ...,  0.0035,  0.0049, -0.0072],\n",
       "         [-0.0146,  0.0154,  0.0171,  ...,  0.0143,  0.0035,  0.0027],\n",
       "         [ 0.0034, -0.0142, -0.0130,  ...,  0.0036, -0.0098, -0.0085]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0062,  0.0022,  0.0006,  ...,  0.0029,  0.0045, -0.0015],\n",
       "         [ 0.0055,  0.0055,  0.0050,  ...,  0.0044,  0.0062, -0.0057],\n",
       "         [ 0.0039,  0.0007, -0.0018,  ...,  0.0012,  0.0023, -0.0003],\n",
       "         ...,\n",
       "         [-0.0002, -0.0042, -0.0053,  ..., -0.0022, -0.0042,  0.0044],\n",
       "         [ 0.0020,  0.0024,  0.0007,  ...,  0.0027,  0.0033, -0.0030],\n",
       "         [-0.0028, -0.0014,  0.0007,  ..., -0.0011, -0.0008, -0.0003]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight': tensor([[-8.0757e-04, -3.4801e-03,  3.5231e-03,  ...,  9.0236e-03,\n",
       "          -7.8253e-03, -1.3155e-02],\n",
       "         [-9.1489e-03,  5.9848e-03, -4.9568e-03,  ..., -1.9127e-03,\n",
       "          -6.4718e-03, -6.3508e-03],\n",
       "         [ 5.3048e-03, -4.5000e-03, -6.5741e-03,  ...,  5.7921e-03,\n",
       "           8.5229e-03,  1.8317e-03],\n",
       "         ...,\n",
       "         [ 1.3186e-02, -9.4292e-03, -1.3870e-06,  ..., -7.8738e-03,\n",
       "          -2.0951e-02, -5.8192e-03],\n",
       "         [-5.8294e-03, -1.2139e-02,  2.7009e-03,  ..., -1.5652e-03,\n",
       "          -3.0624e-04, -7.3320e-03],\n",
       "         [-1.1275e-02, -2.9378e-04, -5.3765e-03,  ..., -1.8657e-02,\n",
       "           1.5873e-02,  8.0009e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight': tensor([[ 5.5348e-03,  6.6563e-03,  5.5213e-03,  ...,  4.6276e-03,\n",
       "          -5.8463e-03, -6.2481e-03],\n",
       "         [-1.9584e-03, -4.1978e-03, -5.4149e-03,  ..., -4.8427e-03,\n",
       "           4.8580e-03,  2.7521e-03],\n",
       "         [-7.4596e-03, -7.1410e-03, -5.9059e-03,  ..., -3.8896e-03,\n",
       "           5.6435e-03,  6.3782e-03],\n",
       "         ...,\n",
       "         [-7.1175e-04, -1.5820e-03,  1.0333e-04,  ...,  1.4259e-03,\n",
       "          -2.0122e-03,  1.7084e-03],\n",
       "         [ 3.7782e-03,  9.2920e-04, -5.7400e-05,  ..., -4.6281e-03,\n",
       "           3.9174e-04, -1.8647e-03],\n",
       "         [-3.7747e-03, -1.7560e-03, -1.1394e-03,  ...,  3.4087e-03,\n",
       "          -2.8430e-03,  2.9442e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight': tensor([[-1.5010e-02,  1.2874e-03, -1.9517e-02,  ..., -3.3524e-03,\n",
       "          -3.9888e-03, -1.8944e-03],\n",
       "         [-1.0502e-02,  8.4951e-03, -7.1585e-03,  ..., -1.1117e-02,\n",
       "          -3.6301e-03, -1.3418e-02],\n",
       "         [-8.3146e-03,  7.8489e-05,  1.2793e-02,  ..., -1.6791e-02,\n",
       "          -1.3388e-02, -7.6709e-03],\n",
       "         ...,\n",
       "         [ 1.6412e-03, -1.1261e-02,  1.3029e-02,  ..., -1.2839e-02,\n",
       "           4.7371e-03,  1.1370e-02],\n",
       "         [ 1.6899e-03,  5.2246e-03, -1.2983e-03,  ...,  9.4643e-03,\n",
       "          -1.1496e-02, -2.2059e-03],\n",
       "         [-2.3098e-04, -4.6002e-03,  2.6091e-03,  ...,  1.7799e-02,\n",
       "          -1.1192e-02,  7.0767e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0030,  0.0041, -0.0014,  ...,  0.0042, -0.0054, -0.0034],\n",
       "         [-0.0036, -0.0027, -0.0038,  ..., -0.0022,  0.0029, -0.0006],\n",
       "         [-0.0028, -0.0020, -0.0041,  ..., -0.0004,  0.0015,  0.0034],\n",
       "         ...,\n",
       "         [ 0.0046,  0.0037, -0.0032,  ...,  0.0042, -0.0051, -0.0038],\n",
       "         [ 0.0046,  0.0036,  0.0069,  ...,  0.0007, -0.0029, -0.0024],\n",
       "         [ 0.0050,  0.0012,  0.0044,  ...,  0.0003, -0.0022, -0.0007]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0105,  0.0160, -0.0034,  ...,  0.0114,  0.0142,  0.0062],\n",
       "         [ 0.0055,  0.0104,  0.0152,  ...,  0.0095, -0.0120,  0.0043],\n",
       "         [-0.0033,  0.0027, -0.0052,  ...,  0.0053, -0.0154,  0.0052],\n",
       "         ...,\n",
       "         [ 0.0075,  0.0153, -0.0025,  ..., -0.0106,  0.0104,  0.0082],\n",
       "         [ 0.0071,  0.0134,  0.0145,  ...,  0.0063, -0.0039,  0.0119],\n",
       "         [ 0.0058, -0.0135,  0.0016,  ..., -0.0029, -0.0028, -0.0030]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0002, -0.0027,  0.0003,  ..., -0.0036, -0.0050,  0.0041],\n",
       "         [ 0.0004, -0.0036, -0.0015,  ..., -0.0027, -0.0034,  0.0022],\n",
       "         [ 0.0005, -0.0029,  0.0006,  ..., -0.0010, -0.0010,  0.0011],\n",
       "         ...,\n",
       "         [ 0.0025, -0.0035, -0.0039,  ..., -0.0024, -0.0023,  0.0033],\n",
       "         [-0.0033,  0.0019,  0.0034,  ..., -0.0001,  0.0035, -0.0025],\n",
       "         [ 0.0005,  0.0029,  0.0032,  ...,  0.0032,  0.0021, -0.0018]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0117, -0.0028,  0.0027,  ...,  0.0107, -0.0129,  0.0163],\n",
       "         [ 0.0011, -0.0022, -0.0036,  ...,  0.0063,  0.0037, -0.0104],\n",
       "         [-0.0171,  0.0034,  0.0039,  ...,  0.0078,  0.0037,  0.0066],\n",
       "         ...,\n",
       "         [ 0.0114, -0.0047, -0.0049,  ..., -0.0195, -0.0131,  0.0099],\n",
       "         [-0.0046, -0.0096,  0.0076,  ...,  0.0065, -0.0110, -0.0108],\n",
       "         [-0.0124,  0.0076,  0.0079,  ...,  0.0059,  0.0157, -0.0030]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight': tensor([[ 8.5991e-04,  3.2731e-05,  2.3727e-03,  ...,  1.1855e-03,\n",
       "           2.1743e-03, -4.6776e-04],\n",
       "         [ 1.8486e-03, -1.9792e-03,  2.3726e-03,  ...,  2.9364e-03,\n",
       "           1.0923e-03, -2.4809e-03],\n",
       "         [-1.2596e-03,  1.2086e-04, -2.2084e-03,  ..., -8.6078e-04,\n",
       "          -2.0930e-03,  5.2563e-05],\n",
       "         ...,\n",
       "         [-7.9171e-04,  5.5190e-04,  7.8988e-04,  ...,  8.0364e-04,\n",
       "           1.7618e-04, -1.9475e-03],\n",
       "         [-3.2323e-04,  3.1261e-04, -1.1859e-03,  ...,  7.2535e-04,\n",
       "           5.6933e-04, -5.4240e-04],\n",
       "         [ 1.9338e-03, -6.6843e-04, -2.4721e-04,  ..., -4.5919e-03,\n",
       "           1.2422e-03,  5.2373e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0015, -0.0055,  0.0032,  ..., -0.0021, -0.0192, -0.0088],\n",
       "         [-0.0090, -0.0139,  0.0027,  ..., -0.0017, -0.0113, -0.0027],\n",
       "         [ 0.0060, -0.0041, -0.0136,  ...,  0.0137,  0.0068,  0.0014],\n",
       "         ...,\n",
       "         [ 0.0151,  0.0111, -0.0032,  ..., -0.0098, -0.0084, -0.0134],\n",
       "         [ 0.0028, -0.0090,  0.0031,  ..., -0.0063,  0.0093, -0.0025],\n",
       "         [-0.0148, -0.0149,  0.0041,  ..., -0.0101, -0.0170,  0.0030]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight': tensor([[-7.1837e-04, -8.1950e-04, -5.5943e-05,  ...,  1.3314e-03,\n",
       "          -4.9648e-04,  3.1050e-03],\n",
       "         [ 1.7954e-03,  3.5911e-04,  9.5707e-04,  ..., -4.1506e-03,\n",
       "          -8.0617e-04,  1.1504e-03],\n",
       "         [-4.1042e-04, -1.0849e-03, -1.7903e-03,  ...,  2.6355e-03,\n",
       "          -3.5992e-03,  1.0918e-03],\n",
       "         ...,\n",
       "         [ 1.9920e-03, -2.7877e-03,  3.2035e-03,  ..., -3.0361e-03,\n",
       "          -2.1844e-03,  3.6043e-03],\n",
       "         [-5.1059e-04,  3.3291e-03, -3.2786e-04,  ..., -1.5944e-03,\n",
       "           2.8721e-03, -1.1570e-03],\n",
       "         [ 1.3307e-03,  2.7641e-03, -3.5468e-03,  ...,  8.6060e-04,\n",
       "           6.4643e-04, -2.2619e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0073, -0.0079,  0.0083,  ..., -0.0068, -0.0012, -0.0022],\n",
       "         [ 0.0058, -0.0077,  0.0153,  ..., -0.0127,  0.0067, -0.0073],\n",
       "         [-0.0094, -0.0101,  0.0104,  ..., -0.0211, -0.0045, -0.0059],\n",
       "         ...,\n",
       "         [-0.0035, -0.0002, -0.0097,  ...,  0.0058,  0.0158, -0.0177],\n",
       "         [ 0.0047, -0.0116, -0.0059,  ..., -0.0101, -0.0008, -0.0133],\n",
       "         [ 0.0139, -0.0169,  0.0189,  ..., -0.0040, -0.0101, -0.0130]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight': tensor([[ 2.3881e-03, -3.0274e-03, -2.0180e-03,  ...,  1.4028e-03,\n",
       "          -2.4489e-03,  1.9779e-03],\n",
       "         [ 1.1024e-04,  1.3691e-03,  2.1276e-03,  ...,  3.8055e-04,\n",
       "           2.3100e-03,  2.0326e-04],\n",
       "         [ 2.0492e-03,  2.6608e-04,  1.2341e-03,  ..., -1.7315e-03,\n",
       "          -1.3033e-03,  4.5979e-03],\n",
       "         ...,\n",
       "         [ 4.4497e-03, -4.9754e-03, -4.0196e-03,  ...,  5.2241e-03,\n",
       "          -5.6933e-03, -3.2190e-03],\n",
       "         [ 3.7814e-03, -1.2908e-03, -3.8149e-03,  ...,  3.8812e-03,\n",
       "          -4.7850e-03, -3.8189e-03],\n",
       "         [ 6.7210e-05, -3.5640e-04, -3.6454e-04,  ...,  2.7862e-04,\n",
       "          -9.5902e-05,  2.5159e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0053,  0.0102, -0.0151,  ..., -0.0079,  0.0064, -0.0033],\n",
       "         [-0.0213, -0.0058, -0.0122,  ..., -0.0016,  0.0054, -0.0092],\n",
       "         [ 0.0147, -0.0145, -0.0157,  ...,  0.0080, -0.0133,  0.0068],\n",
       "         ...,\n",
       "         [-0.0049,  0.0168,  0.0084,  ..., -0.0031,  0.0052,  0.0096],\n",
       "         [-0.0131, -0.0021, -0.0172,  ...,  0.0083, -0.0137, -0.0065],\n",
       "         [ 0.0098,  0.0153, -0.0020,  ...,  0.0026,  0.0115,  0.0079]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight': tensor([[-3.7589e-03,  2.3882e-03, -3.8274e-03,  ...,  3.7028e-03,\n",
       "          -1.9307e-03,  4.2696e-03],\n",
       "         [-2.0524e-04,  1.8496e-03, -3.0430e-03,  ...,  2.1029e-03,\n",
       "          -2.6797e-03,  8.0135e-04],\n",
       "         [ 1.7504e-05,  1.9287e-03, -3.1849e-03,  ...,  2.4436e-03,\n",
       "          -3.3169e-03,  6.8507e-04],\n",
       "         ...,\n",
       "         [ 9.9191e-04, -2.5886e-03,  2.5494e-03,  ...,  1.5983e-03,\n",
       "           4.2537e-04, -1.1208e-03],\n",
       "         [-3.5512e-03,  3.5788e-04, -2.0494e-03,  ...,  2.7102e-03,\n",
       "           2.1292e-03,  8.4364e-04],\n",
       "         [-1.4482e-03, -4.1867e-04,  1.8087e-03,  ...,  2.4063e-03,\n",
       "           1.1047e-03, -8.2694e-04]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0042, -0.0148,  0.0050,  ...,  0.0033,  0.0097,  0.0116],\n",
       "         [-0.0015, -0.0044, -0.0036,  ..., -0.0080, -0.0049,  0.0064],\n",
       "         [-0.0020, -0.0062,  0.0024,  ...,  0.0140,  0.0180, -0.0107],\n",
       "         ...,\n",
       "         [-0.0145,  0.0127, -0.0036,  ...,  0.0079, -0.0013, -0.0065],\n",
       "         [ 0.0150,  0.0113,  0.0121,  ...,  0.0035,  0.0099,  0.0202],\n",
       "         [ 0.0111, -0.0024, -0.0097,  ..., -0.0071, -0.0047,  0.0055]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0009, -0.0018,  0.0028,  ..., -0.0012,  0.0015,  0.0025],\n",
       "         [-0.0014, -0.0003,  0.0039,  ..., -0.0011,  0.0004,  0.0022],\n",
       "         [-0.0035, -0.0040, -0.0042,  ...,  0.0030, -0.0064, -0.0033],\n",
       "         ...,\n",
       "         [ 0.0021,  0.0007,  0.0003,  ...,  0.0036,  0.0023, -0.0031],\n",
       "         [-0.0016, -0.0011,  0.0009,  ...,  0.0045, -0.0020,  0.0027],\n",
       "         [ 0.0031, -0.0011,  0.0069,  ..., -0.0079,  0.0006,  0.0027]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight': tensor([[-4.4583e-03,  1.2720e-02, -1.2319e-02,  ..., -1.1881e-02,\n",
       "          -5.5002e-03, -1.1142e-02],\n",
       "         [ 7.6962e-03, -1.0965e-02,  5.2160e-03,  ...,  7.9385e-03,\n",
       "           1.1208e-02, -1.7852e-03],\n",
       "         [-7.7384e-03,  4.1189e-05,  1.8841e-03,  ..., -1.9493e-02,\n",
       "          -9.8375e-03,  7.9232e-03],\n",
       "         ...,\n",
       "         [ 5.7044e-03,  1.6042e-02, -1.2101e-02,  ..., -2.8091e-03,\n",
       "           6.6348e-03,  1.1564e-02],\n",
       "         [-2.8394e-04, -8.0253e-03,  1.8945e-02,  ...,  9.5589e-03,\n",
       "          -1.4496e-02,  3.6305e-03],\n",
       "         [-5.8631e-03, -2.7056e-03,  1.7175e-02,  ..., -9.7860e-04,\n",
       "           6.2646e-03, -1.5005e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight': tensor([[-4.0783e-03,  5.3734e-03, -2.8370e-03,  ...,  2.9426e-03,\n",
       "          -1.7077e-03,  2.8687e-03],\n",
       "         [ 1.7640e-03,  8.8734e-04, -3.4459e-04,  ..., -1.7186e-04,\n",
       "           4.4962e-03, -2.4356e-03],\n",
       "         [-4.3323e-03,  4.4195e-03, -3.0264e-03,  ...,  3.6923e-03,\n",
       "          -1.4961e-03,  4.6220e-03],\n",
       "         ...,\n",
       "         [-6.8081e-04, -2.9876e-03,  2.5311e-03,  ...,  1.7727e-03,\n",
       "           2.6880e-03, -4.8873e-03],\n",
       "         [-2.3975e-03,  2.3542e-03, -5.1158e-04,  ...,  6.1349e-03,\n",
       "           4.3270e-03,  1.8867e-03],\n",
       "         [-2.2786e-03,  4.1304e-04, -1.4702e-03,  ...,  9.1042e-04,\n",
       "           9.2429e-04, -5.7118e-05]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0184,  0.0039,  0.0076,  ...,  0.0062, -0.0028,  0.0055],\n",
       "         [-0.0074,  0.0013,  0.0020,  ...,  0.0066,  0.0028, -0.0038],\n",
       "         [ 0.0148, -0.0095,  0.0073,  ..., -0.0121, -0.0107, -0.0041],\n",
       "         ...,\n",
       "         [-0.0063,  0.0056, -0.0114,  ...,  0.0080,  0.0107,  0.0024],\n",
       "         [-0.0200, -0.0020,  0.0124,  ..., -0.0041,  0.0124, -0.0034],\n",
       "         [ 0.0093, -0.0127, -0.0202,  ..., -0.0136, -0.0148, -0.0094]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0008, -0.0003, -0.0041,  ..., -0.0036,  0.0048, -0.0018],\n",
       "         [-0.0010,  0.0009,  0.0018,  ...,  0.0016, -0.0018,  0.0038],\n",
       "         [ 0.0015,  0.0012, -0.0016,  ...,  0.0006,  0.0007, -0.0043],\n",
       "         ...,\n",
       "         [ 0.0015,  0.0021, -0.0064,  ..., -0.0016,  0.0061, -0.0087],\n",
       "         [ 0.0063,  0.0014,  0.0022,  ...,  0.0003,  0.0002,  0.0004],\n",
       "         [ 0.0029,  0.0067,  0.0001,  ...,  0.0032,  0.0015, -0.0024]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0126, -0.0058, -0.0069,  ..., -0.0016,  0.0123, -0.0121],\n",
       "         [ 0.0162,  0.0007,  0.0142,  ...,  0.0085,  0.0113,  0.0046],\n",
       "         [-0.0013,  0.0151,  0.0116,  ..., -0.0145, -0.0039, -0.0061],\n",
       "         ...,\n",
       "         [ 0.0135, -0.0145,  0.0012,  ...,  0.0137, -0.0127,  0.0006],\n",
       "         [-0.0025, -0.0097, -0.0160,  ...,  0.0142,  0.0027, -0.0067],\n",
       "         [ 0.0050, -0.0007,  0.0020,  ..., -0.0050, -0.0121, -0.0128]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight': tensor([[ 1.1264e-03, -7.1317e-04, -7.3275e-04,  ...,  2.8548e-03,\n",
       "           5.5941e-04,  1.5035e-03],\n",
       "         [-1.1775e-03, -3.4257e-05, -4.9198e-04,  ...,  1.0822e-03,\n",
       "          -4.1826e-04,  1.9390e-03],\n",
       "         [ 5.0474e-04,  5.6171e-04,  1.4003e-03,  ..., -4.5061e-04,\n",
       "          -5.1425e-04,  2.0321e-03],\n",
       "         ...,\n",
       "         [-1.5818e-03,  1.8074e-04, -2.9895e-03,  ...,  3.5648e-03,\n",
       "          -2.6804e-03,  1.3654e-03],\n",
       "         [ 2.7510e-03, -2.8660e-04,  2.9163e-03,  ..., -5.1286e-03,\n",
       "           2.3341e-03, -1.3272e-03],\n",
       "         [ 4.9252e-04, -1.0563e-03,  1.6050e-03,  ...,  2.3089e-03,\n",
       "           8.6783e-04, -1.0373e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0119,  0.0006,  0.0120,  ...,  0.0210, -0.0017, -0.0069],\n",
       "         [-0.0108,  0.0019, -0.0096,  ...,  0.0001, -0.0058,  0.0012],\n",
       "         [-0.0145, -0.0121, -0.0053,  ..., -0.0071, -0.0016, -0.0010],\n",
       "         ...,\n",
       "         [-0.0054,  0.0135, -0.0115,  ..., -0.0142, -0.0030,  0.0098],\n",
       "         [-0.0099, -0.0139, -0.0051,  ...,  0.0130,  0.0023,  0.0050],\n",
       "         [-0.0054,  0.0125, -0.0077,  ...,  0.0110, -0.0125,  0.0133]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight': tensor([[-4.9273e-03,  3.6263e-03, -3.1062e-03,  ...,  2.2640e-03,\n",
       "           3.3867e-03, -2.9601e-03],\n",
       "         [ 2.5578e-04,  6.0569e-03, -7.5033e-03,  ...,  8.2448e-03,\n",
       "           7.8254e-03, -7.1208e-03],\n",
       "         [-7.5031e-03,  1.9054e-03,  2.9288e-03,  ..., -6.7806e-04,\n",
       "          -1.9115e-03, -5.7573e-05],\n",
       "         ...,\n",
       "         [ 7.6084e-03, -5.8411e-03,  2.8465e-03,  ..., -6.0634e-03,\n",
       "          -3.3044e-03,  4.9794e-03],\n",
       "         [-4.4089e-05, -9.3465e-04,  1.0127e-03,  ...,  4.9264e-04,\n",
       "           6.0724e-04, -1.1981e-04],\n",
       "         [ 6.2781e-03,  5.3952e-04,  2.7847e-03,  ..., -5.2254e-03,\n",
       "          -2.6863e-03,  5.1835e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0105,  0.0093, -0.0083,  ...,  0.0035,  0.0081, -0.0068],\n",
       "         [ 0.0165,  0.0042,  0.0156,  ...,  0.0121,  0.0117,  0.0105],\n",
       "         [ 0.0180, -0.0056, -0.0049,  ...,  0.0178, -0.0008,  0.0029],\n",
       "         ...,\n",
       "         [ 0.0126, -0.0066, -0.0167,  ..., -0.0109,  0.0133,  0.0094],\n",
       "         [ 0.0006,  0.0176,  0.0130,  ..., -0.0039, -0.0044,  0.0197],\n",
       "         [ 0.0177,  0.0206,  0.0079,  ...,  0.0095, -0.0065,  0.0106]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight': tensor([[ 5.2084e-05,  2.1777e-03,  3.8281e-03,  ..., -1.0652e-03,\n",
       "           3.9069e-03,  3.5549e-03],\n",
       "         [ 4.6255e-03,  4.2217e-03,  5.3959e-03,  ..., -1.4603e-03,\n",
       "           6.6721e-03,  7.6761e-03],\n",
       "         [ 1.3529e-03, -3.2073e-03, -2.2083e-03,  ...,  2.8203e-03,\n",
       "          -3.4545e-03, -3.3673e-03],\n",
       "         ...,\n",
       "         [ 2.1283e-03,  2.7196e-03,  3.0533e-03,  ..., -3.5900e-03,\n",
       "           2.3678e-03,  2.0211e-03],\n",
       "         [ 1.4711e-03,  6.0813e-04,  2.5186e-04,  ...,  2.3295e-03,\n",
       "          -7.9655e-04, -2.8883e-03],\n",
       "         [-1.1288e-03, -6.8562e-04, -1.9563e-03,  ...,  3.7256e-03,\n",
       "          -1.1250e-03, -6.9323e-04]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0150,  0.0051,  0.0077,  ..., -0.0041,  0.0109,  0.0131],\n",
       "         [ 0.0035, -0.0192, -0.0178,  ...,  0.0130, -0.0062, -0.0040],\n",
       "         [ 0.0082,  0.0192,  0.0143,  ..., -0.0021, -0.0030,  0.0129],\n",
       "         ...,\n",
       "         [-0.0079, -0.0068,  0.0096,  ..., -0.0145, -0.0219, -0.0112],\n",
       "         [ 0.0091,  0.0043,  0.0051,  ..., -0.0017,  0.0047, -0.0054],\n",
       "         [ 0.0042,  0.0021,  0.0039,  ...,  0.0043, -0.0030, -0.0119]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight': tensor([[-3.0531e-03,  2.2277e-05, -1.8457e-03,  ..., -4.0317e-03,\n",
       "           3.6211e-04, -3.2670e-03],\n",
       "         [ 1.9824e-03, -1.7659e-03,  1.9771e-03,  ...,  1.9662e-03,\n",
       "           2.4308e-03,  2.5474e-03],\n",
       "         [ 2.3034e-03, -2.9794e-04,  1.6460e-03,  ...,  1.6821e-03,\n",
       "           7.1855e-05,  1.9724e-03],\n",
       "         ...,\n",
       "         [-1.1811e-03,  2.4571e-04, -1.9048e-03,  ..., -1.2802e-03,\n",
       "          -1.6313e-03, -2.3861e-03],\n",
       "         [-2.8386e-03,  4.6084e-03, -3.7433e-03,  ..., -6.1263e-03,\n",
       "          -2.8418e-03, -4.0422e-03],\n",
       "         [ 1.5322e-03,  4.4647e-03, -8.8615e-04,  ..., -1.0366e-03,\n",
       "          -1.7095e-03,  2.0472e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0148,  0.0032, -0.0098,  ..., -0.0082, -0.0154, -0.0072],\n",
       "         [-0.0091,  0.0131, -0.0051,  ..., -0.0112, -0.0090, -0.0077],\n",
       "         [ 0.0092,  0.0045,  0.0090,  ..., -0.0106, -0.0148, -0.0200],\n",
       "         ...,\n",
       "         [ 0.0044, -0.0086, -0.0037,  ..., -0.0041,  0.0107,  0.0089],\n",
       "         [-0.0134,  0.0069, -0.0029,  ...,  0.0031,  0.0022, -0.0022],\n",
       "         [-0.0152, -0.0131,  0.0034,  ..., -0.0018,  0.0169,  0.0096]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight': tensor([[ 3.9181e-03, -2.8856e-04,  2.0988e-04,  ..., -2.0473e-04,\n",
       "          -1.0302e-03,  1.7444e-03],\n",
       "         [-2.8125e-03,  2.8900e-04,  8.0497e-04,  ..., -4.8702e-04,\n",
       "           1.5835e-03,  7.3965e-04],\n",
       "         [ 1.7814e-03, -6.3360e-03,  5.7070e-03,  ...,  5.7525e-03,\n",
       "          -5.4567e-03, -5.5755e-03],\n",
       "         ...,\n",
       "         [-4.6859e-05, -3.7602e-03,  1.2939e-03,  ...,  5.2098e-04,\n",
       "           2.8683e-03,  4.3844e-03],\n",
       "         [ 3.0045e-03, -1.0431e-03,  5.7920e-04,  ...,  7.7728e-04,\n",
       "          -1.6334e-03,  5.5867e-04],\n",
       "         [ 3.5901e-04, -1.6905e-03,  4.0500e-04,  ...,  6.3982e-04,\n",
       "           2.1999e-04, -9.2974e-05]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0012,  0.0008, -0.0030,  ..., -0.0014, -0.0017, -0.0020],\n",
       "         [-0.0209,  0.0116,  0.0096,  ...,  0.0082, -0.0026, -0.0106],\n",
       "         [-0.0114,  0.0057,  0.0018,  ...,  0.0023, -0.0043,  0.0116],\n",
       "         ...,\n",
       "         [-0.0010,  0.0104,  0.0043,  ..., -0.0149, -0.0074,  0.0160],\n",
       "         [ 0.0051, -0.0102, -0.0201,  ...,  0.0020,  0.0163, -0.0018],\n",
       "         [ 0.0060,  0.0122, -0.0017,  ...,  0.0115, -0.0075,  0.0109]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight': tensor([[ 4.8578e-03, -4.7387e-03, -4.2171e-03,  ...,  4.4669e-03,\n",
       "          -5.0546e-03,  3.7998e-03],\n",
       "         [-2.1276e-03,  2.7876e-03,  1.1637e-03,  ..., -1.7741e-03,\n",
       "           2.5347e-03,  2.5285e-04],\n",
       "         [-3.3604e-04,  1.0467e-03, -7.1948e-05,  ..., -2.7802e-03,\n",
       "           8.4045e-04, -3.3696e-03],\n",
       "         ...,\n",
       "         [ 6.5669e-04, -3.1124e-04,  3.4496e-04,  ..., -3.1618e-04,\n",
       "          -4.0646e-03, -1.1996e-03],\n",
       "         [ 7.7336e-04, -2.6511e-03,  1.0138e-03,  ...,  2.0218e-03,\n",
       "          -3.9884e-03,  1.9086e-03],\n",
       "         [-1.8408e-03,  1.7115e-03,  2.3754e-03,  ..., -1.6642e-03,\n",
       "           1.8283e-03, -1.9393e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0161,  0.0061, -0.0102,  ..., -0.0152, -0.0124,  0.0137],\n",
       "         [ 0.0028, -0.0069,  0.0158,  ...,  0.0007, -0.0171,  0.0011],\n",
       "         [ 0.0071, -0.0092,  0.0133,  ..., -0.0109, -0.0025, -0.0132],\n",
       "         ...,\n",
       "         [-0.0051, -0.0055, -0.0179,  ...,  0.0107,  0.0128, -0.0128],\n",
       "         [-0.0085,  0.0031, -0.0080,  ...,  0.0076, -0.0053, -0.0066],\n",
       "         [-0.0167,  0.0108,  0.0068,  ...,  0.0048, -0.0177,  0.0168]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0037, -0.0051,  0.0002,  ...,  0.0041, -0.0006,  0.0005],\n",
       "         [-0.0005, -0.0012, -0.0007,  ...,  0.0015, -0.0004, -0.0033],\n",
       "         [-0.0003,  0.0032, -0.0007,  ..., -0.0042, -0.0020,  0.0028],\n",
       "         ...,\n",
       "         [-0.0006,  0.0011,  0.0011,  ..., -0.0040,  0.0011,  0.0010],\n",
       "         [-0.0027, -0.0044, -0.0039,  ...,  0.0036,  0.0031, -0.0063],\n",
       "         [-0.0035, -0.0028, -0.0008,  ...,  0.0033, -0.0002, -0.0023]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0020,  0.0201,  0.0151,  ...,  0.0173,  0.0028,  0.0163],\n",
       "         [-0.0132, -0.0200,  0.0010,  ...,  0.0166,  0.0043, -0.0022],\n",
       "         [-0.0121, -0.0020, -0.0124,  ..., -0.0002, -0.0063, -0.0105],\n",
       "         ...,\n",
       "         [ 0.0146,  0.0075, -0.0076,  ...,  0.0154,  0.0212, -0.0112],\n",
       "         [-0.0042,  0.0086,  0.0108,  ...,  0.0007, -0.0029, -0.0018],\n",
       "         [ 0.0006, -0.0134, -0.0207,  ..., -0.0016, -0.0049, -0.0118]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight': tensor([[ 2.8876e-03,  3.2835e-03,  1.8540e-06,  ...,  4.0342e-03,\n",
       "          -2.2889e-03, -8.7182e-04],\n",
       "         [ 7.8746e-03, -3.9449e-03, -2.7043e-04,  ...,  6.9524e-03,\n",
       "          -1.7056e-03, -7.7281e-03],\n",
       "         [-5.7293e-03, -2.0146e-03,  5.1496e-03,  ..., -8.3055e-03,\n",
       "          -4.6034e-03,  5.3685e-03],\n",
       "         ...,\n",
       "         [ 8.4089e-04,  5.1861e-04, -1.7999e-03,  ...,  6.9588e-03,\n",
       "           2.6445e-03, -4.7264e-03],\n",
       "         [-3.2563e-04,  3.3395e-03,  3.1476e-03,  ...,  2.4231e-03,\n",
       "          -5.4525e-03,  6.8960e-04],\n",
       "         [-1.0221e-03,  1.3545e-04, -2.8577e-03,  ..., -5.4389e-03,\n",
       "          -4.1039e-04,  3.7626e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0008, -0.0078, -0.0040,  ...,  0.0055, -0.0101,  0.0004],\n",
       "         [-0.0027,  0.0171, -0.0103,  ..., -0.0101, -0.0066,  0.0122],\n",
       "         [ 0.0016,  0.0138, -0.0087,  ...,  0.0079, -0.0090, -0.0016],\n",
       "         ...,\n",
       "         [-0.0077,  0.0165,  0.0026,  ..., -0.0071,  0.0046, -0.0097],\n",
       "         [ 0.0161, -0.0125,  0.0030,  ..., -0.0052, -0.0116, -0.0079],\n",
       "         [ 0.0084, -0.0122,  0.0007,  ...,  0.0147, -0.0101,  0.0030]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight': tensor([[ 1.7590e-03, -5.4654e-05,  3.3084e-03,  ..., -4.2204e-03,\n",
       "           1.1670e-03,  1.2971e-03],\n",
       "         [-3.4507e-03, -3.2691e-03, -4.9561e-03,  ..., -1.4393e-03,\n",
       "          -3.8799e-03, -4.1196e-04],\n",
       "         [-3.5346e-03,  3.2799e-03, -1.7029e-03,  ...,  1.8212e-03,\n",
       "          -1.2339e-03,  1.7208e-03],\n",
       "         ...,\n",
       "         [ 9.2776e-06,  1.8732e-04, -1.8253e-03,  ..., -1.1684e-03,\n",
       "          -2.1262e-03,  4.3045e-04],\n",
       "         [ 4.4461e-03,  6.6844e-03,  6.5019e-03,  ...,  3.4992e-03,\n",
       "           4.5299e-03, -5.2456e-03],\n",
       "         [-4.0629e-03, -4.1735e-03, -4.2140e-03,  ..., -4.7562e-03,\n",
       "          -4.6354e-03,  2.5111e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0061,  0.0065,  0.0139,  ..., -0.0150,  0.0076,  0.0148],\n",
       "         [-0.0138, -0.0021, -0.0142,  ...,  0.0105,  0.0064,  0.0085],\n",
       "         [-0.0013, -0.0053, -0.0027,  ...,  0.0122, -0.0139, -0.0041],\n",
       "         ...,\n",
       "         [-0.0067,  0.0116, -0.0090,  ..., -0.0025,  0.0142,  0.0032],\n",
       "         [-0.0016, -0.0067, -0.0070,  ..., -0.0043, -0.0066,  0.0114],\n",
       "         [ 0.0085, -0.0145, -0.0015,  ...,  0.0073,  0.0170,  0.0100]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0026,  0.0012,  0.0026,  ...,  0.0013,  0.0012,  0.0028],\n",
       "         [-0.0046, -0.0040, -0.0011,  ..., -0.0062, -0.0031, -0.0051],\n",
       "         [-0.0047, -0.0041, -0.0053,  ..., -0.0034, -0.0057, -0.0029],\n",
       "         ...,\n",
       "         [ 0.0020,  0.0028, -0.0022,  ...,  0.0028,  0.0015,  0.0016],\n",
       "         [-0.0011, -0.0016, -0.0043,  ..., -0.0002, -0.0051,  0.0006],\n",
       "         [-0.0016, -0.0009,  0.0066,  ..., -0.0041,  0.0033, -0.0030]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0103,  0.0072,  0.0038,  ..., -0.0150, -0.0015, -0.0012],\n",
       "         [-0.0025, -0.0028, -0.0056,  ...,  0.0091, -0.0136,  0.0120],\n",
       "         [ 0.0060, -0.0012,  0.0046,  ...,  0.0025,  0.0045,  0.0041],\n",
       "         ...,\n",
       "         [ 0.0094, -0.0005,  0.0028,  ...,  0.0046,  0.0022,  0.0160],\n",
       "         [-0.0113,  0.0043,  0.0003,  ...,  0.0168, -0.0105,  0.0185],\n",
       "         [ 0.0103,  0.0058, -0.0069,  ..., -0.0035,  0.0109, -0.0155]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight': tensor([[-1.0503e-03, -1.3640e-03,  3.1966e-03,  ...,  2.1963e-03,\n",
       "          -1.7913e-03, -4.5160e-04],\n",
       "         [ 2.1037e-03,  3.0909e-03, -4.8909e-04,  ..., -4.6162e-04,\n",
       "          -7.9399e-04, -1.1523e-03],\n",
       "         [-5.4813e-03,  3.9085e-03,  2.6142e-03,  ...,  1.3046e-03,\n",
       "          -1.8636e-03, -2.8924e-03],\n",
       "         ...,\n",
       "         [-5.6940e-03,  3.4383e-03,  4.3911e-03,  ...,  3.8671e-03,\n",
       "          -5.8240e-03, -4.7825e-03],\n",
       "         [-2.5035e-03,  8.7852e-04,  4.3055e-03,  ...,  3.1128e-03,\n",
       "           7.8643e-05, -3.7569e-03],\n",
       "         [ 3.2435e-03, -4.2220e-03, -4.9899e-04,  ..., -2.9880e-03,\n",
       "          -7.7847e-04,  1.6399e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight': tensor([[-4.9468e-03,  1.1535e-02,  6.6012e-03,  ..., -1.3074e-02,\n",
       "          -2.3311e-02,  1.0688e-02],\n",
       "         [ 6.2915e-06,  7.4549e-03, -1.9976e-02,  ...,  9.8259e-03,\n",
       "           3.7951e-03,  6.9618e-03],\n",
       "         [-1.4311e-02,  1.5806e-02, -1.0929e-03,  ...,  1.8138e-02,\n",
       "           1.0689e-02, -1.4897e-02],\n",
       "         ...,\n",
       "         [-1.0670e-02, -4.6418e-03, -1.3682e-02,  ...,  8.8730e-04,\n",
       "          -1.2636e-02, -8.9575e-03],\n",
       "         [ 4.9596e-04, -5.8813e-03,  1.3355e-02,  ..., -8.1002e-04,\n",
       "           1.5298e-03, -1.1541e-02],\n",
       "         [-5.1122e-03,  1.1612e-02, -7.0334e-03,  ...,  1.3608e-02,\n",
       "           3.3162e-03, -5.0122e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight': tensor([[ 2.2542e-03, -2.2062e-05,  3.8823e-03,  ...,  7.3190e-04,\n",
       "          -4.3815e-03,  5.1088e-03],\n",
       "         [-4.1716e-03,  4.6429e-03, -5.9216e-03,  ..., -4.1457e-03,\n",
       "           6.9610e-03, -4.5127e-03],\n",
       "         [-8.4343e-03, -4.9906e-03, -2.8322e-03,  ..., -9.1463e-03,\n",
       "           2.9515e-03,  4.2111e-04],\n",
       "         ...,\n",
       "         [ 1.7897e-03,  3.8297e-03, -3.5223e-03,  ...,  7.1543e-04,\n",
       "           3.0715e-03, -2.9613e-03],\n",
       "         [-2.0776e-03,  3.5617e-03, -6.1282e-03,  ..., -3.4001e-03,\n",
       "           7.0568e-03, -5.6593e-03],\n",
       "         [-2.4074e-03, -3.5593e-03,  2.4571e-03,  ..., -2.0342e-04,\n",
       "          -1.1022e-03,  1.1858e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0050,  0.0051,  0.0111,  ..., -0.0090,  0.0058,  0.0181],\n",
       "         [-0.0036, -0.0013,  0.0093,  ..., -0.0115,  0.0179,  0.0071],\n",
       "         [-0.0023,  0.0131,  0.0037,  ..., -0.0105,  0.0074,  0.0061],\n",
       "         ...,\n",
       "         [-0.0129,  0.0125, -0.0134,  ..., -0.0142, -0.0105,  0.0022],\n",
       "         [ 0.0004,  0.0122,  0.0132,  ...,  0.0031, -0.0022,  0.0211],\n",
       "         [-0.0097,  0.0070,  0.0018,  ..., -0.0018,  0.0142, -0.0133]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight': tensor([[ 4.3396e-03, -1.7995e-03,  2.1051e-03,  ..., -9.6292e-04,\n",
       "          -2.0193e-03,  3.0442e-03],\n",
       "         [ 2.7330e-03, -1.9179e-03,  3.0166e-03,  ..., -2.4306e-03,\n",
       "           8.9514e-04,  2.2223e-03],\n",
       "         [-1.0950e-03,  1.8594e-03, -2.3952e-03,  ...,  2.6593e-03,\n",
       "           2.7009e-04, -1.9806e-05],\n",
       "         ...,\n",
       "         [-2.3691e-03,  7.5875e-04,  1.9945e-03,  ..., -4.1084e-03,\n",
       "           5.4851e-04,  4.5441e-04],\n",
       "         [-3.1533e-03, -9.5397e-04, -4.3271e-04,  ...,  3.2879e-03,\n",
       "          -3.1912e-03,  1.3629e-03],\n",
       "         [-3.9501e-03, -1.5406e-03,  2.6419e-04,  ..., -6.2733e-04,\n",
       "          -5.7530e-03, -6.5032e-05]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0169, -0.0097,  0.0049,  ..., -0.0135,  0.0091,  0.0140],\n",
       "         [-0.0184,  0.0096, -0.0170,  ..., -0.0086,  0.0026,  0.0072],\n",
       "         [ 0.0032, -0.0045,  0.0197,  ...,  0.0074, -0.0023, -0.0116],\n",
       "         ...,\n",
       "         [ 0.0124,  0.0008,  0.0005,  ..., -0.0100,  0.0235, -0.0198],\n",
       "         [-0.0022,  0.0018, -0.0067,  ..., -0.0036, -0.0059, -0.0220],\n",
       "         [ 0.0057, -0.0043,  0.0168,  ...,  0.0172, -0.0127, -0.0001]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0080,  0.0039, -0.0037,  ...,  0.0001, -0.0011,  0.0013],\n",
       "         [-0.0058,  0.0052, -0.0042,  ...,  0.0045,  0.0053, -0.0044],\n",
       "         [ 0.0018, -0.0061,  0.0047,  ..., -0.0049, -0.0062,  0.0031],\n",
       "         ...,\n",
       "         [-0.0064, -0.0035,  0.0042,  ..., -0.0070, -0.0053,  0.0077],\n",
       "         [-0.0071,  0.0066, -0.0066,  ...,  0.0005,  0.0038, -0.0040],\n",
       "         [-0.0010, -0.0047,  0.0042,  ..., -0.0043, -0.0049,  0.0066]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0092, -0.0040,  0.0079,  ...,  0.0077, -0.0148,  0.0137],\n",
       "         [-0.0080,  0.0057,  0.0162,  ...,  0.0101,  0.0174,  0.0159],\n",
       "         [-0.0105,  0.0136,  0.0123,  ...,  0.0053, -0.0125,  0.0147],\n",
       "         ...,\n",
       "         [ 0.0068, -0.0108, -0.0119,  ...,  0.0051,  0.0147, -0.0126],\n",
       "         [-0.0093,  0.0162,  0.0118,  ...,  0.0005, -0.0135, -0.0176],\n",
       "         [-0.0013,  0.0094, -0.0046,  ...,  0.0056, -0.0105, -0.0050]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.0005, -0.0012, -0.0041,  ...,  0.0048, -0.0017, -0.0015],\n",
       "         [-0.0010,  0.0028,  0.0031,  ..., -0.0038,  0.0003,  0.0040],\n",
       "         [ 0.0007, -0.0002, -0.0039,  ...,  0.0042, -0.0012, -0.0008],\n",
       "         ...,\n",
       "         [-0.0004,  0.0019, -0.0017,  ..., -0.0029, -0.0008,  0.0004],\n",
       "         [-0.0005, -0.0033,  0.0011,  ...,  0.0002,  0.0005, -0.0024],\n",
       "         [-0.0023, -0.0009,  0.0024,  ..., -0.0009, -0.0017,  0.0008]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0034, -0.0085,  0.0086,  ..., -0.0006, -0.0148,  0.0176],\n",
       "         [ 0.0177, -0.0054, -0.0195,  ..., -0.0065,  0.0088, -0.0067],\n",
       "         [ 0.0140, -0.0040, -0.0080,  ...,  0.0121, -0.0015,  0.0035],\n",
       "         ...,\n",
       "         [-0.0195, -0.0095,  0.0074,  ..., -0.0107, -0.0155,  0.0169],\n",
       "         [-0.0048,  0.0015,  0.0029,  ...,  0.0199, -0.0003, -0.0125],\n",
       "         [-0.0039, -0.0002, -0.0013,  ..., -0.0147, -0.0043, -0.0021]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0006, -0.0021, -0.0014,  ...,  0.0014, -0.0004, -0.0015],\n",
       "         [-0.0026,  0.0027, -0.0015,  ..., -0.0029,  0.0029,  0.0007],\n",
       "         [-0.0041, -0.0061, -0.0048,  ...,  0.0043,  0.0042, -0.0053],\n",
       "         ...,\n",
       "         [-0.0004, -0.0045, -0.0019,  ...,  0.0012, -0.0004, -0.0035],\n",
       "         [-0.0038, -0.0031, -0.0067,  ...,  0.0007,  0.0085, -0.0044],\n",
       "         [-0.0003, -0.0079, -0.0019,  ...,  0.0031, -0.0046, -0.0027]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0025,  0.0016, -0.0023,  ...,  0.0113,  0.0119,  0.0040],\n",
       "         [ 0.0022, -0.0038,  0.0089,  ..., -0.0002,  0.0039, -0.0067],\n",
       "         [-0.0185, -0.0063,  0.0010,  ..., -0.0139,  0.0090, -0.0051],\n",
       "         ...,\n",
       "         [ 0.0058,  0.0048, -0.0122,  ...,  0.0126,  0.0154, -0.0067],\n",
       "         [-0.0009, -0.0065,  0.0178,  ...,  0.0143,  0.0048,  0.0191],\n",
       "         [-0.0153,  0.0007, -0.0134,  ...,  0.0028, -0.0110,  0.0096]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight': tensor([[-3.6698e-03,  1.3859e-03,  9.6857e-04,  ...,  1.0381e-03,\n",
       "          -5.5962e-03, -1.6716e-03],\n",
       "         [ 8.1052e-04, -2.9093e-03,  2.5279e-03,  ...,  2.2863e-03,\n",
       "          -2.3725e-03,  7.6297e-04],\n",
       "         [ 2.8462e-03, -2.9657e-03, -1.3929e-03,  ..., -1.8074e-03,\n",
       "           3.9637e-03,  3.5717e-03],\n",
       "         ...,\n",
       "         [-3.0736e-04, -1.8231e-03,  8.4179e-03,  ...,  1.0985e-03,\n",
       "           2.2250e-03, -3.7943e-04],\n",
       "         [ 6.7359e-04, -1.2877e-03,  3.8860e-05,  ..., -5.7723e-05,\n",
       "           1.6635e-03,  2.3360e-03],\n",
       "         [-2.6859e-03,  1.1515e-03, -6.1703e-03,  ..., -3.4396e-03,\n",
       "          -3.0926e-03,  2.4259e-04]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0140,  0.0202,  0.0056,  ...,  0.0012, -0.0205, -0.0130],\n",
       "         [-0.0021,  0.0021,  0.0175,  ...,  0.0033,  0.0002,  0.0123],\n",
       "         [-0.0047,  0.0018,  0.0111,  ..., -0.0061,  0.0305,  0.0118],\n",
       "         ...,\n",
       "         [-0.0180, -0.0062,  0.0046,  ..., -0.0052,  0.0075, -0.0044],\n",
       "         [-0.0045, -0.0048, -0.0058,  ..., -0.0062,  0.0077,  0.0015],\n",
       "         [-0.0163, -0.0017,  0.0122,  ...,  0.0162, -0.0068,  0.0110]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight': tensor([[-5.7799e-03,  6.6272e-03,  5.4695e-03,  ...,  3.7701e-03,\n",
       "           1.9319e-03, -2.6522e-03],\n",
       "         [-2.7106e-03,  4.8790e-03,  8.8328e-03,  ...,  2.6161e-04,\n",
       "           2.5454e-05, -1.9021e-04],\n",
       "         [ 6.6418e-03, -7.4766e-03, -6.8182e-04,  ..., -4.5584e-03,\n",
       "          -3.5556e-03,  4.0095e-03],\n",
       "         ...,\n",
       "         [ 3.5826e-03, -7.5623e-03, -9.3573e-03,  ...,  6.8659e-03,\n",
       "           7.2477e-03, -7.5018e-03],\n",
       "         [ 5.3111e-03, -7.1394e-04, -5.7631e-04,  ..., -2.3540e-03,\n",
       "          -1.2319e-03,  1.8671e-03],\n",
       "         [ 5.2567e-03, -9.4216e-04,  7.4082e-04,  ..., -6.3334e-04,\n",
       "          -1.0317e-03,  1.4205e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0165, -0.0038, -0.0052,  ..., -0.0049,  0.0106, -0.0057],\n",
       "         [-0.0040, -0.0016, -0.0085,  ..., -0.0032, -0.0115, -0.0085],\n",
       "         [-0.0104, -0.0035, -0.0018,  ...,  0.0062,  0.0167,  0.0103],\n",
       "         ...,\n",
       "         [-0.0015,  0.0086, -0.0192,  ...,  0.0070, -0.0161,  0.0117],\n",
       "         [-0.0180, -0.0069, -0.0099,  ..., -0.0073, -0.0200, -0.0059],\n",
       "         [ 0.0037,  0.0018,  0.0015,  ..., -0.0121, -0.0035, -0.0013]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0024,  0.0005,  0.0014,  ...,  0.0034,  0.0003,  0.0018],\n",
       "         [ 0.0026, -0.0017,  0.0011,  ..., -0.0007, -0.0053, -0.0008],\n",
       "         [-0.0033,  0.0013, -0.0004,  ...,  0.0019,  0.0055, -0.0003],\n",
       "         ...,\n",
       "         [-0.0032,  0.0038,  0.0016,  ...,  0.0001,  0.0006,  0.0023],\n",
       "         [ 0.0031, -0.0038, -0.0045,  ..., -0.0047, -0.0042, -0.0045],\n",
       "         [-0.0048,  0.0050,  0.0046,  ...,  0.0050,  0.0063,  0.0069]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0029, -0.0104,  0.0080,  ..., -0.0032,  0.0187,  0.0064],\n",
       "         [ 0.0099, -0.0179, -0.0094,  ...,  0.0039,  0.0012, -0.0143],\n",
       "         [-0.0043,  0.0078, -0.0080,  ..., -0.0176,  0.0139,  0.0074],\n",
       "         ...,\n",
       "         [-0.0098,  0.0216,  0.0062,  ...,  0.0139,  0.0050,  0.0104],\n",
       "         [ 0.0016,  0.0128, -0.0124,  ..., -0.0204,  0.0064, -0.0095],\n",
       "         [ 0.0101, -0.0024,  0.0048,  ...,  0.0041,  0.0230, -0.0046]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight': tensor([[ 5.5266e-04,  1.3855e-03, -1.5491e-03,  ...,  1.2962e-03,\n",
       "          -1.9758e-03,  2.0544e-03],\n",
       "         [-1.3237e-03, -3.6387e-04, -5.8014e-04,  ...,  7.1834e-04,\n",
       "          -1.5397e-03, -8.1333e-05],\n",
       "         [-5.2883e-03, -5.8741e-03, -7.3548e-03,  ...,  7.3082e-03,\n",
       "          -6.4634e-03, -6.0520e-03],\n",
       "         ...,\n",
       "         [-5.7959e-04, -4.5748e-04,  6.2585e-04,  ..., -7.3942e-04,\n",
       "           7.9382e-04, -3.0559e-03],\n",
       "         [-8.9137e-04, -3.1913e-03, -6.7868e-05,  ...,  2.7287e-03,\n",
       "           1.3835e-03, -5.6904e-04],\n",
       "         [ 3.5819e-03,  1.2694e-03,  3.0463e-03,  ..., -2.9232e-03,\n",
       "           4.6282e-03,  4.5274e-04]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0096, -0.0002, -0.0048,  ...,  0.0171, -0.0137,  0.0076],\n",
       "         [-0.0093, -0.0158, -0.0027,  ...,  0.0139, -0.0109, -0.0056],\n",
       "         [-0.0062, -0.0062, -0.0025,  ..., -0.0067, -0.0128, -0.0086],\n",
       "         ...,\n",
       "         [-0.0002, -0.0119, -0.0082,  ...,  0.0076,  0.0116,  0.0046],\n",
       "         [-0.0081,  0.0178, -0.0115,  ..., -0.0092, -0.0010, -0.0062],\n",
       "         [ 0.0162, -0.0020,  0.0084,  ..., -0.0051, -0.0089,  0.0006]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight': tensor([[-2.2395e-04, -6.2471e-04,  2.0527e-03,  ...,  1.6330e-03,\n",
       "          -1.4911e-03, -3.7406e-04],\n",
       "         [-1.4932e-03,  1.7841e-03, -5.0999e-03,  ...,  3.1831e-04,\n",
       "          -6.9806e-04, -3.7320e-03],\n",
       "         [-2.9566e-03, -2.9654e-04,  6.3182e-04,  ...,  6.2686e-04,\n",
       "          -5.2142e-04, -4.4377e-04],\n",
       "         ...,\n",
       "         [ 9.0694e-05, -2.6882e-03,  1.9128e-03,  ..., -1.2733e-03,\n",
       "           3.5800e-03,  4.1506e-03],\n",
       "         [-1.0562e-03,  2.7333e-03,  9.5646e-04,  ...,  7.3211e-04,\n",
       "           1.4216e-03, -1.1728e-03],\n",
       "         [-8.4212e-04,  1.8699e-03, -9.4682e-04,  ...,  5.1225e-04,\n",
       "           4.3577e-04, -3.3107e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0015, -0.0184, -0.0184,  ..., -0.0044, -0.0110,  0.0013],\n",
       "         [-0.0102, -0.0027,  0.0046,  ..., -0.0086, -0.0126,  0.0024],\n",
       "         [-0.0189,  0.0088, -0.0023,  ..., -0.0134, -0.0043, -0.0070],\n",
       "         ...,\n",
       "         [ 0.0111, -0.0038,  0.0135,  ...,  0.0154,  0.0088, -0.0064],\n",
       "         [ 0.0012,  0.0107,  0.0171,  ..., -0.0001, -0.0068, -0.0013],\n",
       "         [ 0.0156,  0.0090,  0.0022,  ..., -0.0040, -0.0261, -0.0003]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0063, -0.0071, -0.0040,  ...,  0.0021,  0.0055, -0.0063],\n",
       "         [ 0.0067, -0.0011,  0.0065,  ..., -0.0068, -0.0060,  0.0005],\n",
       "         [ 0.0090,  0.0086,  0.0064,  ..., -0.0061, -0.0079,  0.0120],\n",
       "         ...,\n",
       "         [ 0.0002, -0.0028,  0.0046,  ..., -0.0035, -0.0007, -0.0025],\n",
       "         [-0.0037,  0.0031, -0.0039,  ...,  0.0038,  0.0029,  0.0024],\n",
       "         [ 0.0048, -0.0012,  0.0041,  ..., -0.0035, -0.0046, -0.0016]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0095,  0.0035,  0.0204,  ..., -0.0006, -0.0080, -0.0014],\n",
       "         [ 0.0094, -0.0175, -0.0025,  ..., -0.0093, -0.0081,  0.0016],\n",
       "         [ 0.0105,  0.0080, -0.0017,  ...,  0.0176,  0.0118, -0.0197],\n",
       "         ...,\n",
       "         [ 0.0047, -0.0043,  0.0088,  ...,  0.0021,  0.0059, -0.0121],\n",
       "         [ 0.0017,  0.0082,  0.0060,  ..., -0.0075,  0.0141,  0.0115],\n",
       "         [ 0.0070,  0.0089, -0.0181,  ..., -0.0100, -0.0019,  0.0098]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0055,  0.0036,  0.0040,  ...,  0.0050,  0.0048,  0.0051],\n",
       "         [-0.0034,  0.0019,  0.0011,  ...,  0.0048,  0.0018,  0.0040],\n",
       "         [-0.0067,  0.0034,  0.0032,  ...,  0.0054,  0.0031,  0.0039],\n",
       "         ...,\n",
       "         [ 0.0038, -0.0071, -0.0044,  ..., -0.0053, -0.0043, -0.0042],\n",
       "         [-0.0022,  0.0029,  0.0025,  ...,  0.0024,  0.0045,  0.0040],\n",
       "         [-0.0058,  0.0057,  0.0065,  ...,  0.0067,  0.0036,  0.0041]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0008,  0.0185,  0.0008,  ...,  0.0027,  0.0085,  0.0060],\n",
       "         [-0.0099, -0.0186,  0.0057,  ..., -0.0038,  0.0107, -0.0020],\n",
       "         [-0.0079, -0.0166, -0.0052,  ...,  0.0097,  0.0198, -0.0086],\n",
       "         ...,\n",
       "         [-0.0043, -0.0088, -0.0069,  ...,  0.0103,  0.0086,  0.0167],\n",
       "         [-0.0057,  0.0075, -0.0064,  ..., -0.0157, -0.0268,  0.0017],\n",
       "         [-0.0156, -0.0123, -0.0034,  ...,  0.0021,  0.0154, -0.0086]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0029, -0.0040, -0.0034,  ...,  0.0046,  0.0005, -0.0018],\n",
       "         [-0.0044,  0.0042, -0.0013,  ..., -0.0005,  0.0022, -0.0022],\n",
       "         [ 0.0014, -0.0012, -0.0067,  ...,  0.0036,  0.0101, -0.0095],\n",
       "         ...,\n",
       "         [ 0.0020, -0.0025, -0.0081,  ...,  0.0073,  0.0056, -0.0071],\n",
       "         [-0.0041,  0.0045,  0.0012,  ..., -0.0038,  0.0037, -0.0040],\n",
       "         [-0.0044,  0.0057, -0.0089,  ..., -0.0004,  0.0122, -0.0120]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0110,  0.0055,  0.0162,  ...,  0.0023,  0.0164, -0.0040],\n",
       "         [ 0.0070, -0.0171,  0.0018,  ...,  0.0077,  0.0046,  0.0037],\n",
       "         [ 0.0081, -0.0155, -0.0027,  ...,  0.0095, -0.0049,  0.0003],\n",
       "         ...,\n",
       "         [ 0.0147,  0.0090,  0.0085,  ...,  0.0096,  0.0098, -0.0075],\n",
       "         [-0.0138, -0.0021,  0.0056,  ...,  0.0091, -0.0095, -0.0133],\n",
       "         [-0.0031,  0.0055, -0.0103,  ...,  0.0126, -0.0167,  0.0128]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0050, -0.0021,  0.0018,  ...,  0.0029, -0.0010,  0.0022],\n",
       "         [-0.0029, -0.0052,  0.0046,  ...,  0.0047, -0.0055,  0.0033],\n",
       "         [-0.0046, -0.0049,  0.0053,  ...,  0.0050, -0.0061,  0.0040],\n",
       "         ...,\n",
       "         [-0.0050,  0.0054,  0.0047,  ..., -0.0059,  0.0045, -0.0044],\n",
       "         [ 0.0045, -0.0043, -0.0046,  ...,  0.0050, -0.0034,  0.0044],\n",
       "         [-0.0030,  0.0020,  0.0054,  ..., -0.0026,  0.0043, -0.0012]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight': tensor([[-1.0696e-02, -2.4287e-03, -7.1237e-03,  ...,  3.8542e-04,\n",
       "           6.1052e-03, -8.8804e-04],\n",
       "         [ 1.4766e-03, -5.1406e-03, -4.4315e-03,  ...,  1.2249e-02,\n",
       "          -1.5378e-02, -6.6623e-03],\n",
       "         [ 1.3272e-02,  1.7633e-02,  3.7823e-05,  ...,  7.1124e-03,\n",
       "           4.3080e-03,  1.3590e-02],\n",
       "         ...,\n",
       "         [ 2.3403e-03, -9.6696e-03, -1.7095e-02,  ..., -1.2321e-02,\n",
       "          -1.7520e-02, -6.4330e-03],\n",
       "         [ 2.1087e-02,  4.9618e-03, -2.2970e-02,  ...,  2.9684e-03,\n",
       "           1.4538e-02,  4.4308e-04],\n",
       "         [ 1.8325e-02, -7.8526e-03,  4.9181e-03,  ..., -1.7439e-02,\n",
       "          -1.6310e-03,  9.5122e-03]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight': tensor([[ 1.4138e-03,  2.4571e-03,  2.5817e-03,  ..., -1.7543e-03,\n",
       "          -1.9900e-03, -1.8392e-03],\n",
       "         [ 4.0860e-03,  2.5318e-03,  1.8966e-03,  ...,  5.6920e-04,\n",
       "           1.1224e-03,  2.1385e-04],\n",
       "         [ 1.7751e-03,  1.4283e-03,  1.4163e-03,  ..., -2.8318e-03,\n",
       "           2.6765e-03,  2.8931e-03],\n",
       "         ...,\n",
       "         [ 1.5063e-03,  3.9753e-03,  4.4665e-03,  ..., -6.0221e-05,\n",
       "          -8.0059e-03, -7.4585e-03],\n",
       "         [ 3.2649e-03,  4.2319e-03,  6.2394e-03,  ..., -1.4400e-03,\n",
       "          -6.4545e-03, -5.7102e-03],\n",
       "         [-5.6424e-03, -1.5425e-03, -4.5832e-03,  ...,  4.7258e-03,\n",
       "          -1.0691e-03,  7.0148e-05]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0061, -0.0032,  0.0095,  ...,  0.0140, -0.0044, -0.0092],\n",
       "         [-0.0088, -0.0093,  0.0080,  ..., -0.0131, -0.0077, -0.0114],\n",
       "         [ 0.0191,  0.0063, -0.0011,  ..., -0.0012, -0.0051,  0.0142],\n",
       "         ...,\n",
       "         [-0.0123,  0.0127, -0.0049,  ..., -0.0171,  0.0102,  0.0067],\n",
       "         [ 0.0094,  0.0091,  0.0044,  ...,  0.0013, -0.0054, -0.0121],\n",
       "         [-0.0072, -0.0123, -0.0058,  ...,  0.0087,  0.0049,  0.0078]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0044,  0.0046, -0.0064,  ...,  0.0027, -0.0010,  0.0032],\n",
       "         [ 0.0018,  0.0002,  0.0031,  ..., -0.0036,  0.0023, -0.0014],\n",
       "         [ 0.0030, -0.0047,  0.0066,  ..., -0.0013,  0.0012, -0.0038],\n",
       "         ...,\n",
       "         [-0.0029, -0.0035,  0.0036,  ...,  0.0037,  0.0029, -0.0047],\n",
       "         [-0.0058,  0.0058, -0.0034,  ...,  0.0067, -0.0020,  0.0037],\n",
       "         [-0.0032,  0.0036, -0.0017,  ...,  0.0037, -0.0017,  0.0069]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight': tensor([[-1.2849e-02,  9.7545e-03, -9.1570e-03,  ..., -3.2442e-03,\n",
       "          -8.6269e-03,  4.2244e-03],\n",
       "         [-1.2526e-02, -3.0117e-03,  1.1357e-02,  ..., -2.9033e-03,\n",
       "          -1.5325e-03, -1.9868e-02],\n",
       "         [-2.8995e-03, -3.9201e-03,  1.4352e-02,  ...,  4.5405e-03,\n",
       "          -8.9383e-03,  1.3279e-03],\n",
       "         ...,\n",
       "         [-1.5803e-02, -1.1558e-03,  9.0816e-03,  ...,  6.2159e-03,\n",
       "           1.2125e-02, -1.0497e-02],\n",
       "         [ 5.0885e-03,  9.3040e-03,  5.3991e-04,  ...,  7.0685e-05,\n",
       "           1.3868e-02, -2.7514e-04],\n",
       "         [-1.4721e-02,  5.9085e-03, -4.2536e-03,  ..., -4.7210e-03,\n",
       "          -1.6487e-02,  1.6784e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0050,  0.0015, -0.0040,  ...,  0.0034, -0.0040, -0.0044],\n",
       "         [-0.0087,  0.0090, -0.0034,  ...,  0.0074, -0.0017, -0.0075],\n",
       "         [ 0.0001, -0.0036, -0.0078,  ..., -0.0015, -0.0052, -0.0024],\n",
       "         ...,\n",
       "         [ 0.0055, -0.0068,  0.0013,  ..., -0.0101, -0.0039,  0.0065],\n",
       "         [-0.0024,  0.0072,  0.0099,  ..., -0.0024,  0.0093,  0.0035],\n",
       "         [ 0.0078, -0.0083, -0.0112,  ..., -0.0019, -0.0120,  0.0014]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0097,  0.0119,  0.0053,  ...,  0.0082,  0.0139, -0.0093],\n",
       "         [ 0.0086,  0.0058,  0.0075,  ...,  0.0088, -0.0209,  0.0157],\n",
       "         [-0.0023, -0.0020, -0.0028,  ...,  0.0095, -0.0061,  0.0002],\n",
       "         ...,\n",
       "         [ 0.0046, -0.0002,  0.0024,  ...,  0.0147, -0.0034,  0.0191],\n",
       "         [ 0.0180, -0.0118,  0.0048,  ...,  0.0004, -0.0062,  0.0170],\n",
       "         [ 0.0117,  0.0026,  0.0127,  ..., -0.0126,  0.0039,  0.0143]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.0052, -0.0063, -0.0016,  ...,  0.0035,  0.0046, -0.0044],\n",
       "         [-0.0050,  0.0052,  0.0034,  ..., -0.0033, -0.0052,  0.0042],\n",
       "         [ 0.0030, -0.0050, -0.0045,  ...,  0.0015,  0.0048, -0.0047],\n",
       "         ...,\n",
       "         [-0.0051,  0.0041, -0.0020,  ..., -0.0061, -0.0043,  0.0048],\n",
       "         [ 0.0018, -0.0026, -0.0014,  ...,  0.0058,  0.0075, -0.0019],\n",
       "         [-0.0067,  0.0073, -0.0033,  ..., -0.0057, -0.0030,  0.0063]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0049,  0.0204,  0.0097,  ...,  0.0079,  0.0020, -0.0127],\n",
       "         [ 0.0053, -0.0141, -0.0072,  ...,  0.0043,  0.0050, -0.0145],\n",
       "         [ 0.0250,  0.0103, -0.0100,  ..., -0.0106,  0.0017,  0.0175],\n",
       "         ...,\n",
       "         [ 0.0180,  0.0171, -0.0026,  ..., -0.0040,  0.0156,  0.0027],\n",
       "         [-0.0243, -0.0120, -0.0103,  ...,  0.0160,  0.0009,  0.0027],\n",
       "         [ 0.0139, -0.0053,  0.0042,  ..., -0.0012, -0.0066,  0.0083]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0022, -0.0019, -0.0002,  ...,  0.0039,  0.0027,  0.0013],\n",
       "         [-0.0079,  0.0079, -0.0114,  ..., -0.0107,  0.0074, -0.0046],\n",
       "         [-0.0098,  0.0096, -0.0089,  ..., -0.0117,  0.0101, -0.0097],\n",
       "         ...,\n",
       "         [-0.0007, -0.0002,  0.0085,  ...,  0.0029, -0.0015, -0.0019],\n",
       "         [ 0.0048, -0.0041,  0.0006,  ...,  0.0030, -0.0031,  0.0048],\n",
       "         [-0.0099,  0.0094, -0.0113,  ..., -0.0118,  0.0106, -0.0091]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_posterior_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234530c-a2c0-4698-b7f2-8a9e390585de",
   "metadata": {},
   "source": [
    "### Task 2: QA+QG EVCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e0bd22e-e909-48d8-ac98-23e524827601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from torch.optim import AdamW\n",
    "import torch.cuda.amp as amp\n",
    "from transformers import get_scheduler\n",
    "from pyro.optim import ExponentialLR\n",
    "evaluation_loss=[]\n",
    "\n",
    "def run_lora_evcl_2(\n",
    "    combined_loader,  \n",
    "    eval_loader,\n",
    "    num_epochs: int = 100,\n",
    "    batch_size: int = 2,\n",
    "    learning_rate: float = 1e-5,\n",
    "    logging_steps: int = 100,\n",
    "    eval_steps: int = 200,\n",
    "    save_steps: int = 500,\n",
    "    output_dir: str = \"finetuned-weights-LoRA-EVCL-Final-Task2\",\n",
    "    load_pyro: bool = False,\n",
    "    best_output_dir=\"finetuned-weights-LoRA-EVCL-Final-Task2_EVCL_best\",\n",
    "    prev_fisher_info: dict = None,            \n",
    "    prev_posterior_means: dict = None,        \n",
    "    ewc_lambda: float = 0.0,                  \n",
    "    synthetic_data_loader=None,               \n",
    "    tokenizer=None,\n",
    "    model=None\n",
    "):\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Ensure all parameters require gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False  # Freeze non-LoRA parameters\n",
    "\n",
    "    def bayesian_guide(input_ids, attention_mask, labels):\n",
    "        # Define variational distributions over the LoRA parameters\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, 'lora_A'):\n",
    "                for key in module.lora_A:\n",
    "                    param_name = f\"{name}.lora_A.{key}\"\n",
    "                    lora_A_param = module.lora_A[key].weight\n",
    "                    device = lora_A_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_A_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_A_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_A_param.dim())\n",
    "                    )\n",
    "            if hasattr(module, 'lora_B'):\n",
    "                for key in module.lora_B:\n",
    "                    param_name = f\"{name}.lora_B.{key}\"\n",
    "                    lora_B_param = module.lora_B[key].weight\n",
    "                    device = lora_B_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_B_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_B_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_B_param.dim())\n",
    "                    )\n",
    "                        \n",
    "    def bayesian_model(input_ids, attention_mask, labels):\n",
    "        # pyro.module(\"model\", model)  # Removed\n",
    "\n",
    "        # Define a function to sample and substitute LoRA parameters\n",
    "        def model_with_sampled_lora():\n",
    "            # Sample LoRA parameters and set them in the model\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, 'lora_A'):\n",
    "                    for key in module.lora_A:\n",
    "                        param_name = f\"{name}.lora_A.{key}\"\n",
    "                        lora_A_module = module.lora_A[key]\n",
    "                        device = lora_A_module.weight.device\n",
    "\n",
    "                        # Use posterior mean from Task 1 as prior mean\n",
    "                        prior_mean = prev_posterior_means.get(param_name, lora_A_module.weight.detach().clone()).to(device)\n",
    "                        prior_std = (0.1 * torch.ones_like(lora_A_module.weight)).to(device)\n",
    "\n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                prior_mean,\n",
    "                                prior_std\n",
    "                            ).to_event(lora_A_module.weight.dim())\n",
    "                        )\n",
    "\n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_A_module.weight.copy_(sampled_weight)\n",
    "\n",
    "                if hasattr(module, 'lora_B'):\n",
    "                    for key in module.lora_B:\n",
    "                        param_name = f\"{name}.lora_B.{key}\"\n",
    "                        lora_B_module = module.lora_B[key]\n",
    "                        device = lora_B_module.weight.device\n",
    "\n",
    "                        # Use posterior mean from Task 1 as prior mean\n",
    "                        prior_mean = prev_posterior_means.get(param_name, lora_B_module.weight.detach().clone()).to(device)\n",
    "                        prior_std = (0.1 * torch.ones_like(lora_B_module.weight)).to(device)\n",
    "\n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                prior_mean,\n",
    "                                prior_std\n",
    "                            ).to_event(lora_B_module.weight.dim())\n",
    "                        )\n",
    "\n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_B_module.weight.copy_(sampled_weight)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Add EWC penalty if previous Fisher info and posterior means are provided\n",
    "            if prev_fisher_info is not None and prev_posterior_means is not None and ewc_lambda > 0.0:\n",
    "                ewc_penalty = 0.0\n",
    "                for name, param in model.named_parameters():\n",
    "                    if 'lora' in name and name in prev_fisher_info:\n",
    "                        fisher = prev_fisher_info[name].to(DEVICE)\n",
    "                        prev_mean = prev_posterior_means[name].to(DEVICE)\n",
    "                        ewc_penalty += (fisher * (param - prev_mean) ** 2).sum()\n",
    "                        # print('penalty of ewc')\n",
    "                        # print(ewc_penalty)\n",
    "                loss += ewc_lambda * ewc_penalty\n",
    "\n",
    "            return loss\n",
    "\n",
    "        # Use the modified model with sampled LoRA parameters\n",
    "        return model_with_sampled_lora()\n",
    "\n",
    "    # Set up SVI\n",
    "    if load_pyro:\n",
    "        print('using previous pyro params')\n",
    "        pyro.get_param_store().load('pyro_param_store_task1_vcl_best.pt')\n",
    "    else:\n",
    "        print('not using previous pyro params')\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "    optim = pyro.optim.PyroOptim(AdamW, {\"lr\": learning_rate, \"weight_decay\": 1e-5})\n",
    "  \n",
    "    scheduler = ExponentialLR({'optimizer': AdamW, 'optim_args': {'lr': learning_rate}, 'gamma': 0.1})\n",
    "    elbo = TraceMeanField_ELBO()\n",
    "    svi = SVI(bayesian_model, bayesian_guide, scheduler, loss=elbo)\n",
    "    evaluation_loss=[]\n",
    "\n",
    "    # optim = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "    # elbo = TraceMeanField_ELBO()\n",
    "    # svi = SVI(bayesian_model, bayesian_guide, optim, loss=elbo)\n",
    "\n",
    "    # Training loop\n",
    "    max_wait=10\n",
    "    best_eval_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "    print(\"Training on new task with EWC and synthetic data from previous task...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for num_batches, batch in enumerate(combined_loader, 1):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            loss = svi.step(input_ids, attention_mask, labels)\n",
    "            total_loss += loss\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            scheduler.step()\n",
    "\n",
    "            # Logging\n",
    "            if num_batches % logging_steps == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch {epoch + 1}, Step {num_batches}, Loss: {avg_loss}\")\n",
    "\n",
    "            # Evaluation\n",
    "            if num_batches % eval_steps == 0:\n",
    "                eval_loss=evaluate_model(model, eval_loader)\n",
    "                evaluation_loss.append(eval_loss)\n",
    "\n",
    "            # Save checkpoints\n",
    "            # if num_batches % save_steps == 0:\n",
    "            #     save_trained_model(model, tokenizer, output_dir)\n",
    "\n",
    "        avg_epoch_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_epoch_loss}\")\n",
    "\n",
    "\n",
    "        if eval_loss<best_eval_loss:\n",
    "            best_eval_loss=eval_loss\n",
    "            no_improvement=0\n",
    "            save_trained_model(model, tokenizer, best_output_dir)\n",
    "            pyro.get_param_store().save('pyro_param_store_task-test2_vcl_best.pt')\n",
    "        else:\n",
    "            no_improvement+=1\n",
    "\n",
    "        if no_improvement>=max_wait and epoch>=50:\n",
    "            print(f'early stopping at epoch: {epoch}')\n",
    "            break\n",
    "\n",
    "    # Save the final trained model after the task\n",
    "    save_trained_model(model, tokenizer, output_dir)\n",
    "    pyro.get_param_store().save('pyro_param_store_task-test2.pt')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dcdfc7d-7765-4548-aeef-6c7032d5961c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17951abb0e748f7bbe1c6442b0ccaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav24/cs-546-project/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/pranav24/cs-546-project/SSR/Latest_Weights/QA_QG_Weights')\n",
    "target_file = \"task074_squad1.1_question_generation.json\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "instances = json_data['Instances'][0:2500]\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"output\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"attention_mask\"] = model_inputs.get(\"attention_mask\", None)\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization and set format\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Split dataset into train and eval\n",
    "train_size = int(0.8 * len(tokenized_datasets))\n",
    "train_dataset = tokenized_datasets.select(range(train_size))\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8  \n",
    "train_loader_2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader_2 = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1370b9d7-8f04-4a2c-a48f-f6f0b8500c56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: json_repair in /opt/conda/lib/python3.11/site-packages (0.30.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install json_repair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd52b9-8cc9-4709-aa6d-7342da74ecb5",
   "metadata": {},
   "source": [
    "#### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65bd9af2-9f5c-45b4-b1ba-7e8ea4c2ee50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a439bbe76748a79da1b930d27926fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json_repair \n",
    "os.chdir('/home/pranav24/cs-546-project/SSR/Synthethic_Data_Generation')\n",
    "target_file = \"qa.train.final_sampled.jsonl\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json_repair.loads(f.read())\n",
    "\n",
    "instances = json_data\n",
    "input_texts = [str(\"\\n\\nContext: \"+ instance['input'].split(\"\\n\\nContext:\")[-1].strip()) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_size = int(1.0 * len(tokenized_datasets))\n",
    "synthetic_train_dataset = tokenized_datasets.select(range(train_size))\n",
    "batch_size = 8  \n",
    "synthetic_loader_1 = DataLoader(synthetic_train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98e0db01-be03-419b-abde-11df8e635b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pranav24/cs-546-project/SSR/Synthethic_Data_Generation\n",
      "/home/pranav24/cs-546-project\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('/home/pranav24/cs-546-project/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8ad22aa-217f-4d30-9ef2-c882368a26f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined dataloader\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "# Combine datasets\n",
    "if synthetic_loader_1 is not None:\n",
    "    print('combined dataloader')\n",
    "    combined_dataset = ConcatDataset([train_loader_2.dataset, synthetic_loader_1.dataset])\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
    "else:\n",
    "    print('not combined dataloader')\n",
    "    combined_loader = train_loader_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5211d71-2108-4c2e-b790-758068ced9f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using previous pyro params\n",
      "Training on new task with EWC and synthetic data from previous task...\n",
      "penalty of ewc\n",
      "tensor(0.0196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(1.6195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(3.5828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(3.5857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(3.5896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(5.4967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(6.5233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(6.5260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(6.5313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(7.7422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(8.5372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(8.5398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(8.5432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.0268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.7794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.7801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.7813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(10.0753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.0346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.0349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.0351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.5057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.0078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.0082, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.0087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.1807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.6812, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.6814, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.6816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.9697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.3372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.3373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.3375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.6593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.0056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.0057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.0060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.1683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.4435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.4437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.4438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.6061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.7475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.7477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.7479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.9007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.2117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.2118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.2118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.3352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.5566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.5568, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.5568, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.7483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.9571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.9574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.9578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.0949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.1607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.1608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.1609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.3892, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.8759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.8759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.8760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.2006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.7414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.7414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.7416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.0252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.3584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.3588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.3589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.6020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.7529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.7529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.7529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.8983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.0891, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.0892, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.0892, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.1866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.2600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.2600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.2601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.3312, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.6798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.6799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.6800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.7196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.9085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.9086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.9088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.2527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.5345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.5348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.5349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.7461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.3482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.5980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.5980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.5980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.7354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.2169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.2170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.2172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.3750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.1432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.1435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.1438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.3204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.6244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.6245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.6245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.0427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.2331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.2332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.2332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.4801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.8598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.8598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.8598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.3178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.7499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(0.0149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(5.2521, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(7.6421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(7.6449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(7.6488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.4486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(10.4724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(10.4755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(10.4804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.6337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.4319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.4344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.4378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.9418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.7143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.7149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.7161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.0192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.9694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.9697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.9699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.4476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.9228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.9232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.9236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.1002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.5905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.5907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.5909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.8766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.2431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.2431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.2433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.5449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.8525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.8526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.8528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.0196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.3135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.3137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.3138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.4676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.6116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.6118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.6120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.7554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.0726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.0726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.0727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.1936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.4110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.4111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.4111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.6014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.8271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.8273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.8277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.9603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.0238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.0239, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.0240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.2634, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.7048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.7049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.7049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.0443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.5480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.5481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.5482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.8318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.1552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.1555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.1556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.4053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.5499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.5499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.5499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.6929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.8656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.8656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.8657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.9658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.0457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.0457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.0457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.1144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.4370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.4372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.4373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.4771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.6926, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.6927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.6929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.0552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.3280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.3283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.3284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.5416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.8136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.8137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.8137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.1554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.3948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.3948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.3949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.5315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.9975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.9976, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.9978, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.1564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.9294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.9297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.9300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.1036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.4200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.4200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.4201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.8286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(28.0354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(28.0354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(28.0355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(28.2843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(28.6351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(28.6351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(28.6351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(29.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(29.5414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(0.0183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(2.2775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(4.2942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(4.2971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(4.3010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(6.2134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(7.2346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(7.2376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(7.2426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(8.4000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.1799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.1824, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.1860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.6739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(10.4427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(10.4433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(10.4445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(10.7501, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.6710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.6713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.6715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.1624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.6736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.6740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.6745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.8523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.3716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.3718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.3719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.6636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.0926, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.0927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.0929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.3930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.7273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.7274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.7277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.8940, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.1722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.1723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.1725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.3287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.4756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.4757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.4759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.6231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.9673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.9674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.9674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.0884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.2978, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.2980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.2980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.4898, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.7274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.7276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.7279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.8629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.9288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.9289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.9289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.1611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.6545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.6546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.6546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.9899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.5366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.5367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.5368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.8170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.1485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.1488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.1489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.4012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.5494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.5494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.5494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.6981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.8839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.8840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.8840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.9783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.0526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.0526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.0526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.1251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.4612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.4613, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.4614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.5005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.6955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.6956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.6958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.0396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.3414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.3416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.3417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.5430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.7934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.7934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.7935, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.1401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.3918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.3919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.3919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.5272, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.9963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.9965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.9967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.1573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.8869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.8872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.8875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.0674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.3804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.3805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.3805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.7999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.9880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.9881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.9881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.2288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.5752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.5752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.5752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.4802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(0.0221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(3.9619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(5.8616, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(5.8646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(5.8684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(7.9548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.0354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.0381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.0430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(10.1711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.0491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.0517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.5724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.3275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.3281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.3294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.6218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.5167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.5170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.5172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.9869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.5019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.5023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.5028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.6929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.2148, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.2150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.2152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.5041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.8864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.8865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.8867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.1928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.5074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.5075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.5077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.6724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.0082, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.0084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.0085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.1620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.3032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.3034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.3035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.4563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.7803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.7803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.7804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.9023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.1260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.1262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.1262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.3151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.5527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.5529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.5533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.6883, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.7534, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.7535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.7535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.9854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.4570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.4571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.4571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.7807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.3106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.3107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.3108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.5938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.9396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.9400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.9401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.1868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.3194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.3194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.3194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.4674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.6445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.6446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.6446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.7452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.8281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.8281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.8282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.8971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.2167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.2168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.2169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.2581, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.4551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.4553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.4554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.8111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.0986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.0989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.0990, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.3174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.5740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.5740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.5741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.9264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.1930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.1930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.1930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.3276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.8296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.8298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.8300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.9923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.7768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.7770, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.7774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.9628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.2939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.2940, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.2940, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.7201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.8984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.8984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.8985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.1319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.4895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.4895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.4895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.9364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(28.3888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(0.0240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(4.6754, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(6.5871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(6.5904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(6.5941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(8.5505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.5588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.5618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.5666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(10.8053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.5710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.5736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.5772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.7865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.7872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.7885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.0903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.0493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.0496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.0498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.5038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.9942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.9947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.9951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.1722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.6445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.6447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.6449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.9360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.3243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.3244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.3245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.6448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.9822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.9823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.9825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.1546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.4168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.4170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.4171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.5682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.7075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.7077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.7079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.8609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.1984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.1984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.1985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.3195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.5367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.5368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.5369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.7245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.9489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.9491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.9495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.0859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.1517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.1518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.1519, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.3795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.8355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.8355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.8356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.1673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.6302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.6302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.6304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.9093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.2352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.2355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.2356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.4804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.6358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.6358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.6358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.7823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.9712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.9713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.9713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.0707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.1448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.1449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.1449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.2167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.5635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.5637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.5638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.6044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.7983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.7984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.7986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.1367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.4211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.4214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.4215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.6389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.8930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.8930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.8931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.2300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.4714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.4714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.4715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.6070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.0884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.0885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.0887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.2439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.0522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.0524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.0528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.2299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.5617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.5618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.5618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(26.9839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.1764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.1765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.1766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.4238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.7832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.7832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(27.7832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(28.2348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(28.6868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(0.0251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(1.2438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(3.3158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(3.3186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(3.3222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(5.6233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(6.6526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(6.6561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(6.6610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(7.8456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(8.6529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(8.6555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(8.6588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.1601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.9234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.9241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(9.9253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(10.2224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.2088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.2091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.2093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(11.6921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.1869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.1873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.1878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.3630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.8624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.8626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(12.8628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.1403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.5189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.5189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.5191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(13.8301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.1370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.1372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.1374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.3043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.6148, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.6149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.6150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.7701, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.9199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.9201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(14.9202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.0642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.4158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.4158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.4159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.5412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.7586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.7587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.7588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(15.9497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.1801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.1803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.1807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.3189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.3863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.3864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.3864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(16.6171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.4286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.9279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.9280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(17.9281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.1984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.5488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.5491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.5492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.8076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.9552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.9553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(18.9553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.1043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.2902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.2903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.2903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.3836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.4627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.4627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.4628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.5321, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.8604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.8605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.8606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(19.9000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.0943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.0944, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.0946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.4409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.7430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.7432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.7433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(20.9525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.2112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.2113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.2113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.5434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.7779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.7779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.7779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(21.9109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.3693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.3695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.3696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(22.5319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.2977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.2980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.2983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.4812, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.8155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.8156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(23.8156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.2349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.4295, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.4296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.4296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(24.6682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.0206, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.0206, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.0206, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.4722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "penalty of ewc\n",
      "tensor(25.9162, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ewc_lambda \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50.0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model_task_2\u001b[38;5;241m=\u001b[39m\u001b[43mrun_lora_evcl_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombined_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinetuned-weights-LoRA-EVCL-Task-Test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_pyro\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinetuned-weights-LoRA-EVCL-Final-Task-Test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_fisher_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfisher_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_posterior_means\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_posterior_means\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mewc_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mewc_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynthetic_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynthetic_loader_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 196\u001b[0m, in \u001b[0;36mrun_lora_evcl_2\u001b[0;34m(combined_loader, eval_loader, num_epochs, batch_size, learning_rate, logging_steps, eval_steps, save_steps, output_dir, load_pyro, best_output_dir, prev_fisher_info, prev_posterior_means, ewc_lambda, synthetic_data_loader, tokenizer, model)\u001b[0m\n\u001b[1;32m    193\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    194\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m--> 196\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43msvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    199\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m)\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/pyro/infer/svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[0;32m--> 145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    148\u001b[0m     site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/pyro/infer/trace_elbo.py:140\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# grab a trace from the generator\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide_trace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_traces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_particle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurrogate_loss_particle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_differentiable_loss_particle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide_trace\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_particle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_particles\u001b[49m\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/pyro/infer/elbo.py:237\u001b[0m, in \u001b[0;36mELBO._get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles):\n\u001b[0;32m--> 237\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/pyro/infer/trace_mean_field_elbo.py:82\u001b[0m, in \u001b[0;36mTraceMeanField_ELBO._get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, guide, args, kwargs):\n\u001b[0;32m---> 82\u001b[0m     model_trace, guide_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     84\u001b[0m         _check_mean_field_requirement(model_trace, guide_trace)\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/pyro/infer/trace_elbo.py:57\u001b[0m, in \u001b[0;36mTrace_ELBO._get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, guide, args, kwargs):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    Returns a single trace from the guide, and the model that is run\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    against it.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     model_trace, guide_trace \u001b[38;5;241m=\u001b[39m \u001b[43mget_importance_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_plate_nesting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     61\u001b[0m         check_if_enumerated(guide_trace)\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/pyro/infer/enum.py:76\u001b[0m, in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     73\u001b[0m model_trace \u001b[38;5;241m=\u001b[39m prune_subsample_sites(model_trace)\n\u001b[1;32m     75\u001b[0m model_trace\u001b[38;5;241m.\u001b[39mcompute_log_prob()\n\u001b[0;32m---> 76\u001b[0m \u001b[43mguide_trace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_score_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m model_trace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/pyro/poutine/trace_struct.py:304\u001b[0m, in \u001b[0;36mTrace.compute_score_parts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# Note that ScoreParts overloads the multiplication operator\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# to correctly scale each of its three parts.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_parts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    308\u001b[0m     _, exc_value, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/pyro/distributions/distribution.py:115\u001b[0m, in \u001b[0;36mDistribution.score_parts\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore_parts\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    Computes ingredients for stochastic gradient estimators of ELBO.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    :rtype: ScoreParts\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_rsample:\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ScoreParts(\n\u001b[1;32m    118\u001b[0m             log_prob\u001b[38;5;241m=\u001b[39mlog_prob, score_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, entropy_term\u001b[38;5;241m=\u001b[39mlog_prob\n\u001b[1;32m    119\u001b[0m         )\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/torch/distributions/independent.py:110\u001b[0m, in \u001b[0;36mIndependent.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m--> 110\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sum_rightmost(log_prob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinterpreted_batch_ndims)\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/torch/distributions/normal.py:91\u001b[0m, in \u001b[0;36mNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     84\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     85\u001b[0m log_scale \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     86\u001b[0m     math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, Real) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m.\u001b[39mlog()\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;241m-\u001b[39m((value \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m var)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;241m-\u001b[39m log_scale\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi))\n\u001b[1;32m     92\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ewc_lambda = 50.0\n",
    "model_task_2=run_lora_evcl_2(\n",
    "    combined_loader=combined_loader,\n",
    "    eval_loader=eval_loader,\n",
    "    num_epochs= 100,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=100,\n",
    "    eval_steps=200,\n",
    "    save_steps=500,\n",
    "    output_dir=\"finetuned-weights-LoRA-EVCL-Task2\",\n",
    "    load_pyro=True,\n",
    "    best_output_dir=\"finetuned-weights-LoRA-EVCL-Final-Task2_EVCL_best\",\n",
    "    prev_fisher_info=fisher_info,\n",
    "    prev_posterior_means=prev_posterior_means,\n",
    "    ewc_lambda=ewc_lambda,\n",
    "    synthetic_data_loader=synthetic_loader_1,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6457d3f-72ee-4eca-9d75-84b53d953adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89919ef9-216e-44d2-a83a-c2458951d58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844525ac-9677-4cc2-846d-bae8ebb87667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_546)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
