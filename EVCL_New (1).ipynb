{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df9f10-1efb-4437-b04e-e6c2020be1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import tyxe\n",
    "\n",
    "import random\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO, Trace_ELBO\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, TensorDataset\n",
    "\n",
    "from datasets import load_dataset  # Added to load SuperNI dataset\n",
    "\n",
    "from typing import Optional, List\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d43227-5365-4537-a585-1f370b504d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_info_llm(bnn, prev_fisher_info, data_loader, n_samples=5000, ewc_gamma=1.0):\n",
    "    est_fisher_info = {}\n",
    "    # Only compute Fisher Information for LoRA parameters\n",
    "    for name, param in bnn.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            est_fisher_info[name] = torch.zeros_like(param)\n",
    "\n",
    "    old_training_state = bnn.net.training\n",
    "    bnn.net.eval()\n",
    "\n",
    "    num_samples = 0\n",
    "    for index, batch in enumerate(data_loader):\n",
    "        if n_samples is not None and num_samples >= n_samples:\n",
    "            break\n",
    "\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        outputs = bnn.net(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        bnn.net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for name, param in bnn.named_parameters():\n",
    "            if 'lora' in name and param.grad is not None:\n",
    "                est_fisher_info[name] += param.grad.detach() ** 2\n",
    "\n",
    "        num_samples += input_ids.size(0)\n",
    "\n",
    "    # Normalize the estimated Fisher information\n",
    "    est_fisher_info = {n: p / num_samples for n, p in est_fisher_info.items()}\n",
    "\n",
    "    if prev_fisher_info is not None:\n",
    "        for name in est_fisher_info:\n",
    "            if name in prev_fisher_info:\n",
    "                est_fisher_info[name] += ewc_gamma * prev_fisher_info[name]\n",
    "\n",
    "    bnn.net.train(old_training_state)\n",
    "\n",
    "    return est_fisher_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb69d9-1465-4f9c-baee-5b562fdd840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_nlp_datasets(tokenizer, batch_size, num_tasks, start_task=1):\n",
    "    train_loaders = []\n",
    "    test_loaders = []\n",
    "\n",
    "    # Load the SuperNI dataset\n",
    "    # You can specify the split and tasks you need\n",
    "    superni_dataset = load_dataset('super_glue', 'ni')  # Adjust if necessary\n",
    "\n",
    "    # Assuming tasks are numbered starting from 1\n",
    "    for task_index in range(start_task, num_tasks + 1):\n",
    "        if task_index == 1:\n",
    "            # Load QA task from SuperNI\n",
    "            train_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qa', split='train')\n",
    "            test_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qa', split='validation')\n",
    "        elif task_index == 2:\n",
    "            # Load QG task from SuperNI\n",
    "            train_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qg', split='train')\n",
    "            test_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qg', split='validation')\n",
    "        else:\n",
    "            # Load additional tasks if needed\n",
    "            pass\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        train_loaders.append(train_loader)\n",
    "        test_loaders.append(test_loader)\n",
    "\n",
    "    return train_loaders, test_loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d73a0-e0a1-4260-9876-5c755ea94ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_superni_task_dataset(superni_dataset, tokenizer, task_type='qa', split='train'):\n",
    "    # Filter the dataset for the specific task type\n",
    "    # SuperNI tasks are identified by their task names or IDs\n",
    "    # For example, you can filter tasks that contain 'question answering' or 'question generation'\n",
    "\n",
    "    # Example of filtering:\n",
    "    if task_type == 'qa':\n",
    "        task_filter = lambda ex: 'question answering' in ex['Task']\n",
    "    elif task_type == 'qg':\n",
    "        task_filter = lambda ex: 'question generation' in ex['Task']\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "\n",
    "    dataset = superni_dataset[split].filter(task_filter)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # For SuperNI, inputs and outputs are in 'Input' and 'Output' fields\n",
    "        inputs = examples['Input']\n",
    "        targets = examples['Output']\n",
    "\n",
    "        # Tokenize inputs and targets\n",
    "        model_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=512)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        return model_inputs\n",
    "\n",
    "    dataset = dataset.map(preprocess_function, batched=True)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a9ca12-dd23-43c9-9b4c-fe2d6798a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader_for_task1(tokenizer, batch_size):\n",
    "    # Load the SuperNI dataset\n",
    "    superni_dataset = load_dataset('super_glue', 'ni')  # Adjust if necessary\n",
    "\n",
    "    # Load QA task\n",
    "    train_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qa', split='train')\n",
    "    test_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qa', split='validation')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e713f74-5c6a-415a-a725-29d8a167ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evcl(\n",
    "    num_tasks: int = 2,  # Assuming tasks 1 (QA) and 2 (QG)\n",
    "    num_epochs: int = 3,\n",
    "    experiment_name: str = 'llama_evcl_superni',\n",
    "    base_model_name: str = \"meta-llama/Llama-2-7b-hf\",\n",
    "    lora_model_path: str = 'path/to/your/lora/model',\n",
    "    batch_size: int = 8,\n",
    "    coreset_size: int = 200,\n",
    "    coreset_method: str = 'random',\n",
    "    ewc_lambda: float = 100.0,\n",
    "    ewc_gamma: float = 1.0,\n",
    "):\n",
    "    print(\"Loading base model...\")\n",
    "    # Load the model already fine-tuned on the first task\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "    print(\"Applying LoRA adapter...\")\n",
    "    model = PeftModel.from_pretrained(model, lora_model_path)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare the prior using the fine-tuned model\n",
    "    prior = MLEPrior(model)\n",
    "    obs = tyxe.likelihoods.Categorical(-1)\n",
    "    guide = functools.partial(\n",
    "        tyxe.guides.AutoNormal,\n",
    "        init_scale=1e-4,\n",
    "        init_loc_fn=tyxe.guides.PretrainedInitializer.from_net(model, prefix=\"net\")\n",
    "    )\n",
    "\n",
    "    # Initialize Bayesian model\n",
    "    bnn = VariationalBNNWithEWC(model, prior, obs, guide)\n",
    "\n",
    "    # Load the first task's data\n",
    "    train_loader_task1, test_loader_task1 = get_data_loader_for_task1(tokenizer, batch_size)\n",
    "\n",
    "    # Generate the initial coreset from the first task's data\n",
    "    prev_coreset = update_coreset(prev_coreset=[], train_loader=train_loader_task1, coreset_size=coreset_size, selection_method=coreset_method)\n",
    "\n",
    "    # Compute the initial Fisher Information Matrix and previous parameters\n",
    "    prev_fisher_info = compute_fisher_info_llm(\n",
    "        bnn, prev_fisher_info=None, data_loader=train_loader_task1, n_samples=5000, ewc_gamma=ewc_gamma\n",
    "    )\n",
    "    prev_params = {\n",
    "        name: param.detach().clone()\n",
    "        for name, param in bnn.named_parameters()\n",
    "        if 'lora' in name\n",
    "    }\n",
    "\n",
    "    # Now proceed with tasks 2 and onwards\n",
    "    # Prepare tasks 2 to num_tasks\n",
    "    train_loaders, test_loaders = fetch_nlp_datasets(tokenizer, batch_size, num_tasks, start_task=2)\n",
    "\n",
    "    for task_index, train_loader in enumerate(train_loaders, 2):  # Start from task_index=2\n",
    "        print(f\"Training on Task {task_index}...\")\n",
    "\n",
    "        # Update coreset\n",
    "        if coreset_size > 0:\n",
    "            curr_coreset = update_coreset(prev_coreset, train_loader, coreset_size, coreset_method)\n",
    "        else:\n",
    "            curr_coreset = []\n",
    "\n",
    "        # Training loop for current task\n",
    "        def callback(epoch, step, loss):\n",
    "            print(f\"Epoch {epoch}, Step {step}, Loss: {loss}\")\n",
    "\n",
    "        # Fine-tune with variational inference and EWC\n",
    "        update_variational_approx(\n",
    "            bnn, train_loader, curr_coreset, num_epochs, callback, ewc_lambda,\n",
    "            fisher_info=prev_fisher_info, prev_params=prev_params\n",
    "        )\n",
    "\n",
    "        # Compute Fisher Information Matrix for current task\n",
    "        fisher_info = compute_fisher_info_llm(\n",
    "            bnn, prev_fisher_info, train_loader, n_samples=5000, ewc_gamma=ewc_gamma\n",
    "        )\n",
    "\n",
    "        # Update prev_params and prev_fisher_info\n",
    "        prev_params = {\n",
    "            name: param.detach().clone()\n",
    "            for name, param in bnn.named_parameters()\n",
    "            if 'lora' in name\n",
    "        }\n",
    "        prev_fisher_info = fisher_info\n",
    "\n",
    "        # Update prior with posterior from current task\n",
    "        site_names = [site for site in tyxe.util.pyro_sample_sites(bnn) if 'lora' in site]\n",
    "        params_to_update = tyxe.priors.DictPrior({\n",
    "            site: list(bnn.net_guide.get_detached_distributions(site).values())[0]\n",
    "            for site in site_names\n",
    "        })\n",
    "        bnn.update_prior(params_to_update)\n",
    "\n",
    "        # Update prev_coreset\n",
    "        prev_coreset = curr_coreset\n",
    "\n",
    "        # Evaluate on all tasks up to current\n",
    "        for j, test_loader in enumerate([test_loader_task1] + test_loaders[:task_index - 2], 1):\n",
    "            print(f\"Evaluating Task {j}...\")\n",
    "            total_loss = 0.0\n",
    "            num_batches = 0\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    outputs = bnn.net(input_ids, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f\"Task {j} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd853c25-4eab-40f1-a52f-54cb97f9406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_evcl(\n",
    "        num_tasks=2,  # QA and QG tasks\n",
    "        num_epochs=3,\n",
    "        experiment_name='llama_evcl_superni',\n",
    "        base_model_name='meta-llama/Llama-2-7b-hf',\n",
    "        lora_model_path='path/to/your/lora/model',\n",
    "        batch_size=8,\n",
    "        coreset_size=200,  # Adjust as needed\n",
    "        ewc_lambda=100.0,\n",
    "        ewc_gamma=1.0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b94c2a-6f14-4c26-92b0-da100fa4aba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a628a465-6ee6-4673-ba01-0591db3ab9ea",
   "metadata": {},
   "source": [
    "How to Run the Process with SuperNI Dataset\n",
    "\n",
    "Step 1: Environment Setup\n",
    "(Same as previously described)\n",
    "\n",
    "Step 2: Preparing the SuperNI Dataset\n",
    "Install the datasets Library:\n",
    "Ensure you have the datasets library installed:\n",
    "\n",
    "pip install datasets\n",
    "Inspect the SuperNI Dataset:\n",
    "The SuperNI dataset can be loaded using:\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "superni_dataset = load_dataset('super_nat_instruct', 'v1_1')\n",
    "Note: Replace 'super_nat_instruct' and 'v1_1' with the correct dataset identifier if necessary.\n",
    "Identify QA and QG Tasks:\n",
    "SuperNI contains multiple tasks with task descriptions.\n",
    "You need to identify the task IDs or names corresponding to QA and QG.\n",
    "You can print out the tasks to find the ones you need:\n",
    "for task in superni_dataset['train']['Task']:\n",
    "    print(task)\n",
    "Adjust the load_superni_task_dataset Function:\n",
    "Modify the task_filter in load_superni_task_dataset to match the task identifiers for QA and QG.\n",
    "For example:\n",
    "if task_type == 'qa':\n",
    "    task_ids = ['task_id_for_qa1', 'task_id_for_qa2']  # Replace with actual task IDs\n",
    "    task_filter = lambda ex: ex['TaskID'] in task_ids\n",
    "elif task_type == 'qg':\n",
    "    task_ids = ['task_id_for_qg1', 'task_id_for_qg2']  # Replace with actual task IDs\n",
    "    task_filter = lambda ex: ex['TaskID'] in task_ids\n",
    "Adjust Data Preprocessing:\n",
    "Ensure that the Input and Output fields are correctly used.\n",
    "For some tasks, you might need to concatenate context and question.\n",
    "Step 3: Running the Code\n",
    "(Same as previously described)\n",
    "\n",
    "Step 4: Monitoring and Evaluation\n",
    "(Same as previously described)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844525ac-9677-4cc2-846d-bae8ebb87667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
