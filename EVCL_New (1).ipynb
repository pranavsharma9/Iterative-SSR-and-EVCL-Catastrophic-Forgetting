{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3df9f10-1efb-4437-b04e-e6c2020be1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import tyxe\n",
    "\n",
    "import random\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO, Trace_ELBO\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, TensorDataset\n",
    "\n",
    "from datasets import load_dataset  # Added to load SuperNI dataset\n",
    "\n",
    "from typing import Optional, List\n",
    "from model.mle_prior import MLEPrior\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaff032f-ae3f-4be6-a016-d5ee7237f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current Device Index: 0\n",
      "Current Device Name: NVIDIA L40S\n",
      "Number of GPUs: 4\n",
      "Device 0: NVIDIA L40S\n",
      "Device 1: NVIDIA L40S\n",
      "Device 2: NVIDIA L40S\n",
      "Device 3: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get the current device index\n",
    "current_device = torch.cuda.current_device()\n",
    "print(\"Current Device Index:\", current_device)\n",
    "\n",
    "# Get the name of the current device\n",
    "device_name = torch.cuda.get_device_name(current_device)\n",
    "print(\"Current Device Name:\", device_name)\n",
    "\n",
    "# Get the number of GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "# List all GPUs\n",
    "for device_id in range(num_gpus):\n",
    "    print(f\"Device {device_id}: {torch.cuda.get_device_name(device_id)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80eb69d9-1465-4f9c-baee-5b562fdd840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_nlp_datasets(tokenizer, batch_size, num_tasks, start_task=1):\n",
    "#     train_loaders = []\n",
    "#     test_loaders = []\n",
    "\n",
    "#     # Load the SuperNI dataset\n",
    "#     # You can specify the split and tasks you need\n",
    "#     superni_dataset = load_dataset('super_glue', 'ni')  # Adjust if necessary\n",
    "\n",
    "#     # Assuming tasks are numbered starting from 1\n",
    "#     for task_index in range(start_task, num_tasks + 1):\n",
    "#         if task_index == 1:\n",
    "#             # Load QA task from SuperNI\n",
    "#             train_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qa', split='train')\n",
    "#             test_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qa', split='validation')\n",
    "#         elif task_index == 2:\n",
    "#             # Load QG task from SuperNI\n",
    "#             train_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qg', split='train')\n",
    "#             test_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qg', split='validation')\n",
    "#         else:\n",
    "#             # Load additional tasks if needed\n",
    "#             pass\n",
    "\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#         test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         train_loaders.append(train_loader)\n",
    "#         test_loaders.append(test_loader)\n",
    "\n",
    "#     return train_loaders, test_loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211d73a0-e0a1-4260-9876-5c755ea94ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# def load_superni_task_dataset(tokenizer, task_type='qa', split='train'):\n",
    "#     # Filter the dataset for the specific task type\n",
    "#     # SuperNI tasks are identified by their task names or IDs\n",
    "#     # For example, you can filter tasks that contain 'question answering' or 'question generation'\n",
    "\n",
    "#     # Example of filtering:\n",
    "#     if task_type == 'qa':\n",
    "#         path='/home/pranav24/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/QA_FineTuned'\n",
    "#         os.chdir(path)\n",
    "#         target_file = r\"task024_cosmosqa_answer_generation.json\"\n",
    "#         with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "#             json_data = json.load(f)\n",
    "\n",
    "#         dataset = json_data['Instances'][0:2223]\n",
    "        \n",
    "#         # task_filter = lambda ex: 'question answering' in ex['Task']\n",
    "#     elif task_type == 'qg':\n",
    "#         # task_filter = lambda ex: 'question generation' in ex['Task']\n",
    "#         path='/home/pranav24/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/QG_FineTuned/QG_FineTuned'\n",
    "#         os.chdir(path)\n",
    "#         target_file = r\"task074_squad1.1_question_generation.json\"\n",
    "#         with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "#             json_data = json.load(f)\n",
    "\n",
    "#         dataset = json_data['Instances'][0:2223]\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "\n",
    "\n",
    "#     def preprocess_function(examples):\n",
    "#         # For SuperNI, inputs and outputs are in 'Input' and 'Output' fields\n",
    "#         inputs = examples['Input']\n",
    "#         targets = examples['Output']\n",
    "    \n",
    "#         # Tokenize inputs and targets\n",
    "#         model_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=512)\n",
    "#         with tokenizer.as_target_tokenizer():\n",
    "#             labels = tokenizer(targets, truncation=True, padding='max_length', max_length=512)\n",
    "    \n",
    "#         model_inputs['labels'] = labels['input_ids']\n",
    "#         return model_inputs\n",
    "    \n",
    "#     dataset = dataset.map(preprocess_function, batched=True)\n",
    "#     dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "#     return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3a9ca12-dd23-43c9-9b4c-fe2d6798a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_loader_for_task1(tokenizer, batch_size):\n",
    "#     # Load the SuperNI dataset\n",
    "\n",
    "#     # Load QA task\n",
    "#     train_dataset = load_superni_task_dataset(tokenizer, task_type='qa', split='train')\n",
    "#     test_dataset = load_superni_task_dataset(tokenizer, task_type='qa', split='validation')\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3192f-96f4-44da-9eb4-aea2856aab4f",
   "metadata": {},
   "source": [
    "### Task1 -QA LoRA+EVCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006361fb-8a46-4d83-9eab-78ca4ee72eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrainingConfig:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         output_dir,\n",
    "#         num_train_epochs,\n",
    "#         per_device_train_batch_size,\n",
    "#         gradient_accumulation_steps,\n",
    "#         learning_rate,\n",
    "#         logging_steps,\n",
    "#         eval_steps,\n",
    "#         save_steps,\n",
    "#         save_total_limit,\n",
    "#         fp16,\n",
    "#     ):\n",
    "#         self.output_dir = output_dir\n",
    "#         self.num_train_epochs = num_train_epochs\n",
    "#         self.per_device_train_batch_size = per_device_train_batch_size\n",
    "#         self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.logging_steps = logging_steps\n",
    "#         self.eval_steps = eval_steps\n",
    "#         self.save_steps = save_steps\n",
    "#         self.save_total_limit = save_total_limit\n",
    "#         self.fp16 = fp16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d43227-5365-4537-a585-1f370b504d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_fisher_info_llm(bnn, prev_fisher_info, data_loader, n_samples=5000, ewc_gamma=1.0):\n",
    "#     est_fisher_info = {}\n",
    "#     # Only compute Fisher Information for LoRA parameters\n",
    "#     for name, param in bnn.named_parameters():\n",
    "#         if 'lora' in name:\n",
    "#             est_fisher_info[name] = torch.zeros_like(param)\n",
    "\n",
    "#     old_training_state = bnn.net.training\n",
    "#     bnn.net.eval()\n",
    "\n",
    "#     num_samples = 0\n",
    "#     for index, batch in enumerate(data_loader):\n",
    "#         if n_samples is not None and num_samples >= n_samples:\n",
    "#             break\n",
    "\n",
    "#         input_ids = batch['input_ids'].to(DEVICE)\n",
    "#         labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "#         outputs = bnn.net(input_ids, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "#         bnn.net.zero_grad()\n",
    "#         loss.backward()\n",
    "\n",
    "#         for name, param in bnn.named_parameters():\n",
    "#             if 'lora' in name and param.grad is not None:\n",
    "#                 est_fisher_info[name] += param.grad.detach() ** 2\n",
    "\n",
    "#         num_samples += input_ids.size(0)\n",
    "\n",
    "#     # Normalize the estimated Fisher information\n",
    "#     est_fisher_info = {n: p / num_samples for n, p in est_fisher_info.items()}\n",
    "\n",
    "#     if prev_fisher_info is not None:\n",
    "#         for name in est_fisher_info:\n",
    "#             if name in prev_fisher_info:\n",
    "#                 est_fisher_info[name] += ewc_gamma * prev_fisher_info[name]\n",
    "\n",
    "#     bnn.net.train(old_training_state)\n",
    "\n",
    "#     return est_fisher_info\n",
    "\n",
    "def compute_fisher_info(model, data_loader):\n",
    "    fisher = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            fisher[name] = torch.zeros_like(param)\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        model.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'lora' in name and param.grad is not None:\n",
    "                fisher[name] += param.grad.data ** 2\n",
    "                \n",
    "    # Normalize\n",
    "    for name in fisher:\n",
    "        fisher[name] = fisher[name] / len(data_loader)\n",
    "    return fisher\n",
    "\n",
    "# Function to get variational posterior means\n",
    "def get_variational_posterior_means():\n",
    "    posterior_means = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'lora_A'):\n",
    "            lora_A_loc = pyro.param(f\"{name}.lora_A_loc\").detach().clone()\n",
    "            posterior_means[f\"{name}.lora_A\"] = lora_A_loc\n",
    "        if hasattr(module, 'lora_B'):\n",
    "            lora_B_loc = pyro.param(f\"{name}.lora_B_loc\").detach().clone()\n",
    "            posterior_means[f\"{name}.lora_B\"] = lora_B_loc\n",
    "    return posterior_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422d585f-dba5-4e42-bbff-db3f22caad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VariationalBNNWithEWC(tyxe.VariationalBNN):\n",
    "#     def __init__(self, net, prior, likelihood, net_guide_builder):\n",
    "#         super().__init__(net, prior, likelihood, net_guide_builder)\n",
    "\n",
    "#     def fit(self, data_loader, optim, num_epochs, callback=None, num_particles=1, closed_form_kl=True, device=None, ewc_lambda=0.0, fisher_info=None, prev_params=None):\n",
    "#         old_training_state = self.net.training\n",
    "#         self.net.train(True)\n",
    "\n",
    "#         loss_fn = TraceMeanField_ELBO(num_particles) if closed_form_kl else Trace_ELBO(num_particles)\n",
    "#         svi = SVI(self.model, self.guide, optim, loss=loss_fn)\n",
    "\n",
    "#         for epoch in range(num_epochs):\n",
    "#             total_loss = 0.0\n",
    "#             num_batches = 0\n",
    "#             for num_batches, batch in enumerate(data_loader, 1):\n",
    "#                 input_data = batch['input_ids'].to(device)\n",
    "#                 observation_data = batch['labels'].to(device)\n",
    "#                 elbo = svi.step(input_data, observation_data)\n",
    "\n",
    "#                 if ewc_lambda > 0 and fisher_info is not None:\n",
    "#                     ewc_loss = 0\n",
    "#                     for name, param in self.named_parameters():\n",
    "#                         if 'lora' in name and name in fisher_info:\n",
    "#                             ewc_loss += (fisher_info[name] * (param - prev_params[name]) ** 2).sum()\n",
    "#                     ewc_loss = 0.5 * ewc_lambda * ewc_loss\n",
    "#                     total_loss += elbo + ewc_loss\n",
    "#                 else:\n",
    "#                     total_loss += elbo\n",
    "\n",
    "#                 if callback is not None:\n",
    "#                     callback(epoch, num_batches, total_loss / num_batches)\n",
    "\n",
    "#         self.net.train(old_training_state)\n",
    "#         return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4265f3da-cf93-4c14-b872-82c7f2817cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import LoraLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcc12ed2-8b42-4e1e-87f5-ed00d1eb2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import init_empty_weights\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "\n",
    "\n",
    "def initialize_lora():\n",
    "    login(\"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\")\n",
    "    # Set environment variable to manage memory fragmentation\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    \n",
    "     \n",
    "    # Specify directories and the path to the zip file\n",
    "    offload_dir = os.path.expanduser(\"llama_offload_evcl/\")\n",
    "     \n",
    "    os.makedirs(offload_dir, exist_ok=True)\n",
    "     \n",
    "    # Extract only the specified JSON file from the zip archive\n",
    "    os.chdir('/home/pranav24/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/QA_FineTuned/')\n",
    "    target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "     \n",
    "    # Load tokenizer from Hugging Face\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    max_memory = {\n",
    "    0: \"40GiB\",  # GPU 0\n",
    "    1: \"40GiB\",  # GPU 1\n",
    "    2: \"40GiB\"  # GPU 2\n",
    "    # \"cpu\": \"100GiB\"  # Adjust based on your system's RAM\n",
    "    }\n",
    "\n",
    "    # Load the model with accelerate's offloading and device map auto-setup\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B\",\n",
    "            device_map=\"auto\",\n",
    "            max_memory=max_memory,\n",
    "            offload_folder=offload_dir,\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True\n",
    "        )\n",
    "     \n",
    "    # Configure LoRA with reduced rank\n",
    "    lora_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    #printing the trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if 'lora' in name:\n",
    "    #         print(name)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d62ff472-4fa4-47c2-b2bf-f39fd4fbfbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2e511ba72c4857ac2b8fe393d852be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,703,936 || all params: 8,031,965,184 || trainable%: 0.0212\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading base model...\")\n",
    "model,tokenizer=initialize_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57385901-11a1-491b-abb3-1b924f4d6cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f8bd213832491bb75bd1ec45768b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav24/cs-546/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/pranav24/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/QA_FineTuned/')\n",
    "target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "instances = json_data['Instances'][0:2223]\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"output\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"attention_mask\"] = model_inputs.get(\"attention_mask\", None)\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization and set format\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Split dataset into train and eval\n",
    "train_size = int(0.9 * len(tokenized_datasets))\n",
    "train_dataset = tokenized_datasets.select(range(train_size))\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89919ef9-216e-44d2-a83a-c2458951d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_variational_approx(bnn, train_loader, curr_coreset, num_epochs, callback, ewc_lambda, fisher_info=None, prev_params=None):\n",
    "#     if curr_coreset:\n",
    "#         # Create a dataset from the coreset\n",
    "#         print('Coreset is true')\n",
    "#         coreset_input_ids = torch.stack([item[0] for item in curr_coreset])\n",
    "#         coreset_labels = torch.stack([item[1] for item in curr_coreset])\n",
    "#         coreset_dataset = TensorDataset(coreset_input_ids, coreset_labels)\n",
    "\n",
    "#         # Combine coreset and current task data\n",
    "#         combined_dataset = ConcatDataset([train_loader.dataset, coreset_dataset])\n",
    "#         data_loader = DataLoader(combined_dataset, batch_size=train_loader.batch_size, shuffle=True)\n",
    "#     else:\n",
    "#         print('coreset not true')\n",
    "#         data_loader = train_loader\n",
    "\n",
    "#     optim = pyro.optim.Adam({\"lr\": 1e-5})  # Adjust learning rate as needed\n",
    "\n",
    "#     with tyxe.poutine.local_reparameterization():\n",
    "#         bnn.fit(data_loader, optim, num_epochs, device=DEVICE, callback=callback, ewc_lambda=ewc_lambda, fisher_info=fisher_info, prev_params=prev_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c77ef6a0-a2a7-4c38-be77-fab57110e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer\n",
    "\n",
    "# class EWCTrainer(Trainer):\n",
    "#     def __init__(self, ewc_lambda, fisher_info, prev_params, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.ewc_lambda = ewc_lambda\n",
    "#         self.fisher_info = fisher_info\n",
    "#         self.prev_params = prev_params\n",
    "\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         # Standard loss\n",
    "#         outputs = model(**inputs)\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         # EWC loss\n",
    "#         if self.fisher_info is not None and self.prev_params is not None:\n",
    "#             ewc_loss = 0\n",
    "#             for name, param in model.named_parameters():\n",
    "#                 if 'lora' in name and name in self.fisher_info:\n",
    "#                     ewc_loss += (self.fisher_info[name] * (param - self.prev_params[name]) ** 2).sum()\n",
    "#             ewc_loss = 0.5 * self.ewc_lambda * ewc_loss\n",
    "#             loss += ewc_loss\n",
    "\n",
    "#         return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c480446-d4b9-4423-8728-7654876ee118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyro.nn as pynn\n",
    "# import torch.nn as nn\n",
    "# class BayesianLoRAModule(pynn.PyroModule):\n",
    "#     def __init__(self, model):\n",
    "#         super().__init__()\n",
    "#         self.model = model  # The base model remains unchanged\n",
    "\n",
    "#         # Replace LoRA parameters with PyroSample\n",
    "#         for name, module in self.model.named_modules():\n",
    "#             if isinstance(module, nn.Linear) and hasattr(module, 'lora_A'):\n",
    "#                 # Replace lora_A and lora_B with PyroSample\n",
    "#                 lora_A_name = f\"{name}.lora_A\"\n",
    "#                 lora_B_name = f\"{name}.lora_B\"\n",
    "\n",
    "#                 # Get the existing parameters\n",
    "#                 lora_A = getattr(module, 'lora_A')\n",
    "#                 lora_B = getattr(module, 'lora_B')\n",
    "\n",
    "#                 # Register PyroSample parameters\n",
    "#                 setattr(module, 'lora_A', pynn.PyroSample(dist.Normal(lora_A.data, 1.0).to_event(2)))\n",
    "#                 setattr(module, 'lora_B', pynn.PyroSample(dist.Normal(lora_B.data, 1.0).to_event(2)))\n",
    "\n",
    "#     def forward(self, *args, **kwargs):\n",
    "#         return self.model(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61acd4b7-fd02-40b3-8136-fd713a901ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bayesian_model_and_guide(model):\n",
    "#     bayesian_model = BayesianLoRAModule(model)\n",
    "\n",
    "#     # Define the guide\n",
    "#     def guide(*args, **kwargs):\n",
    "#         for name, module in bayesian_model.named_modules():\n",
    "#             if isinstance(module, nn.Linear) and hasattr(module, 'lora_A'):\n",
    "#                 # Define variational distributions for lora_A and lora_B\n",
    "#                 lora_A_loc = pyro.param(f\"{name}.lora_A_loc\", torch.zeros_like(module.lora_A.data))\n",
    "#                 lora_A_scale = pyro.param(f\"{name}.lora_A_scale\", torch.ones_like(module.lora_A.data), constraint=pyro.distributions.constraints.positive)\n",
    "#                 lora_B_loc = pyro.param(f\"{name}.lora_B_loc\", torch.zeros_like(module.lora_B.data))\n",
    "#                 lora_B_scale = pyro.param(f\"{name}.lora_B_scale\", torch.ones_like(module.lora_B.data), constraint=pyro.distributions.constraints.positive)\n",
    "#                 pyro.sample(f\"{name}.lora_A\", dist.Normal(lora_A_loc, lora_A_scale).to_event(2))\n",
    "#                 pyro.sample(f\"{name}.lora_B\", dist.Normal(lora_B_loc, lora_B_scale).to_event(2))\n",
    "#     return bayesian_model, guide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6015f59-7b73-46b6-940a-9f1ec22c3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(bnn, tokenizer, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Save the state dictionary of the model\n",
    "    torch.save(bnn.net.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    # Save the tokenizer\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93cecb98-c125-4a32-a87f-05e4addea699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            outputs = model(input_ids, labels=labels, attention_mask=attention_mask)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Evaluation Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ae06200-7093-45ff-bec3-abc1e93834af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "\n",
    "def run_lora_evcl_1(\n",
    "    num_epochs: int = 3,\n",
    "    base_model_name: str = \"meta-llama/Meta-Llama-3-8B\",\n",
    "    batch_size: int = 8,\n",
    "    learning_rate: float = 1e-5,\n",
    "    logging_steps: int = 100,\n",
    "    eval_steps: int = 200,\n",
    "    save_steps: int = 500,\n",
    "    output_dir: str = \"finetuned-weights-LoRA-EVCL\",\n",
    "):\n",
    "\n",
    "\n",
    "    def bayesian_guide(input_ids, attention_mask, labels):\n",
    "        # Define variational distributions over the LoRA parameters\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, 'lora_A'):\n",
    "                for key in module.lora_A:\n",
    "                    param_name = f\"{name}.lora_A.{key}\"\n",
    "                    lora_A_param = module.lora_A[key]\n",
    "                    device = lora_A_param.weight.device\n",
    "                    loc_init = lora_A_param.weight.clone().detach().to(device)\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_A_param.weight)).to(device)\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    # Ensure loc and scale are on the correct device\n",
    "                    loc = loc.to(device)\n",
    "                    scale = scale.to(device)\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_A_param.weight.dim()),\n",
    "                        infer={\"is_auxiliary\": True}\n",
    "                    )\n",
    "            if hasattr(module, 'lora_B'):\n",
    "                for key in module.lora_B:\n",
    "                    param_name = f\"{name}.lora_B.{key}\"\n",
    "                    lora_B_param = module.lora_B[key]\n",
    "                    device = lora_B_param.weight.device\n",
    "                    loc_init = lora_B_param.weight.clone().detach().to(device)\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_B_param.weight)).to(device)\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    # Ensure loc and scale are on the correct device\n",
    "                    loc = loc.to(device)\n",
    "                    scale = scale.to(device)\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_B_param.weight.dim()),\n",
    "                        infer={\"is_auxiliary\": True}\n",
    "                    )\n",
    "    def bayesian_model(input_ids, attention_mask, labels):\n",
    "        # Define prior distributions over LoRA parameters\n",
    "        priors = {}\n",
    "        devices = set()\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, 'lora_A'):\n",
    "                for key in module.lora_A:\n",
    "                    param_name = f\"{name}.lora_A.{key}\"\n",
    "                    lora_A_param = module.lora_A[key]\n",
    "                    device = lora_A_param.weight.device\n",
    "                    devices.add(device)\n",
    "                    loc = lora_A_param.weight.clone().detach().to(device)\n",
    "                    scale = (0.1 * torch.ones_like(lora_A_param.weight)).to(device)\n",
    "                    prior = dist.Normal(loc, scale).to_event(lora_A_param.weight.dim())\n",
    "                    priors[param_name] = prior\n",
    "            if hasattr(module, 'lora_B'):\n",
    "                for key in module.lora_B:\n",
    "                    param_name = f\"{name}.lora_B.{key}\"\n",
    "                    lora_B_param = module.lora_B[key]\n",
    "                    device = lora_B_param.weight.device\n",
    "                    devices.add(device)\n",
    "                    loc = lora_B_param.weight.clone().detach().to(device)\n",
    "                    scale = (0.1 * torch.ones_like(lora_B_param.weight)).to(device)\n",
    "                    prior = dist.Normal(loc, scale).to_event(lora_B_param.weight.dim())\n",
    "                    priors[param_name] = prior\n",
    "    \n",
    "        # Ensure all devices are the same\n",
    "        if len(devices) > 1:\n",
    "            raise RuntimeError(f\"Expected all parameters to be on the same device, but found devices: {devices}\")\n",
    "    \n",
    "        # Use Pyro's lifted module to sample parameters\n",
    "        lifted_module = pyro.random_module(\"model\", model, priors)\n",
    "        lifted_model = lifted_module()\n",
    "    \n",
    "        # Ensure inputs are on the correct device(s)\n",
    "        device = next(iter(devices))\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs = lifted_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        return loss\n",
    "\n",
    "    # Set up SVI\n",
    "    pyro.clear_param_store()\n",
    "    optim = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "    elbo = TraceMeanField_ELBO()\n",
    "    svi = SVI(bayesian_model, bayesian_guide, optim, loss=elbo)\n",
    "\n",
    "    \n",
    "    # Training loop for Task 1\n",
    "    print(f\"Training on Task 1...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for num_batches, batch in enumerate(train_loader, 1):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            loss = svi.step(input_ids, attention_mask, labels)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Logging\n",
    "            if num_batches % logging_steps == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch {epoch}, Step {num_batches}, Loss: {avg_loss}\")\n",
    "\n",
    "            # Evaluation\n",
    "            if num_batches % eval_steps == 0:\n",
    "                evaluate_model(model, eval_loader)\n",
    "\n",
    "            # Save checkpoints\n",
    "            if num_batches % save_steps == 0:\n",
    "                save_trained_model(model, tokenizer, output_dir)\n",
    "\n",
    "        avg_epoch_loss = total_loss / num_batches\n",
    "        print(f\"Task 1 Epoch {epoch} completed. Average Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    # Save the final trained model after Task 1\n",
    "    save_trained_model(model, tokenizer, output_dir)\n",
    "\n",
    "    # After Task 1, compute FIM and save posterior means\n",
    "    fisher_info = compute_fisher_info(model, train_loader)\n",
    "    prev_posterior_means = get_variational_posterior_means()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ed87466-51c2-43f4-bccc-e0710e0b8fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Task 1...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all parameters to be on the same device, but found devices: {device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2)}\nTrace Shapes:\n Param Sites:\nSample Sites:",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/cs-546/venv/lib/python3.10/site-packages/pyro/poutine/trace_messenger.py:191\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/cs-546/venv/lib/python3.10/site-packages/pyro/poutine/messenger.py:32\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 94\u001b[0m, in \u001b[0;36mrun_lora_evcl_1.<locals>.bayesian_model\u001b[0;34m(input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(devices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all parameters to be on the same device, but found devices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Use Pyro's lifted module to sample parameters\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all parameters to be on the same device, but found devices: {device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2)}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTORCH_CUDA_ALLOC_CONF\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpandable_segments:True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mrun_lora_evcl_1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Meta-Llama-3-8B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinetuned-weights-LoRA-EVCL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 130\u001b[0m, in \u001b[0;36mrun_lora_evcl_1\u001b[0;34m(num_epochs, base_model_name, batch_size, learning_rate, logging_steps, eval_steps, save_steps, output_dir)\u001b[0m\n\u001b[1;32m    127\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    128\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m--> 130\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43msvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n",
      "File \u001b[0;32m~/cs-546/venv/lib/python3.10/site-packages/pyro/infer/svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[0;32m--> 145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    148\u001b[0m     site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[0;32m~/cs-546/venv/lib/python3.10/site-packages/pyro/infer/trace_elbo.py:140\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# grab a trace from the generator\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_trace, guide_trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_traces(model, guide, args, kwargs):\n\u001b[1;32m    141\u001b[0m     loss_particle, surrogate_loss_particle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_differentiable_loss_particle(\n\u001b[1;32m    142\u001b[0m         model_trace, guide_trace\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_particle \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles\n",
      "File \u001b[0;32m~/cs-546/venv/lib/python3.10/site-packages/pyro/infer/elbo.py:237\u001b[0m, in \u001b[0;36mELBO._get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles):\n\u001b[0;32m--> 237\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs-546/venv/lib/python3.10/site-packages/pyro/infer/trace_mean_field_elbo.py:82\u001b[0m, in \u001b[0;36mTraceMeanField_ELBO._get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, guide, args, kwargs):\n\u001b[0;32m---> 82\u001b[0m     model_trace, guide_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     84\u001b[0m         _check_mean_field_requirement(model_trace, guide_trace)\n",
      "File \u001b[0;32m~/cs-546/venv/lib/python3.10/site-packages/pyro/infer/trace_elbo.py:57\u001b[0m, in \u001b[0;36mTrace_ELBO._get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, guide, args, kwargs):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    Returns a single trace from the guide, and the model that is run\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    against it.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     model_trace, guide_trace \u001b[38;5;241m=\u001b[39m \u001b[43mget_importance_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_plate_nesting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     61\u001b[0m         check_if_enumerated(guide_trace)\n",
      "File \u001b[0;32m~/cs-546/venv/lib/python3.10/site-packages/pyro/infer/enum.py:65\u001b[0m, in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m detach:\n\u001b[1;32m     64\u001b[0m         guide_trace\u001b[38;5;241m.\u001b[39mdetach_()\n\u001b[0;32m---> 65\u001b[0m     model_trace \u001b[38;5;241m=\u001b[39m \u001b[43mpoutine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpoutine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguide_trace\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_type\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     70\u001b[0m     check_model_guide_match(model_trace, guide_trace, max_plate_nesting)\n",
      "File \u001b[0;32m~/cs-546/venv/lib/python3.10/site-packages/pyro/poutine/trace_messenger.py:216\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Trace:\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    :returns: data structure\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mget_trace()\n",
      "File \u001b[0;32m~/cs-546/venv/lib/python3.10/site-packages/pyro/poutine/trace_messenger.py:198\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m         exc \u001b[38;5;241m=\u001b[39m exc_type(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(exc_value, shapes))\n\u001b[1;32m    197\u001b[0m         exc \u001b[38;5;241m=\u001b[39m exc\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[0;32m--> 198\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_RETURN\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_RETURN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39mret\n\u001b[1;32m    201\u001b[0m     )\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/cs-546/venv/lib/python3.10/site-packages/pyro/poutine/trace_messenger.py:191\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    193\u001b[0m     exc_type, exc_value, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/cs-546/venv/lib/python3.10/site-packages/pyro/poutine/messenger.py:32\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_context_wrap\u001b[39m(\n\u001b[1;32m     26\u001b[0m     context: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessenger\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     fn: Callable,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m*\u001b[39margs: Any,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     30\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m---> 32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 94\u001b[0m, in \u001b[0;36mrun_lora_evcl_1.<locals>.bayesian_model\u001b[0;34m(input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Ensure all devices are the same\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(devices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all parameters to be on the same device, but found devices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Use Pyro's lifted module to sample parameters\u001b[39;00m\n\u001b[1;32m     97\u001b[0m lifted_module \u001b[38;5;241m=\u001b[39m pyro\u001b[38;5;241m.\u001b[39mrandom_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, model, priors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all parameters to be on the same device, but found devices: {device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2)}\nTrace Shapes:\n Param Sites:\nSample Sites:"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_lora_evcl_1(\n",
    "        num_epochs=3,\n",
    "        base_model_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "        batch_size=8,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=100,\n",
    "        eval_steps=200,\n",
    "        save_steps=500,\n",
    "        output_dir=\"finetuned-weights-LoRA-EVCL\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0159a88-7c03-406a-a7b9-c308562314ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get the current device index\n",
    "current_device = torch.cuda.current_device()\n",
    "print(\"Current Device Index:\", current_device)\n",
    "\n",
    "# Get the name of the current device\n",
    "device_name = torch.cuda.get_device_name(current_device)\n",
    "print(\"Current Device Name:\", device_name)\n",
    "\n",
    "# Get the number of GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "# List all GPUs\n",
    "for device_id in range(num_gpus):\n",
    "    print(f\"Device {device_id}: {torch.cuda.get_device_name(device_id)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed938bb-c6ce-4049-9431-77e6b5e338f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7904e-a05c-4272-8bcc-74bcc2c82097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2375fcc-a362-4f1a-8156-4bdb9e424063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed5fe3-5ade-44f9-b08a-197b881d4894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e713f74-5c6a-415a-a725-29d8a167ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evcl(\n",
    "    num_tasks: int = 2,  # Assuming tasks 1 (QA) and 2 (QG)\n",
    "    num_epochs: int = 3,\n",
    "    experiment_name: str = 'llama_evcl_superni',\n",
    "    base_model_name: str = \"meta-llama/Llama-2-7b-hf\",\n",
    "    lora_model_path: str = 'fine-tuned-llama-lora',\n",
    "    batch_size: int = 8,\n",
    "    coreset_size: int = 200,\n",
    "    coreset_method: str = 'random',\n",
    "    ewc_lambda: float = 100.0,\n",
    "    ewc_gamma: float = 1.0,\n",
    "    task_folder='QA_FineTuned'\n",
    "):\n",
    "    os.chdir(f'/home/pranav24/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/{task_folder}/finetuned-weights')\n",
    "    \n",
    "    print(\"Loading base model...\")\n",
    "    # Load the model already fine-tuned on the first task\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "    print(\"Applying LoRA adapter...\")\n",
    "    model = PeftModel.from_pretrained(model, lora_model_path)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare the prior using the fine-tuned model\n",
    "    prior = MLEPrior(model)\n",
    "    obs = tyxe.likelihoods.Categorical(-1)\n",
    "    guide = functools.partial(\n",
    "        tyxe.guides.AutoNormal,\n",
    "        init_scale=1e-4,\n",
    "        init_loc_fn=tyxe.guides.PretrainedInitializer.from_net(model, prefix=\"net\")\n",
    "    )\n",
    "\n",
    "    # Initialize Bayesian model\n",
    "    bnn = VariationalBNNWithEWC(model, prior, obs, guide)\n",
    "\n",
    "    # Load the first task's data\n",
    "    train_loader_task1, test_loader_task1 = get_data_loader_for_task1(tokenizer, batch_size)\n",
    "\n",
    "    # Generate the initial coreset from the first task's data\n",
    "    prev_coreset = update_coreset(prev_coreset=[], train_loader=train_loader_task1, coreset_size=coreset_size, selection_method=coreset_method)\n",
    "\n",
    "    # Compute the initial Fisher Information Matrix and previous parameters\n",
    "    prev_fisher_info = compute_fisher_info_llm(\n",
    "        bnn, prev_fisher_info=None, data_loader=train_loader_task1, n_samples=5000, ewc_gamma=ewc_gamma\n",
    "    )\n",
    "    prev_params = {\n",
    "        name: param.detach().clone()\n",
    "        for name, param in bnn.named_parameters()\n",
    "        if 'lora' in name\n",
    "    }\n",
    "\n",
    "    # Now proceed with tasks 2 and onwards\n",
    "    # Prepare tasks 2 to num_tasks\n",
    "    train_loaders, test_loaders = fetch_nlp_datasets(tokenizer, batch_size, num_tasks, start_task=2)\n",
    "\n",
    "    for task_index, train_loader in enumerate(train_loaders, 2):  # Start from task_index=2\n",
    "        print(f\"Training on Task {task_index}...\")\n",
    "\n",
    "        # Update coreset\n",
    "        if coreset_size > 0:\n",
    "            curr_coreset = update_coreset(prev_coreset, train_loader, coreset_size, coreset_method)\n",
    "            # curr_coreset = update_coreset(prev_coreset, train_loader_task1, coreset_size, coreset_method)\n",
    "            # curr_coreset=prev_coreset\n",
    "        else:\n",
    "            curr_coreset = []\n",
    "\n",
    "        # Training loop for current task\n",
    "        def callback(epoch, step, loss):\n",
    "            print(f\"Epoch {epoch}, Step {step}, Loss: {loss}\")\n",
    "\n",
    "        # Fine-tune with variational inference and EWC\n",
    "        update_variational_approx(\n",
    "            bnn, train_loader, curr_coreset, num_epochs, callback, ewc_lambda,\n",
    "            fisher_info=prev_fisher_info, prev_params=prev_params\n",
    "        )\n",
    "\n",
    "        # Compute Fisher Information Matrix for current task\n",
    "        fisher_info = compute_fisher_info_llm(\n",
    "            bnn, prev_fisher_info, train_loader, n_samples=5000, ewc_gamma=ewc_gamma\n",
    "        )\n",
    "\n",
    "        # Update prev_params and prev_fisher_info\n",
    "        prev_params = {\n",
    "            name: param.detach().clone()\n",
    "            for name, param in bnn.named_parameters()\n",
    "            if 'lora' in name\n",
    "        }\n",
    "        prev_fisher_info = fisher_info\n",
    "\n",
    "        # Update prior with posterior from current task\n",
    "        site_names = [site for site in tyxe.util.pyro_sample_sites(bnn) if 'lora' in site]\n",
    "        params_to_update = tyxe.priors.DictPrior({\n",
    "            site: list(bnn.net_guide.get_detached_distributions(site).values())[0]\n",
    "            for site in site_names\n",
    "        })\n",
    "        bnn.update_prior(params_to_update)\n",
    "\n",
    "        # Update prev_coreset\n",
    "        prev_coreset = curr_coreset\n",
    "\n",
    "        # Evaluate on all tasks up to current\n",
    "        for j, test_loader in enumerate([test_loader_task1] + test_loaders[:task_index - 2], 1):\n",
    "            print(f\"Evaluating Task {j}...\")\n",
    "            total_loss = 0.0\n",
    "            num_batches = 0\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    outputs = bnn.net(input_ids, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f\"Task {j} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd853c25-4eab-40f1-a52f-54cb97f9406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_evcl(\n",
    "        num_tasks=2,  # QA and QG tasks\n",
    "        num_epochs=3,\n",
    "        experiment_name='llama_evcl_superni',\n",
    "        base_model_name='meta-llama/Llama-2-7b-hf',\n",
    "        lora_model_path='path/to/your/lora/model',\n",
    "        batch_size=8,\n",
    "        coreset_size=200,  # Adjust as needed\n",
    "        ewc_lambda=100.0,\n",
    "        ewc_gamma=1.0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b94c2a-6f14-4c26-92b0-da100fa4aba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a628a465-6ee6-4673-ba01-0591db3ab9ea",
   "metadata": {},
   "source": [
    "How to Run the Process with SuperNI Dataset\n",
    "\n",
    "Step 1: Environment Setup\n",
    "(Same as previously described)\n",
    "\n",
    "Step 2: Preparing the SuperNI Dataset\n",
    "Install the datasets Library:\n",
    "Ensure you have the datasets library installed:\n",
    "\n",
    "pip install datasets\n",
    "Inspect the SuperNI Dataset:\n",
    "The SuperNI dataset can be loaded using:\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "superni_dataset = load_dataset('super_nat_instruct', 'v1_1')\n",
    "Note: Replace 'super_nat_instruct' and 'v1_1' with the correct dataset identifier if necessary.\n",
    "Identify QA and QG Tasks:\n",
    "SuperNI contains multiple tasks with task descriptions.\n",
    "You need to identify the task IDs or names corresponding to QA and QG.\n",
    "You can print out the tasks to find the ones you need:\n",
    "for task in superni_dataset['train']['Task']:\n",
    "    print(task)\n",
    "Adjust the load_superni_task_dataset Function:\n",
    "Modify the task_filter in load_superni_task_dataset to match the task identifiers for QA and QG.\n",
    "For example:\n",
    "if task_type == 'qa':\n",
    "    task_ids = ['task_id_for_qa1', 'task_id_for_qa2']  # Replace with actual task IDs\n",
    "    task_filter = lambda ex: ex['TaskID'] in task_ids\n",
    "elif task_type == 'qg':\n",
    "    task_ids = ['task_id_for_qg1', 'task_id_for_qg2']  # Replace with actual task IDs\n",
    "    task_filter = lambda ex: ex['TaskID'] in task_ids\n",
    "Adjust Data Preprocessing:\n",
    "Ensure that the Input and Output fields are correctly used.\n",
    "For some tasks, you might need to concatenate context and question.\n",
    "Step 3: Running the Code\n",
    "(Same as previously described)\n",
    "\n",
    "Step 4: Monitoring and Evaluation\n",
    "(Same as previously described)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844525ac-9677-4cc2-846d-bae8ebb87667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
