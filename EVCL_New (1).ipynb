{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3df9f10-1efb-4437-b04e-e6c2020be1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import tyxe\n",
    "\n",
    "import random\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO, Trace_ELBO\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, TensorDataset\n",
    "\n",
    "from datasets import load_dataset  # Added to load SuperNI dataset\n",
    "\n",
    "from typing import Optional, List\n",
    "from model.mle_prior import MLEPrior\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c439a507-6dbb-4ca1-9e41-c0efcd6c165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaff032f-ae3f-4be6-a016-d5ee7237f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current Device Index: 0\n",
      "Current Device Name: NVIDIA A100-SXM4-80GB\n",
      "Number of GPUs: 1\n",
      "Device 0: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get the current device index\n",
    "current_device = torch.cuda.current_device()\n",
    "print(\"Current Device Index:\", current_device)\n",
    "\n",
    "# Get the name of the current device\n",
    "device_name = torch.cuda.get_device_name(current_device)\n",
    "print(\"Current Device Name:\", device_name)\n",
    "\n",
    "# Get the number of GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "# List all GPUs\n",
    "for device_id in range(num_gpus):\n",
    "    print(f\"Device {device_id}: {torch.cuda.get_device_name(device_id)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80eb69d9-1465-4f9c-baee-5b562fdd840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_nlp_datasets(tokenizer, batch_size, num_tasks, start_task=1):\n",
    "#     train_loaders = []\n",
    "#     test_loaders = []\n",
    "\n",
    "#     # Load the SuperNI dataset\n",
    "#     # You can specify the split and tasks you need\n",
    "#     superni_dataset = load_dataset('super_glue', 'ni')  # Adjust if necessary\n",
    "\n",
    "#     # Assuming tasks are numbered starting from 1\n",
    "#     for task_index in range(start_task, num_tasks + 1):\n",
    "#         if task_index == 1:\n",
    "#             # Load QA task from SuperNI\n",
    "#             train_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qa', split='train')\n",
    "#             test_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qa', split='validation')\n",
    "#         elif task_index == 2:\n",
    "#             # Load QG task from SuperNI\n",
    "#             train_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qg', split='train')\n",
    "#             test_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qg', split='validation')\n",
    "#         else:\n",
    "#             # Load additional tasks if needed\n",
    "#             pass\n",
    "\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#         test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         train_loaders.append(train_loader)\n",
    "#         test_loaders.append(test_loader)\n",
    "\n",
    "#     return train_loaders, test_loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211d73a0-e0a1-4260-9876-5c755ea94ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# def load_superni_task_dataset(tokenizer, task_type='qa', split='train'):\n",
    "#     # Filter the dataset for the specific task type\n",
    "#     # SuperNI tasks are identified by their task names or IDs\n",
    "#     # For example, you can filter tasks that contain 'question answering' or 'question generation'\n",
    "\n",
    "#     # Example of filtering:\n",
    "#     if task_type == 'qa':\n",
    "#         path='/home/pranav24/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/QA_FineTuned'\n",
    "#         os.chdir(path)\n",
    "#         target_file = r\"task024_cosmosqa_answer_generation.json\"\n",
    "#         with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "#             json_data = json.load(f)\n",
    "\n",
    "#         dataset = json_data['Instances'][0:2223]\n",
    "        \n",
    "#         # task_filter = lambda ex: 'question answering' in ex['Task']\n",
    "#     elif task_type == 'qg':\n",
    "#         # task_filter = lambda ex: 'question generation' in ex['Task']\n",
    "#         path='/home/pranav24/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/QG_FineTuned/QG_FineTuned'\n",
    "#         os.chdir(path)\n",
    "#         target_file = r\"task074_squad1.1_question_generation.json\"\n",
    "#         with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "#             json_data = json.load(f)\n",
    "\n",
    "#         dataset = json_data['Instances'][0:2223]\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "\n",
    "\n",
    "#     def preprocess_function(examples):\n",
    "#         # For SuperNI, inputs and outputs are in 'Input' and 'Output' fields\n",
    "#         inputs = examples['Input']\n",
    "#         targets = examples['Output']\n",
    "    \n",
    "#         # Tokenize inputs and targets\n",
    "#         model_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=512)\n",
    "#         with tokenizer.as_target_tokenizer():\n",
    "#             labels = tokenizer(targets, truncation=True, padding='max_length', max_length=512)\n",
    "    \n",
    "#         model_inputs['labels'] = labels['input_ids']\n",
    "#         return model_inputs\n",
    "    \n",
    "#     dataset = dataset.map(preprocess_function, batched=True)\n",
    "#     dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "#     return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a9ca12-dd23-43c9-9b4c-fe2d6798a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_loader_for_task1(tokenizer, batch_size):\n",
    "#     # Load the SuperNI dataset\n",
    "\n",
    "#     # Load QA task\n",
    "#     train_dataset = load_superni_task_dataset(tokenizer, task_type='qa', split='train')\n",
    "#     test_dataset = load_superni_task_dataset(tokenizer, task_type='qa', split='validation')\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3192f-96f4-44da-9eb4-aea2856aab4f",
   "metadata": {},
   "source": [
    "### Task1 -QA LoRA+EVCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "006361fb-8a46-4d83-9eab-78ca4ee72eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrainingConfig:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         output_dir,\n",
    "#         num_train_epochs,\n",
    "#         per_device_train_batch_size,\n",
    "#         gradient_accumulation_steps,\n",
    "#         learning_rate,\n",
    "#         logging_steps,\n",
    "#         eval_steps,\n",
    "#         save_steps,\n",
    "#         save_total_limit,\n",
    "#         fp16,\n",
    "#     ):\n",
    "#         self.output_dir = output_dir\n",
    "#         self.num_train_epochs = num_train_epochs\n",
    "#         self.per_device_train_batch_size = per_device_train_batch_size\n",
    "#         self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.logging_steps = logging_steps\n",
    "#         self.eval_steps = eval_steps\n",
    "#         self.save_steps = save_steps\n",
    "#         self.save_total_limit = save_total_limit\n",
    "#         self.fp16 = fp16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18d43227-5365-4537-a585-1f370b504d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_fisher_info_llm(bnn, prev_fisher_info, data_loader, n_samples=5000, ewc_gamma=1.0):\n",
    "#     est_fisher_info = {}\n",
    "#     # Only compute Fisher Information for LoRA parameters\n",
    "#     for name, param in bnn.named_parameters():\n",
    "#         if 'lora' in name:\n",
    "#             est_fisher_info[name] = torch.zeros_like(param)\n",
    "\n",
    "#     old_training_state = bnn.net.training\n",
    "#     bnn.net.eval()\n",
    "\n",
    "#     num_samples = 0\n",
    "#     for index, batch in enumerate(data_loader):\n",
    "#         if n_samples is not None and num_samples >= n_samples:\n",
    "#             break\n",
    "\n",
    "#         input_ids = batch['input_ids'].to(DEVICE)\n",
    "#         labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "#         outputs = bnn.net(input_ids, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "#         bnn.net.zero_grad()\n",
    "#         loss.backward()\n",
    "\n",
    "#         for name, param in bnn.named_parameters():\n",
    "#             if 'lora' in name and param.grad is not None:\n",
    "#                 est_fisher_info[name] += param.grad.detach() ** 2\n",
    "\n",
    "#         num_samples += input_ids.size(0)\n",
    "\n",
    "#     # Normalize the estimated Fisher information\n",
    "#     est_fisher_info = {n: p / num_samples for n, p in est_fisher_info.items()}\n",
    "\n",
    "#     if prev_fisher_info is not None:\n",
    "#         for name in est_fisher_info:\n",
    "#             if name in prev_fisher_info:\n",
    "#                 est_fisher_info[name] += ewc_gamma * prev_fisher_info[name]\n",
    "\n",
    "#     bnn.net.train(old_training_state)\n",
    "\n",
    "#     return est_fisher_info\n",
    "\n",
    "def compute_fisher_info(model, data_loader):\n",
    "    fisher = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            fisher[name] = torch.zeros_like(param)\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        model.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'lora' in name and param.grad is not None:\n",
    "                fisher[name] += param.grad.data ** 2\n",
    "                \n",
    "    # Normalize\n",
    "    for name in fisher:\n",
    "        fisher[name] = fisher[name] / len(data_loader)\n",
    "    return fisher\n",
    "\n",
    "# Function to get variational posterior means\n",
    "def get_variational_posterior_means():\n",
    "    posterior_means = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'lora_A'):\n",
    "            lora_A_loc = pyro.param(f\"{name}.lora_A_loc\").detach().clone()\n",
    "            posterior_means[f\"{name}.lora_A\"] = lora_A_loc\n",
    "        if hasattr(module, 'lora_B'):\n",
    "            lora_B_loc = pyro.param(f\"{name}.lora_B_loc\").detach().clone()\n",
    "            posterior_means[f\"{name}.lora_B\"] = lora_B_loc\n",
    "    return posterior_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422d585f-dba5-4e42-bbff-db3f22caad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VariationalBNNWithEWC(tyxe.VariationalBNN):\n",
    "#     def __init__(self, net, prior, likelihood, net_guide_builder):\n",
    "#         super().__init__(net, prior, likelihood, net_guide_builder)\n",
    "\n",
    "#     def fit(self, data_loader, optim, num_epochs, callback=None, num_particles=1, closed_form_kl=True, device=None, ewc_lambda=0.0, fisher_info=None, prev_params=None):\n",
    "#         old_training_state = self.net.training\n",
    "#         self.net.train(True)\n",
    "\n",
    "#         loss_fn = TraceMeanField_ELBO(num_particles) if closed_form_kl else Trace_ELBO(num_particles)\n",
    "#         svi = SVI(self.model, self.guide, optim, loss=loss_fn)\n",
    "\n",
    "#         for epoch in range(num_epochs):\n",
    "#             total_loss = 0.0\n",
    "#             num_batches = 0\n",
    "#             for num_batches, batch in enumerate(data_loader, 1):\n",
    "#                 input_data = batch['input_ids'].to(device)\n",
    "#                 observation_data = batch['labels'].to(device)\n",
    "#                 elbo = svi.step(input_data, observation_data)\n",
    "\n",
    "#                 if ewc_lambda > 0 and fisher_info is not None:\n",
    "#                     ewc_loss = 0\n",
    "#                     for name, param in self.named_parameters():\n",
    "#                         if 'lora' in name and name in fisher_info:\n",
    "#                             ewc_loss += (fisher_info[name] * (param - prev_params[name]) ** 2).sum()\n",
    "#                     ewc_loss = 0.5 * ewc_lambda * ewc_loss\n",
    "#                     total_loss += elbo + ewc_loss\n",
    "#                 else:\n",
    "#                     total_loss += elbo\n",
    "\n",
    "#                 if callback is not None:\n",
    "#                     callback(epoch, num_batches, total_loss / num_batches)\n",
    "\n",
    "#         self.net.train(old_training_state)\n",
    "#         return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4265f3da-cf93-4c14-b872-82c7f2817cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import LoraLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcc12ed2-8b42-4e1e-87f5-ed00d1eb2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import init_empty_weights\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "\n",
    "\n",
    "def initialize_lora():\n",
    "    login(\"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\")\n",
    "    # Set environment variable to manage memory fragmentation\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    \n",
    "     \n",
    "    # Specify directories and the path to the zip file\n",
    "    offload_dir = os.path.expanduser(\"llama_offload_evcl/\")\n",
    "     \n",
    "    os.makedirs(offload_dir, exist_ok=True)\n",
    "     \n",
    "    # Extract only the specified JSON file from the zip archive\n",
    "    os.chdir('/home/pranav24/cs-546-project/QA_FineTuned')\n",
    "    target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "     \n",
    "    # Load tokenizer from Hugging Face\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # max_memory = {\n",
    "    # 0: \"40GiB\",  # GPU 0\n",
    "    # 1: \"40GiB\",  # GPU 1\n",
    "    # 2: \"40GiB\"  # GPU 2\n",
    "    # # \"cpu\": \"100GiB\"  # Adjust based on your system's RAM\n",
    "    # }\n",
    "\n",
    "    # Load the model with accelerate's offloading and device map auto-setup\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B\",\n",
    "            device_map=\"auto\",\n",
    "            # max_memory=max_memory,\n",
    "            offload_folder=offload_dir,\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True\n",
    "        )\n",
    "     \n",
    "    # Configure LoRA with reduced rank\n",
    "    lora_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    #printing the trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if 'lora' in name:\n",
    "    #         print(name)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d62ff472-4fa4-47c2-b2bf-f39fd4fbfbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4850cac4e54d44acc1665d3c2d07af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,703,936 || all params: 8,031,965,184 || trainable%: 0.0212\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading base model...\")\n",
    "model,tokenizer=initialize_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57385901-11a1-491b-abb3-1b924f4d6cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b786673a7f654f7883524f1973fb536a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav24/cs-546-project/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/pranav24/cs-546-project/QA_FineTuned')\n",
    "target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "instances = json_data['Instances'][0:2223]\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"output\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"attention_mask\"] = model_inputs.get(\"attention_mask\", None)\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization and set format\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Split dataset into train and eval\n",
    "train_size = int(0.9 * len(tokenized_datasets))\n",
    "train_dataset = tokenized_datasets.select(range(train_size))\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89919ef9-216e-44d2-a83a-c2458951d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_variational_approx(bnn, train_loader, curr_coreset, num_epochs, callback, ewc_lambda, fisher_info=None, prev_params=None):\n",
    "#     if curr_coreset:\n",
    "#         # Create a dataset from the coreset\n",
    "#         print('Coreset is true')\n",
    "#         coreset_input_ids = torch.stack([item[0] for item in curr_coreset])\n",
    "#         coreset_labels = torch.stack([item[1] for item in curr_coreset])\n",
    "#         coreset_dataset = TensorDataset(coreset_input_ids, coreset_labels)\n",
    "\n",
    "#         # Combine coreset and current task data\n",
    "#         combined_dataset = ConcatDataset([train_loader.dataset, coreset_dataset])\n",
    "#         data_loader = DataLoader(combined_dataset, batch_size=train_loader.batch_size, shuffle=True)\n",
    "#     else:\n",
    "#         print('coreset not true')\n",
    "#         data_loader = train_loader\n",
    "\n",
    "#     optim = pyro.optim.Adam({\"lr\": 1e-5})  # Adjust learning rate as needed\n",
    "\n",
    "#     with tyxe.poutine.local_reparameterization():\n",
    "#         bnn.fit(data_loader, optim, num_epochs, device=DEVICE, callback=callback, ewc_lambda=ewc_lambda, fisher_info=fisher_info, prev_params=prev_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c77ef6a0-a2a7-4c38-be77-fab57110e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer\n",
    "\n",
    "# class EWCTrainer(Trainer):\n",
    "#     def __init__(self, ewc_lambda, fisher_info, prev_params, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.ewc_lambda = ewc_lambda\n",
    "#         self.fisher_info = fisher_info\n",
    "#         self.prev_params = prev_params\n",
    "\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         # Standard loss\n",
    "#         outputs = model(**inputs)\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         # EWC loss\n",
    "#         if self.fisher_info is not None and self.prev_params is not None:\n",
    "#             ewc_loss = 0\n",
    "#             for name, param in model.named_parameters():\n",
    "#                 if 'lora' in name and name in self.fisher_info:\n",
    "#                     ewc_loss += (self.fisher_info[name] * (param - self.prev_params[name]) ** 2).sum()\n",
    "#             ewc_loss = 0.5 * self.ewc_lambda * ewc_loss\n",
    "#             loss += ewc_loss\n",
    "\n",
    "#         return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c480446-d4b9-4423-8728-7654876ee118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyro.nn as pynn\n",
    "# import torch.nn as nn\n",
    "# class BayesianLoRAModule(pynn.PyroModule):\n",
    "#     def __init__(self, model):\n",
    "#         super().__init__()\n",
    "#         self.model = model  # The base model remains unchanged\n",
    "\n",
    "#         # Replace LoRA parameters with PyroSample\n",
    "#         for name, module in self.model.named_modules():\n",
    "#             if isinstance(module, nn.Linear) and hasattr(module, 'lora_A'):\n",
    "#                 # Replace lora_A and lora_B with PyroSample\n",
    "#                 lora_A_name = f\"{name}.lora_A\"\n",
    "#                 lora_B_name = f\"{name}.lora_B\"\n",
    "\n",
    "#                 # Get the existing parameters\n",
    "#                 lora_A = getattr(module, 'lora_A')\n",
    "#                 lora_B = getattr(module, 'lora_B')\n",
    "\n",
    "#                 # Register PyroSample parameters\n",
    "#                 setattr(module, 'lora_A', pynn.PyroSample(dist.Normal(lora_A.data, 1.0).to_event(2)))\n",
    "#                 setattr(module, 'lora_B', pynn.PyroSample(dist.Normal(lora_B.data, 1.0).to_event(2)))\n",
    "\n",
    "#     def forward(self, *args, **kwargs):\n",
    "#         return self.model(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61acd4b7-fd02-40b3-8136-fd713a901ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bayesian_model_and_guide(model):\n",
    "#     bayesian_model = BayesianLoRAModule(model)\n",
    "\n",
    "#     # Define the guide\n",
    "#     def guide(*args, **kwargs):\n",
    "#         for name, module in bayesian_model.named_modules():\n",
    "#             if isinstance(module, nn.Linear) and hasattr(module, 'lora_A'):\n",
    "#                 # Define variational distributions for lora_A and lora_B\n",
    "#                 lora_A_loc = pyro.param(f\"{name}.lora_A_loc\", torch.zeros_like(module.lora_A.data))\n",
    "#                 lora_A_scale = pyro.param(f\"{name}.lora_A_scale\", torch.ones_like(module.lora_A.data), constraint=pyro.distributions.constraints.positive)\n",
    "#                 lora_B_loc = pyro.param(f\"{name}.lora_B_loc\", torch.zeros_like(module.lora_B.data))\n",
    "#                 lora_B_scale = pyro.param(f\"{name}.lora_B_scale\", torch.ones_like(module.lora_B.data), constraint=pyro.distributions.constraints.positive)\n",
    "#                 pyro.sample(f\"{name}.lora_A\", dist.Normal(lora_A_loc, lora_A_scale).to_event(2))\n",
    "#                 pyro.sample(f\"{name}.lora_B\", dist.Normal(lora_B_loc, lora_B_scale).to_event(2))\n",
    "#     return bayesian_model, guide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6015f59-7b73-46b6-940a-9f1ec22c3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(model, tokenizer, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Save the model\n",
    "    model.save_pretrained(output_dir)\n",
    "    # Save the tokenizer\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93cecb98-c125-4a32-a87f-05e4addea699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            outputs = model(input_ids, labels=labels, attention_mask=attention_mask)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Evaluation Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ae06200-7093-45ff-bec3-abc1e93834af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "def run_lora_evcl_1(\n",
    "    num_epochs: int = 3,\n",
    "    base_model_name: str = \"meta-llama/Meta-Llama-3-8B\",\n",
    "    batch_size: int = 2,\n",
    "    learning_rate: float = 1e-5,\n",
    "    logging_steps: int = 100,\n",
    "    eval_steps: int = 200,\n",
    "    save_steps: int = 500,\n",
    "    output_dir: str = \"finetuned-weights-LoRA-EVCL\",\n",
    "):\n",
    "\n",
    "\n",
    "    def bayesian_guide(input_ids, attention_mask, labels):\n",
    "        # Define variational distributions over the LoRA parameters\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, 'lora_A'):\n",
    "                for key in module.lora_A:\n",
    "                    param_name = f\"{name}.lora_A.{key}\"\n",
    "                    lora_A_param = module.lora_A[key].weight\n",
    "                    device = lora_A_param.device\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        lora_A_param.clone().detach().to(device)\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        (0.1 * torch.ones_like(lora_A_param)).to(device),\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_A_param.dim())\n",
    "                    )\n",
    "            if hasattr(module, 'lora_B'):\n",
    "                for key in module.lora_B:\n",
    "                    param_name = f\"{name}.lora_B.{key}\"\n",
    "                    lora_B_param = module.lora_B[key].weight\n",
    "                    device = lora_B_param.device\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        lora_B_param.clone().detach().to(device)\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        (0.1 * torch.ones_like(lora_B_param)).to(device),\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_B_param.dim())\n",
    "                    )\n",
    "                    \n",
    "    def bayesian_model(input_ids, attention_mask, labels):\n",
    "        # Define a function to sample and substitute LoRA parameters\n",
    "        def model_with_sampled_lora():\n",
    "            # Sample LoRA parameters and set them in the model\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, 'lora_A'):\n",
    "                    for key in module.lora_A:\n",
    "                        param_name = f\"{name}.lora_A.{key}\"\n",
    "                        lora_A_module = module.lora_A[key]\n",
    "                        device = lora_A_module.weight.device\n",
    "    \n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                lora_A_module.weight.detach().to(device),\n",
    "                                (0.1 * torch.ones_like(lora_A_module.weight)).to(device)\n",
    "                            ).to_event(lora_A_module.weight.dim())\n",
    "                        )\n",
    "    \n",
    "                        # Assign the sampled weight to the module\n",
    "                        lora_A_module.weight = torch.nn.Parameter(sampled_weight)\n",
    "    \n",
    "                if hasattr(module, 'lora_B'):\n",
    "                    for key in module.lora_B:\n",
    "                        param_name = f\"{name}.lora_B.{key}\"\n",
    "                        lora_B_module = module.lora_B[key]\n",
    "                        device = lora_B_module.weight.device\n",
    "    \n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                lora_B_module.weight.detach().to(device),\n",
    "                                (0.1 * torch.ones_like(lora_B_module.weight)).to(device)\n",
    "                            ).to_event(lora_B_module.weight.dim())\n",
    "                        )\n",
    "    \n",
    "                        # Assign the sampled weight to the module\n",
    "                        lora_B_module.weight = torch.nn.Parameter(sampled_weight)\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            return loss\n",
    "    \n",
    "        # Use the modified model with sampled LoRA parameters\n",
    "        return model_with_sampled_lora()\n",
    "\n",
    "\n",
    "    # Set up SVI\n",
    "    pyro.clear_param_store()\n",
    "    optim = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "    elbo = TraceMeanField_ELBO()\n",
    "    svi = SVI(bayesian_model, bayesian_guide, optim, loss=elbo)\n",
    "\n",
    "    \n",
    "    # Training loop for Task 1\n",
    "    print(f\"Training on Task 1...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for num_batches, batch in enumerate(train_loader, 1):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            loss = svi.step(input_ids, attention_mask, labels)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Logging\n",
    "            if num_batches % logging_steps == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch {epoch}, Step {num_batches}, Loss: {avg_loss}\")\n",
    "\n",
    "            # Evaluation\n",
    "            if num_batches % eval_steps == 0:\n",
    "                evaluate_model(model, eval_loader)\n",
    "\n",
    "            # Save checkpoints\n",
    "            if num_batches % save_steps == 0:\n",
    "                save_trained_model(model, tokenizer, output_dir)\n",
    "\n",
    "        avg_epoch_loss = total_loss / num_batches\n",
    "        print(f\"Task 1 Epoch {epoch} completed. Average Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    # Save the final trained model after Task 1\n",
    "    save_trained_model(model, tokenizer, output_dir)\n",
    "\n",
    "    # After Task 1, compute FIM and save posterior means\n",
    "    fisher_info = compute_fisher_info(model, train_loader)\n",
    "    prev_posterior_means = get_variational_posterior_means()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ed87466-51c2-43f4-bccc-e0710e0b8fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Task 1...\n",
      "Epoch 0, Step 100, Loss: 843384.354375\n",
      "Epoch 0, Step 200, Loss: 847636.0365625\n",
      "Evaluation Loss: 15.5557\n",
      "Task 1 Epoch 0 completed. Average Loss: 848479.90925\n",
      "Epoch 1, Step 100, Loss: 852074.631875\n",
      "Epoch 1, Step 200, Loss: 851945.700625\n",
      "Evaluation Loss: 13.2223\n",
      "Task 1 Epoch 1 completed. Average Loss: 851934.56475\n",
      "Epoch 2, Step 100, Loss: 851971.97625\n",
      "Epoch 2, Step 200, Loss: 851939.976875\n",
      "Evaluation Loss: 14.5898\n",
      "Task 1 Epoch 2 completed. Average Loss: 851958.75075\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 680.81 MiB is free. Process 2816447 has 21.43 GiB memory in use. Process 2954983 has 57.03 GiB memory in use. Of the allocated memory 56.07 GiB is allocated by PyTorch, and 455.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTORCH_CUDA_ALLOC_CONF\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpandable_segments:True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mrun_lora_evcl_1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Meta-Llama-3-8B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinetuned-weights-LoRA-EVCL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 148\u001b[0m, in \u001b[0;36mrun_lora_evcl_1\u001b[0;34m(num_epochs, base_model_name, batch_size, learning_rate, logging_steps, eval_steps, save_steps, output_dir)\u001b[0m\n\u001b[1;32m    145\u001b[0m save_trained_model(model, tokenizer, output_dir)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# After Task 1, compute FIM and save posterior means\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m fisher_info \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_fisher_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m prev_posterior_means \u001b[38;5;241m=\u001b[39m get_variational_posterior_means()\n",
      "Cell \u001b[0;32mIn[8], line 56\u001b[0m, in \u001b[0;36mcompute_fisher_info\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m     53\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     54\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 56\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     58\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/peft/peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1210\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[0;32m-> 1210\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1212\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/cs-546-project/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 680.81 MiB is free. Process 2816447 has 21.43 GiB memory in use. Process 2954983 has 57.03 GiB memory in use. Of the allocated memory 56.07 GiB is allocated by PyTorch, and 455.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_lora_evcl_1(\n",
    "        num_epochs=3,\n",
    "        base_model_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "        batch_size=2,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=100,\n",
    "        eval_steps=200,\n",
    "        save_steps=500,\n",
    "        output_dir=\"finetuned-weights-LoRA-EVCL\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0159a88-7c03-406a-a7b9-c308562314ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get the current device index\n",
    "current_device = torch.cuda.current_device()\n",
    "print(\"Current Device Index:\", current_device)\n",
    "\n",
    "# Get the name of the current device\n",
    "device_name = torch.cuda.get_device_name(current_device)\n",
    "print(\"Current Device Name:\", device_name)\n",
    "\n",
    "# Get the number of GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "# List all GPUs\n",
    "for device_id in range(num_gpus):\n",
    "    print(f\"Device {device_id}: {torch.cuda.get_device_name(device_id)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed938bb-c6ce-4049-9431-77e6b5e338f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7904e-a05c-4272-8bcc-74bcc2c82097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2375fcc-a362-4f1a-8156-4bdb9e424063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed5fe3-5ade-44f9-b08a-197b881d4894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e713f74-5c6a-415a-a725-29d8a167ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evcl(\n",
    "    num_tasks: int = 2,  # Assuming tasks 1 (QA) and 2 (QG)\n",
    "    num_epochs: int = 3,\n",
    "    experiment_name: str = 'llama_evcl_superni',\n",
    "    base_model_name: str = \"meta-llama/Llama-2-7b-hf\",\n",
    "    lora_model_path: str = 'fine-tuned-llama-lora',\n",
    "    batch_size: int = 8,\n",
    "    coreset_size: int = 200,\n",
    "    coreset_method: str = 'random',\n",
    "    ewc_lambda: float = 100.0,\n",
    "    ewc_gamma: float = 1.0,\n",
    "    task_folder='QA_FineTuned'\n",
    "):\n",
    "    os.chdir(f'/home/pranav24/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/{task_folder}/finetuned-weights')\n",
    "    \n",
    "    print(\"Loading base model...\")\n",
    "    # Load the model already fine-tuned on the first task\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "    print(\"Applying LoRA adapter...\")\n",
    "    model = PeftModel.from_pretrained(model, lora_model_path)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare the prior using the fine-tuned model\n",
    "    prior = MLEPrior(model)\n",
    "    obs = tyxe.likelihoods.Categorical(-1)\n",
    "    guide = functools.partial(\n",
    "        tyxe.guides.AutoNormal,\n",
    "        init_scale=1e-4,\n",
    "        init_loc_fn=tyxe.guides.PretrainedInitializer.from_net(model, prefix=\"net\")\n",
    "    )\n",
    "\n",
    "    # Initialize Bayesian model\n",
    "    bnn = VariationalBNNWithEWC(model, prior, obs, guide)\n",
    "\n",
    "    # Load the first task's data\n",
    "    train_loader_task1, test_loader_task1 = get_data_loader_for_task1(tokenizer, batch_size)\n",
    "\n",
    "    # Generate the initial coreset from the first task's data\n",
    "    prev_coreset = update_coreset(prev_coreset=[], train_loader=train_loader_task1, coreset_size=coreset_size, selection_method=coreset_method)\n",
    "\n",
    "    # Compute the initial Fisher Information Matrix and previous parameters\n",
    "    prev_fisher_info = compute_fisher_info_llm(\n",
    "        bnn, prev_fisher_info=None, data_loader=train_loader_task1, n_samples=5000, ewc_gamma=ewc_gamma\n",
    "    )\n",
    "    prev_params = {\n",
    "        name: param.detach().clone()\n",
    "        for name, param in bnn.named_parameters()\n",
    "        if 'lora' in name\n",
    "    }\n",
    "\n",
    "    # Now proceed with tasks 2 and onwards\n",
    "    # Prepare tasks 2 to num_tasks\n",
    "    train_loaders, test_loaders = fetch_nlp_datasets(tokenizer, batch_size, num_tasks, start_task=2)\n",
    "\n",
    "    for task_index, train_loader in enumerate(train_loaders, 2):  # Start from task_index=2\n",
    "        print(f\"Training on Task {task_index}...\")\n",
    "\n",
    "        # Update coreset\n",
    "        if coreset_size > 0:\n",
    "            curr_coreset = update_coreset(prev_coreset, train_loader, coreset_size, coreset_method)\n",
    "            # curr_coreset = update_coreset(prev_coreset, train_loader_task1, coreset_size, coreset_method)\n",
    "            # curr_coreset=prev_coreset\n",
    "        else:\n",
    "            curr_coreset = []\n",
    "\n",
    "        # Training loop for current task\n",
    "        def callback(epoch, step, loss):\n",
    "            print(f\"Epoch {epoch}, Step {step}, Loss: {loss}\")\n",
    "\n",
    "        # Fine-tune with variational inference and EWC\n",
    "        update_variational_approx(\n",
    "            bnn, train_loader, curr_coreset, num_epochs, callback, ewc_lambda,\n",
    "            fisher_info=prev_fisher_info, prev_params=prev_params\n",
    "        )\n",
    "\n",
    "        # Compute Fisher Information Matrix for current task\n",
    "        fisher_info = compute_fisher_info_llm(\n",
    "            bnn, prev_fisher_info, train_loader, n_samples=5000, ewc_gamma=ewc_gamma\n",
    "        )\n",
    "\n",
    "        # Update prev_params and prev_fisher_info\n",
    "        prev_params = {\n",
    "            name: param.detach().clone()\n",
    "            for name, param in bnn.named_parameters()\n",
    "            if 'lora' in name\n",
    "        }\n",
    "        prev_fisher_info = fisher_info\n",
    "\n",
    "        # Update prior with posterior from current task\n",
    "        site_names = [site for site in tyxe.util.pyro_sample_sites(bnn) if 'lora' in site]\n",
    "        params_to_update = tyxe.priors.DictPrior({\n",
    "            site: list(bnn.net_guide.get_detached_distributions(site).values())[0]\n",
    "            for site in site_names\n",
    "        })\n",
    "        bnn.update_prior(params_to_update)\n",
    "\n",
    "        # Update prev_coreset\n",
    "        prev_coreset = curr_coreset\n",
    "\n",
    "        # Evaluate on all tasks up to current\n",
    "        for j, test_loader in enumerate([test_loader_task1] + test_loaders[:task_index - 2], 1):\n",
    "            print(f\"Evaluating Task {j}...\")\n",
    "            total_loss = 0.0\n",
    "            num_batches = 0\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    outputs = bnn.net(input_ids, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f\"Task {j} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd853c25-4eab-40f1-a52f-54cb97f9406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_evcl(\n",
    "        num_tasks=2,  # QA and QG tasks\n",
    "        num_epochs=3,\n",
    "        experiment_name='llama_evcl_superni',\n",
    "        base_model_name='meta-llama/Llama-2-7b-hf',\n",
    "        lora_model_path='path/to/your/lora/model',\n",
    "        batch_size=8,\n",
    "        coreset_size=200,  # Adjust as needed\n",
    "        ewc_lambda=100.0,\n",
    "        ewc_gamma=1.0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b94c2a-6f14-4c26-92b0-da100fa4aba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a628a465-6ee6-4673-ba01-0591db3ab9ea",
   "metadata": {},
   "source": [
    "How to Run the Process with SuperNI Dataset\n",
    "\n",
    "Step 1: Environment Setup\n",
    "(Same as previously described)\n",
    "\n",
    "Step 2: Preparing the SuperNI Dataset\n",
    "Install the datasets Library:\n",
    "Ensure you have the datasets library installed:\n",
    "\n",
    "pip install datasets\n",
    "Inspect the SuperNI Dataset:\n",
    "The SuperNI dataset can be loaded using:\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "superni_dataset = load_dataset('super_nat_instruct', 'v1_1')\n",
    "Note: Replace 'super_nat_instruct' and 'v1_1' with the correct dataset identifier if necessary.\n",
    "Identify QA and QG Tasks:\n",
    "SuperNI contains multiple tasks with task descriptions.\n",
    "You need to identify the task IDs or names corresponding to QA and QG.\n",
    "You can print out the tasks to find the ones you need:\n",
    "for task in superni_dataset['train']['Task']:\n",
    "    print(task)\n",
    "Adjust the load_superni_task_dataset Function:\n",
    "Modify the task_filter in load_superni_task_dataset to match the task identifiers for QA and QG.\n",
    "For example:\n",
    "if task_type == 'qa':\n",
    "    task_ids = ['task_id_for_qa1', 'task_id_for_qa2']  # Replace with actual task IDs\n",
    "    task_filter = lambda ex: ex['TaskID'] in task_ids\n",
    "elif task_type == 'qg':\n",
    "    task_ids = ['task_id_for_qg1', 'task_id_for_qg2']  # Replace with actual task IDs\n",
    "    task_filter = lambda ex: ex['TaskID'] in task_ids\n",
    "Adjust Data Preprocessing:\n",
    "Ensure that the Input and Output fields are correctly used.\n",
    "For some tasks, you might need to concatenate context and question.\n",
    "Step 3: Running the Code\n",
    "(Same as previously described)\n",
    "\n",
    "Step 4: Monitoring and Evaluation\n",
    "(Same as previously described)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844525ac-9677-4cc2-846d-bae8ebb87667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "<virtual env>",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
