{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3df9f10-1efb-4437-b04e-e6c2020be1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import tyxe\n",
    "\n",
    "import random\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO, Trace_ELBO\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, TensorDataset\n",
    "\n",
    "from datasets import load_dataset  # Added to load SuperNI dataset\n",
    "\n",
    "from typing import Optional, List\n",
    "from model.mle_prior import MLEPrior\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaff032f-ae3f-4be6-a016-d5ee7237f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current Device Index: 0\n",
      "Current Device Name: NVIDIA A100-SXM4-80GB\n",
      "Number of GPUs: 1\n",
      "Device 0: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "current_device = torch.cuda.current_device()\n",
    "print(\"Current Device Index:\", current_device)\n",
    "\n",
    "device_name = torch.cuda.get_device_name(current_device)\n",
    "print(\"Current Device Name:\", device_name)\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "for device_id in range(num_gpus):\n",
    "    print(f\"Device {device_id}: {torch.cuda.get_device_name(device_id)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3192f-96f4-44da-9eb4-aea2856aab4f",
   "metadata": {},
   "source": [
    "### Task1 -QA LoRA+EVCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18d43227-5365-4537-a585-1f370b504d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_info(\n",
    "    model, \n",
    "    data_loader, \n",
    "    prev_fisher_info=None, \n",
    "    ewc_gamma=1.0, \n",
    "    num_epochs=1, \n",
    "    head_modules=None, \n",
    "    n_samples=None\n",
    "):\n",
    "\n",
    "    fisher = {}\n",
    "    \n",
    "    # Initialize Fisher matrix for LoRA parameters, excluding head modules if provided\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name and (head_modules is None or not any(name.startswith(head) for head in head_modules)):\n",
    "            fisher[name] = torch.zeros_like(param).to(DEVICE)\n",
    "    \n",
    "    # Save the model's current training state and set to eval\n",
    "    old_training_state = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    batch_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            if n_samples is not None and batch_count >= n_samples:\n",
    "                break\n",
    "\n",
    "            print(f\"Processing batch {batch_count + 1}\")\n",
    "            model.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            try:\n",
    "                # with autocast(device_type='cuda'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "            # scaler.scale(loss).backward()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {batch_count + 1}: {e}\")\n",
    "                break\n",
    "\n",
    "            # Accumulate Fisher information for LoRA parameters\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'lora' in name and param.grad is not None and (head_modules is None or not any(name.startswith(head) for head in head_modules)):\n",
    "                    fisher[name] += param.grad.data ** 2\n",
    "\n",
    "            print(f\"Completed batch {batch_count + 1}\")\n",
    "            batch_count += 1\n",
    "\n",
    "    # Normalize Fisher information by the number of processed batches or samples\n",
    "    normalization_factor = batch_count if n_samples is None else min(batch_count, n_samples)\n",
    "    for name in fisher:\n",
    "        fisher[name] = fisher[name] / normalization_factor\n",
    "\n",
    "    # Integrate previous Fisher information with EWC scaling\n",
    "    if prev_fisher_info is not None:\n",
    "        for name in fisher:\n",
    "            if name in prev_fisher_info:\n",
    "                fisher[name] += ewc_gamma * prev_fisher_info[name]\n",
    "\n",
    "    # Restore the model's original training state\n",
    "    model.train(old_training_state)\n",
    "    \n",
    "    return fisher\n",
    "\n",
    "# Function to get variational posterior means\n",
    "def get_variational_posterior_means(model):\n",
    "    posterior_means = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'lora_A'):\n",
    "            # print('yes')\n",
    "            for key in module.lora_A:\n",
    "                param_name = f\"{name}.lora_A.{key}\"\n",
    "                loc_name = f\"{param_name}_loc\"\n",
    "                if loc_name in pyro.get_param_store():\n",
    "                    lora_A_loc = pyro.param(loc_name).detach().clone()\n",
    "                    # Add '.weight' to the parameter name\n",
    "                    posterior_means[f\"{param_name}.weight\"] = lora_A_loc\n",
    "        if hasattr(module, 'lora_B'):\n",
    "            # print('yes')\n",
    "            for key in module.lora_B:\n",
    "                param_name = f\"{name}.lora_B.{key}\"\n",
    "                loc_name = f\"{param_name}_loc\"\n",
    "                if loc_name in pyro.get_param_store():\n",
    "                    lora_B_loc = pyro.param(loc_name).detach().clone()\n",
    "                    # Add '.weight' to the parameter name\n",
    "                    posterior_means[f\"{param_name}.weight\"] = lora_B_loc\n",
    "    return posterior_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4265f3da-cf93-4c14-b872-82c7f2817cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import LoraLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc12ed2-8b42-4e1e-87f5-ed00d1eb2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import init_empty_weights\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "\n",
    "\n",
    "def initialize_lora():\n",
    "    login(\"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\")\n",
    "    # Set environment variable to manage memory fragmentation\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    \n",
    "     \n",
    "    # Specify directories and the path to the zip file\n",
    "    offload_dir = os.path.expanduser(\"llama_offload_evcl/\")\n",
    "     \n",
    "    os.makedirs(offload_dir, exist_ok=True)\n",
    "     \n",
    "    # Extract only the specified JSON file from the zip archive\n",
    "    os.chdir('/home/pranav24/cs-546-project/SSR/Latest_Weights/QA_Weights')\n",
    "    target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "     \n",
    "    # Load tokenizer from Hugging Face\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "    # Load the model with accelerate's offloading and device map auto-setup\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B\",\n",
    "            device_map=\"auto\",\n",
    "            # max_memory=max_memory,\n",
    "            offload_folder=offload_dir,\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True\n",
    "        )\n",
    "     \n",
    "    # Configure LoRA with reduced rank\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    #printing the trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if 'lora' in name:\n",
    "    #         print(name)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62ff472-4fa4-47c2-b2bf-f39fd4fbfbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc21088618ce409281781d5619e8faaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading base model...\")\n",
    "model,tokenizer=initialize_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57385901-11a1-491b-abb3-1b924f4d6cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07124e6af574836810d893099d0f231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav24/cs-546-project/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import init_empty_weights\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "os.chdir('/home/pranav24/cs-546-project/SSR/Latest_Weights/QA_Weights')\n",
    "target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "instances = json_data['Instances'][0:2500]\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"output\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"attention_mask\"] = model_inputs.get(\"attention_mask\", None)\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization and set format\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Split dataset into train and eval\n",
    "train_size = int(0.8 * len(tokenized_datasets))\n",
    "train_dataset = tokenized_datasets.select(range(train_size))\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6015f59-7b73-46b6-940a-9f1ec22c3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(model, tokenizer, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93cecb98-c125-4a32-a87f-05e4addea699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            outputs = model(input_ids, labels=labels, attention_mask=attention_mask)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Evaluation Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ae06200-7093-45ff-bec3-abc1e93834af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "evaluation_loss=[]\n",
    "\n",
    "def run_lora_evcl_1(\n",
    "    num_epochs: int = 100,\n",
    "    base_model_name: str = \"meta-llama/Meta-Llama-3-8B\",\n",
    "    batch_size: int = 2,\n",
    "    learning_rate: float = 2e-4,\n",
    "    logging_steps: int = 100,\n",
    "    eval_steps: int = 20,\n",
    "    save_steps: int = 500,\n",
    "    output_dir: str = \"finetuned-weights-LoRA-EVCL\",\n",
    "):\n",
    "\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False  # Freeze non-LoRA parameters\n",
    "\n",
    "    def bayesian_guide(input_ids, attention_mask, labels):\n",
    "        # Define variational distributions over the LoRA parameters\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, 'lora_A'):\n",
    "                for key in module.lora_A:\n",
    "                    param_name = f\"{name}.lora_A.{key}\"\n",
    "                    lora_A_param = module.lora_A[key].weight\n",
    "                    device = lora_A_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_A_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_A_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_A_param.dim())\n",
    "                    )\n",
    "            if hasattr(module, 'lora_B'):\n",
    "                for key in module.lora_B:\n",
    "                    param_name = f\"{name}.lora_B.{key}\"\n",
    "                    lora_B_param = module.lora_B[key].weight\n",
    "                    device = lora_B_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_B_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_B_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_B_param.dim())\n",
    "                    )\n",
    "                    \n",
    "    def bayesian_model(input_ids, attention_mask, labels):\n",
    "        # Define a function to sample and substitute LoRA parameters\n",
    "        def model_with_sampled_lora():\n",
    "            # Sample LoRA parameters and set them in the model\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, 'lora_A'):\n",
    "                    for key in module.lora_A:\n",
    "                        param_name = f\"{name}.lora_A.{key}\"\n",
    "                        lora_A_module = module.lora_A[key]\n",
    "                        device = lora_A_module.weight.device\n",
    "    \n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                lora_A_module.weight.detach().to(device),\n",
    "                                (0.1 * torch.ones_like(lora_A_module.weight)).to(device)\n",
    "                            ).to_event(lora_A_module.weight.dim())\n",
    "                        )\n",
    "    \n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_A_module.weight.copy_(sampled_weight)\n",
    "    \n",
    "                if hasattr(module, 'lora_B'):\n",
    "                    for key in module.lora_B:\n",
    "                        param_name = f\"{name}.lora_B.{key}\"\n",
    "                        lora_B_module = module.lora_B[key]\n",
    "                        device = lora_B_module.weight.device\n",
    "    \n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                lora_B_module.weight.detach().to(device),\n",
    "                                (0.1 * torch.ones_like(lora_B_module.weight)).to(device)\n",
    "                            ).to_event(lora_B_module.weight.dim())\n",
    "                        )\n",
    "    \n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_B_module.weight.copy_(sampled_weight)\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            return loss\n",
    "    \n",
    "        # Use the modified model with sampled LoRA parameters\n",
    "        return model_with_sampled_lora()\n",
    "\n",
    "\n",
    "    # Set up SVI\n",
    "    pyro.clear_param_store()\n",
    "    # optim = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "    optim = pyro.optim.PyroOptim(AdamW, {\"lr\": learning_rate, \"weight_decay\": 1e-5})\n",
    "    elbo = TraceMeanField_ELBO()\n",
    "    svi = SVI(bayesian_model, bayesian_guide, optim, loss=elbo)\n",
    "    scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optim.pt_optim_constructor([p for p in model.parameters() if p.requires_grad]),\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(train_loader) * num_epochs,\n",
    ")\n",
    "\n",
    "    \n",
    "    # Training loop for Task 1\n",
    "    print(f\"Training on Task 1...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for num_batches, batch in enumerate(train_loader, 1):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            loss = svi.step(input_ids, attention_mask, labels)\n",
    "            total_loss += loss\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scheduler.step()\n",
    "\n",
    "            # Logging\n",
    "            if num_batches % logging_steps == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch {epoch}, Step {num_batches}, Loss: {avg_loss}\")\n",
    "\n",
    "            # Evaluation\n",
    "            if num_batches % eval_steps == 0:\n",
    "                eval_loss=evaluate_model(model, eval_loader)\n",
    "                evaluation_loss.append(eval_loss)\n",
    "                \n",
    "\n",
    "            # Save checkpoints\n",
    "            if num_batches % save_steps == 0:\n",
    "                save_trained_model(model, tokenizer, output_dir)\n",
    "\n",
    "        avg_epoch_loss = total_loss / num_batches\n",
    "        print(f\"Task 1 Epoch {epoch} completed. Average Loss: {avg_epoch_loss}\")\n",
    "\n",
    "        # Save the final trained model after Task 1\n",
    "        if epoch%15 ==0:\n",
    "            save_trained_model(model, tokenizer, output_dir)\n",
    "            pyro.get_param_store().save('pyro_param_store_task1.pt')\n",
    "    \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a88408ab-cd21-43bf-a8bb-6c9f4e8e3b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pranav24/cs-546-project/SSR/Latest_Weights/QA_Weights\n",
      "/home/pranav24/cs-546-project\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('/home/pranav24/cs-546-project/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed87466-51c2-43f4-bccc-e0710e0b8fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Task 1...\n",
      "Evaluation Loss: 14.5942\n",
      "Evaluation Loss: 13.3917\n",
      "Evaluation Loss: 11.6520\n",
      "Evaluation Loss: 13.7036\n",
      "Epoch 0, Step 100, Loss: 1704047.64625\n",
      "Evaluation Loss: 13.9954\n",
      "Evaluation Loss: 15.2600\n",
      "Evaluation Loss: 12.7381\n",
      "Evaluation Loss: 16.9244\n",
      "Evaluation Loss: 12.5366\n",
      "Epoch 0, Step 200, Loss: 1703948.27625\n",
      "Evaluation Loss: 13.3007\n",
      "Evaluation Loss: 12.1357\n",
      "Evaluation Loss: 13.8521\n",
      "Task 1 Epoch 0 completed. Average Loss: 1703939.5525\n",
      "Evaluation Loss: 12.7660\n",
      "Evaluation Loss: 17.5034\n",
      "Evaluation Loss: 14.1891\n",
      "Evaluation Loss: 12.1262\n",
      "Epoch 1, Step 100, Loss: 1703812.1525\n",
      "Evaluation Loss: 15.8960\n",
      "Evaluation Loss: 15.7041\n",
      "Evaluation Loss: 11.0468\n",
      "Evaluation Loss: 15.4577\n",
      "Evaluation Loss: 9.9875\n",
      "Epoch 1, Step 200, Loss: 1703828.008125\n",
      "Evaluation Loss: 14.6006\n",
      "Evaluation Loss: 16.8933\n",
      "Evaluation Loss: 18.0348\n",
      "Task 1 Epoch 1 completed. Average Loss: 1703845.544\n",
      "Evaluation Loss: 9.7949\n",
      "Evaluation Loss: 14.3534\n",
      "Evaluation Loss: 13.0281\n",
      "Evaluation Loss: 16.1358\n",
      "Epoch 2, Step 100, Loss: 1703902.87\n",
      "Evaluation Loss: 10.3672\n",
      "Evaluation Loss: 16.9499\n",
      "Evaluation Loss: 11.5772\n",
      "Evaluation Loss: 17.2301\n",
      "Evaluation Loss: 14.4756\n",
      "Epoch 2, Step 200, Loss: 1703820.566875\n",
      "Evaluation Loss: 13.7563\n",
      "Evaluation Loss: 14.9767\n",
      "Evaluation Loss: 9.5853\n",
      "Task 1 Epoch 2 completed. Average Loss: 1703848.0845\n",
      "Evaluation Loss: 13.2001\n",
      "Evaluation Loss: 12.9122\n",
      "Evaluation Loss: 14.7124\n",
      "Evaluation Loss: 13.3758\n",
      "Epoch 3, Step 100, Loss: 1704015.16625\n",
      "Evaluation Loss: 16.5503\n",
      "Evaluation Loss: 16.0930\n",
      "Evaluation Loss: 14.2516\n",
      "Evaluation Loss: 11.3570\n",
      "Evaluation Loss: 14.6004\n",
      "Epoch 3, Step 200, Loss: 1703924.135\n",
      "Evaluation Loss: 9.8137\n",
      "Evaluation Loss: 10.9157\n",
      "Evaluation Loss: 15.7511\n",
      "Task 1 Epoch 3 completed. Average Loss: 1703913.3945\n",
      "Evaluation Loss: 14.6911\n",
      "Evaluation Loss: 14.9880\n",
      "Evaluation Loss: 15.0984\n",
      "Evaluation Loss: 12.1901\n",
      "Epoch 4, Step 100, Loss: 1703846.16125\n",
      "Evaluation Loss: 10.2699\n",
      "Evaluation Loss: 14.6590\n",
      "Evaluation Loss: 11.2717\n",
      "Evaluation Loss: 14.0984\n",
      "Evaluation Loss: 14.9296\n",
      "Epoch 4, Step 200, Loss: 1703969.92625\n",
      "Evaluation Loss: 16.3405\n",
      "Evaluation Loss: 12.2536\n",
      "Evaluation Loss: 14.0069\n",
      "Task 1 Epoch 4 completed. Average Loss: 1704010.633\n",
      "Evaluation Loss: 16.0786\n",
      "Evaluation Loss: 16.8091\n",
      "Evaluation Loss: 12.4945\n",
      "Evaluation Loss: 10.6006\n",
      "Epoch 5, Step 100, Loss: 1703805.23625\n",
      "Evaluation Loss: 17.0391\n",
      "Evaluation Loss: 14.6965\n",
      "Evaluation Loss: 11.7326\n",
      "Evaluation Loss: 13.1967\n",
      "Evaluation Loss: 9.3649\n",
      "Epoch 5, Step 200, Loss: 1703707.555\n",
      "Evaluation Loss: 15.3368\n",
      "Evaluation Loss: 16.6840\n",
      "Evaluation Loss: 14.2493\n",
      "Task 1 Epoch 5 completed. Average Loss: 1703757.8935\n",
      "Evaluation Loss: 13.6402\n",
      "Evaluation Loss: 12.5660\n",
      "Evaluation Loss: 15.7136\n",
      "Evaluation Loss: 12.9367\n",
      "Epoch 6, Step 100, Loss: 1703754.5975\n",
      "Evaluation Loss: 10.0338\n",
      "Evaluation Loss: 12.5710\n",
      "Evaluation Loss: 14.0639\n",
      "Evaluation Loss: 11.8356\n",
      "Evaluation Loss: 12.8626\n",
      "Epoch 6, Step 200, Loss: 1703786.626875\n",
      "Evaluation Loss: 14.5373\n",
      "Evaluation Loss: 12.9491\n",
      "Evaluation Loss: 12.5001\n",
      "Task 1 Epoch 6 completed. Average Loss: 1703816.2625\n",
      "Evaluation Loss: 14.1721\n",
      "Evaluation Loss: 14.8762\n",
      "Evaluation Loss: 13.7470\n",
      "Evaluation Loss: 13.5609\n",
      "Epoch 7, Step 100, Loss: 1704049.53125\n",
      "Evaluation Loss: 16.0601\n",
      "Evaluation Loss: 14.8765\n",
      "Evaluation Loss: 11.4048\n",
      "Evaluation Loss: 11.6350\n",
      "Evaluation Loss: 16.4653\n",
      "Epoch 7, Step 200, Loss: 1703930.671875\n",
      "Evaluation Loss: 15.0196\n",
      "Evaluation Loss: 12.4495\n",
      "Evaluation Loss: 12.3952\n",
      "Task 1 Epoch 7 completed. Average Loss: 1703888.6905\n",
      "Evaluation Loss: 13.4726\n",
      "Evaluation Loss: 18.9280\n",
      "Evaluation Loss: 13.6908\n",
      "Evaluation Loss: 19.2062\n",
      "Epoch 8, Step 100, Loss: 1704061.0325\n",
      "Evaluation Loss: 12.6878\n",
      "Evaluation Loss: 12.0324\n",
      "Evaluation Loss: 16.3282\n",
      "Evaluation Loss: 15.5249\n",
      "Evaluation Loss: 13.9443\n",
      "Epoch 8, Step 200, Loss: 1703968.73\n",
      "Evaluation Loss: 13.0750\n",
      "Evaluation Loss: 12.8404\n",
      "Evaluation Loss: 13.5695\n",
      "Task 1 Epoch 8 completed. Average Loss: 1704039.924\n",
      "Evaluation Loss: 14.0222\n",
      "Evaluation Loss: 13.9784\n",
      "Evaluation Loss: 12.9995\n",
      "Evaluation Loss: 11.3908\n",
      "Epoch 9, Step 100, Loss: 1703854.96125\n",
      "Evaluation Loss: 17.5134\n",
      "Evaluation Loss: 13.9366\n",
      "Evaluation Loss: 13.2852\n",
      "Evaluation Loss: 10.6896\n",
      "Evaluation Loss: 14.2121\n",
      "Epoch 9, Step 200, Loss: 1703988.39125\n",
      "Evaluation Loss: 16.7162\n",
      "Evaluation Loss: 14.3650\n",
      "Evaluation Loss: 13.5493\n",
      "Task 1 Epoch 9 completed. Average Loss: 1704039.9515\n",
      "Evaluation Loss: 11.7133\n",
      "Evaluation Loss: 15.4973\n",
      "Evaluation Loss: 17.4129\n",
      "Evaluation Loss: 16.9366\n",
      "Epoch 10, Step 100, Loss: 1703996.7525\n",
      "Evaluation Loss: 13.7099\n",
      "Evaluation Loss: 13.8936\n",
      "Evaluation Loss: 15.8680\n",
      "Evaluation Loss: 15.4595\n",
      "Evaluation Loss: 13.4391\n",
      "Epoch 10, Step 200, Loss: 1703988.480625\n",
      "Evaluation Loss: 11.5144\n",
      "Evaluation Loss: 13.6783\n",
      "Evaluation Loss: 14.5748\n",
      "Task 1 Epoch 10 completed. Average Loss: 1703961.1215\n",
      "Evaluation Loss: 11.6535\n",
      "Evaluation Loss: 17.2772\n",
      "Evaluation Loss: 11.5763\n",
      "Evaluation Loss: 14.2197\n",
      "Epoch 11, Step 100, Loss: 1703878.825\n",
      "Evaluation Loss: 17.7858\n",
      "Evaluation Loss: 11.6724\n",
      "Evaluation Loss: 11.3615\n",
      "Evaluation Loss: 15.0882\n",
      "Evaluation Loss: 13.2584\n",
      "Epoch 11, Step 200, Loss: 1703907.584375\n",
      "Evaluation Loss: 14.6802\n",
      "Evaluation Loss: 14.9673\n",
      "Evaluation Loss: 12.0650\n",
      "Task 1 Epoch 11 completed. Average Loss: 1703943.7015\n",
      "Evaluation Loss: 10.8815\n",
      "Evaluation Loss: 14.2918\n",
      "Evaluation Loss: 16.5813\n",
      "Evaluation Loss: 12.3200\n",
      "Epoch 12, Step 100, Loss: 1703859.985\n",
      "Evaluation Loss: 16.1555\n",
      "Evaluation Loss: 13.0354\n",
      "Evaluation Loss: 15.2893\n",
      "Evaluation Loss: 20.3462\n",
      "Evaluation Loss: 15.7727\n",
      "Epoch 12, Step 200, Loss: 1703970.265625\n",
      "Evaluation Loss: 13.6762\n",
      "Evaluation Loss: 14.6345\n",
      "Evaluation Loss: 15.4016\n",
      "Task 1 Epoch 12 completed. Average Loss: 1704007.337\n",
      "Evaluation Loss: 11.6119\n",
      "Evaluation Loss: 14.7883\n",
      "Evaluation Loss: 16.1959\n",
      "Evaluation Loss: 14.8678\n",
      "Epoch 13, Step 100, Loss: 1703945.64125\n",
      "Evaluation Loss: 14.3903\n",
      "Evaluation Loss: 11.5105\n",
      "Evaluation Loss: 11.4948\n",
      "Evaluation Loss: 15.5386\n",
      "Evaluation Loss: 14.2414\n",
      "Epoch 13, Step 200, Loss: 1704044.98125\n",
      "Evaluation Loss: 12.4084\n",
      "Evaluation Loss: 15.6616\n",
      "Evaluation Loss: 14.5989\n",
      "Task 1 Epoch 13 completed. Average Loss: 1704027.441\n",
      "Evaluation Loss: 13.4817\n",
      "Evaluation Loss: 9.9948\n",
      "Evaluation Loss: 12.1930\n",
      "Evaluation Loss: 17.0033\n",
      "Epoch 14, Step 100, Loss: 1703854.3075\n",
      "Evaluation Loss: 10.8926\n",
      "Evaluation Loss: 15.6426\n",
      "Evaluation Loss: 16.4370\n",
      "Evaluation Loss: 17.2078\n",
      "Evaluation Loss: 15.5638\n",
      "Epoch 14, Step 200, Loss: 1703804.88\n",
      "Evaluation Loss: 19.0945\n",
      "Evaluation Loss: 14.7533\n",
      "Evaluation Loss: 13.3225\n",
      "Task 1 Epoch 14 completed. Average Loss: 1703936.0005\n",
      "Evaluation Loss: 15.8888\n",
      "Evaluation Loss: 10.9586\n",
      "Evaluation Loss: 13.5010\n",
      "Evaluation Loss: 18.5185\n",
      "Epoch 15, Step 100, Loss: 1703907.76875\n",
      "Evaluation Loss: 10.2315\n",
      "Evaluation Loss: 8.6265\n",
      "Evaluation Loss: 14.8057\n",
      "Evaluation Loss: 12.5387\n",
      "Evaluation Loss: 16.7426\n",
      "Epoch 15, Step 200, Loss: 1704008.140625\n",
      "Evaluation Loss: 14.4104\n",
      "Evaluation Loss: 13.0350\n",
      "Evaluation Loss: 9.6175\n",
      "Task 1 Epoch 15 completed. Average Loss: 1703965.753\n",
      "Evaluation Loss: 14.6831\n",
      "Evaluation Loss: 14.1132\n",
      "Evaluation Loss: 14.3295\n",
      "Evaluation Loss: 15.0064\n",
      "Epoch 16, Step 100, Loss: 1703933.0175\n",
      "Evaluation Loss: 11.9626\n",
      "Evaluation Loss: 14.6048\n",
      "Evaluation Loss: 15.1571\n",
      "Evaluation Loss: 11.0617\n",
      "Evaluation Loss: 15.6479\n",
      "Epoch 16, Step 200, Loss: 1703846.3125\n",
      "Evaluation Loss: 12.8283\n",
      "Evaluation Loss: 15.3480\n",
      "Evaluation Loss: 10.0291\n",
      "Task 1 Epoch 16 completed. Average Loss: 1703832.4055\n",
      "Evaluation Loss: 14.1971\n",
      "Evaluation Loss: 16.1159\n",
      "Evaluation Loss: 13.2665\n",
      "Evaluation Loss: 14.1734\n",
      "Epoch 17, Step 100, Loss: 1703994.08625\n",
      "Evaluation Loss: 14.8708\n",
      "Evaluation Loss: 17.1111\n",
      "Evaluation Loss: 10.5861\n",
      "Evaluation Loss: 11.4962\n",
      "Evaluation Loss: 11.5813\n",
      "Epoch 17, Step 200, Loss: 1703889.120625\n",
      "Evaluation Loss: 17.9630\n",
      "Evaluation Loss: 10.1768\n",
      "Evaluation Loss: 9.8495\n",
      "Task 1 Epoch 17 completed. Average Loss: 1703931.554\n",
      "Evaluation Loss: 11.3480\n",
      "Evaluation Loss: 13.2083\n",
      "Evaluation Loss: 12.1030\n",
      "Evaluation Loss: 14.6151\n",
      "Epoch 18, Step 100, Loss: 1703952.57875\n",
      "Evaluation Loss: 17.7886\n",
      "Evaluation Loss: 14.9798\n",
      "Evaluation Loss: 10.6658\n",
      "Evaluation Loss: 15.0348\n",
      "Evaluation Loss: 10.4983\n",
      "Epoch 18, Step 200, Loss: 1703969.014375\n",
      "Evaluation Loss: 13.6471\n",
      "Evaluation Loss: 15.0704\n",
      "Evaluation Loss: 16.1166\n",
      "Task 1 Epoch 18 completed. Average Loss: 1704014.506\n",
      "Evaluation Loss: 14.6921\n",
      "Evaluation Loss: 13.4576\n",
      "Evaluation Loss: 13.4290\n",
      "Evaluation Loss: 14.5832\n",
      "Epoch 19, Step 100, Loss: 1703712.1725\n",
      "Evaluation Loss: 12.9832\n",
      "Evaluation Loss: 15.3322\n",
      "Evaluation Loss: 13.0640\n",
      "Evaluation Loss: 10.8248\n",
      "Evaluation Loss: 11.7800\n",
      "Epoch 19, Step 200, Loss: 1703823.293125\n",
      "Evaluation Loss: 11.2666\n",
      "Evaluation Loss: 11.1368\n",
      "Evaluation Loss: 13.6019\n",
      "Task 1 Epoch 19 completed. Average Loss: 1703866.0705\n",
      "Evaluation Loss: 14.5587\n",
      "Evaluation Loss: 14.3858\n",
      "Evaluation Loss: 13.8358\n",
      "Evaluation Loss: 13.0659\n",
      "Epoch 20, Step 100, Loss: 1704072.7425\n",
      "Evaluation Loss: 10.6001\n",
      "Evaluation Loss: 14.7790\n",
      "Evaluation Loss: 16.1564\n",
      "Evaluation Loss: 12.5699\n",
      "Evaluation Loss: 13.9441\n",
      "Epoch 20, Step 200, Loss: 1704088.00625\n",
      "Evaluation Loss: 14.3150\n",
      "Evaluation Loss: 11.9589\n",
      "Evaluation Loss: 17.7749\n",
      "Task 1 Epoch 20 completed. Average Loss: 1704040.465\n",
      "Evaluation Loss: 13.2420\n",
      "Evaluation Loss: 9.5643\n",
      "Evaluation Loss: 17.8809\n",
      "Evaluation Loss: 14.6772\n",
      "Epoch 21, Step 100, Loss: 1703616.9875\n",
      "Evaluation Loss: 10.6942\n",
      "Evaluation Loss: 13.8874\n",
      "Evaluation Loss: 15.5848\n",
      "Evaluation Loss: 15.4160\n",
      "Evaluation Loss: 10.4832\n",
      "Epoch 21, Step 200, Loss: 1703856.499375\n",
      "Evaluation Loss: 9.4351\n",
      "Evaluation Loss: 14.7377\n",
      "Evaluation Loss: 14.4705\n",
      "Task 1 Epoch 21 completed. Average Loss: 1703946.634\n",
      "Evaluation Loss: 13.5205\n",
      "Evaluation Loss: 14.7424\n",
      "Evaluation Loss: 14.1308\n",
      "Evaluation Loss: 16.7194\n",
      "Epoch 22, Step 100, Loss: 1704270.62625\n",
      "Evaluation Loss: 13.8324\n",
      "Evaluation Loss: 11.4255\n",
      "Evaluation Loss: 13.4220\n",
      "Evaluation Loss: 11.8069\n",
      "Evaluation Loss: 13.6131\n",
      "Epoch 22, Step 200, Loss: 1704135.485625\n",
      "Evaluation Loss: 12.2086\n",
      "Evaluation Loss: 16.7075\n",
      "Evaluation Loss: 11.3829\n",
      "Task 1 Epoch 22 completed. Average Loss: 1704117.7305\n",
      "Evaluation Loss: 12.1255\n",
      "Evaluation Loss: 13.6205\n",
      "Evaluation Loss: 12.6305\n",
      "Evaluation Loss: 13.6807\n",
      "Epoch 23, Step 100, Loss: 1703862.335\n",
      "Evaluation Loss: 15.9595\n",
      "Evaluation Loss: 11.3771\n",
      "Evaluation Loss: 10.3694\n",
      "Evaluation Loss: 13.1646\n",
      "Evaluation Loss: 16.9967\n",
      "Epoch 23, Step 200, Loss: 1703887.314375\n",
      "Evaluation Loss: 13.8420\n",
      "Evaluation Loss: 15.7496\n",
      "Evaluation Loss: 10.7346\n",
      "Task 1 Epoch 23 completed. Average Loss: 1703864.862\n",
      "Evaluation Loss: 14.3849\n",
      "Evaluation Loss: 12.3226\n",
      "Evaluation Loss: 18.1056\n",
      "Evaluation Loss: 13.9299\n",
      "Epoch 24, Step 100, Loss: 1704038.48\n",
      "Evaluation Loss: 16.3770\n",
      "Evaluation Loss: 13.4672\n",
      "Evaluation Loss: 15.0657\n",
      "Evaluation Loss: 14.0387\n",
      "Evaluation Loss: 16.6401\n",
      "Epoch 24, Step 200, Loss: 1704025.47\n",
      "Evaluation Loss: 12.5084\n",
      "Evaluation Loss: 15.0390\n",
      "Evaluation Loss: 13.7521\n",
      "Task 1 Epoch 24 completed. Average Loss: 1703999.652\n",
      "Evaluation Loss: 16.2728\n",
      "Evaluation Loss: 12.2499\n",
      "Evaluation Loss: 16.0791\n",
      "Evaluation Loss: 14.5678\n",
      "Epoch 25, Step 100, Loss: 1703856.35\n",
      "Evaluation Loss: 11.2171\n",
      "Evaluation Loss: 14.9310\n",
      "Evaluation Loss: 15.5070\n",
      "Evaluation Loss: 12.3520\n",
      "Evaluation Loss: 14.1311\n",
      "Epoch 25, Step 200, Loss: 1703849.641875\n",
      "Evaluation Loss: 13.4125\n",
      "Evaluation Loss: 13.3780\n",
      "Evaluation Loss: 16.7365\n",
      "Task 1 Epoch 25 completed. Average Loss: 1703830.5105\n",
      "Evaluation Loss: 15.3554\n",
      "Evaluation Loss: 16.3941\n",
      "Evaluation Loss: 11.5028\n",
      "Evaluation Loss: 11.7306\n",
      "Epoch 26, Step 100, Loss: 1704005.3725\n",
      "Evaluation Loss: 15.1480\n",
      "Evaluation Loss: 15.7354\n",
      "Evaluation Loss: 17.5190\n",
      "Evaluation Loss: 11.6921\n",
      "Evaluation Loss: 12.6677\n",
      "Epoch 26, Step 200, Loss: 1704024.353125\n",
      "Evaluation Loss: 13.2621\n",
      "Evaluation Loss: 14.5975\n",
      "Evaluation Loss: 13.0183\n",
      "Task 1 Epoch 26 completed. Average Loss: 1704028.252\n",
      "Evaluation Loss: 14.7820\n",
      "Evaluation Loss: 12.5427\n",
      "Evaluation Loss: 10.9994\n",
      "Evaluation Loss: 13.0635\n",
      "Epoch 27, Step 100, Loss: 1703966.46375\n",
      "Evaluation Loss: 16.0876\n",
      "Evaluation Loss: 15.8870\n",
      "Evaluation Loss: 15.4798\n",
      "Evaluation Loss: 11.8296\n",
      "Evaluation Loss: 15.1416\n",
      "Epoch 27, Step 200, Loss: 1703904.114375\n",
      "Evaluation Loss: 11.8106\n",
      "Evaluation Loss: 10.4538\n",
      "Evaluation Loss: 12.0323\n",
      "Task 1 Epoch 27 completed. Average Loss: 1703873.325\n",
      "Evaluation Loss: 15.3185\n",
      "Evaluation Loss: 13.2928\n",
      "Evaluation Loss: 12.4336\n",
      "Evaluation Loss: 13.2945\n",
      "Epoch 28, Step 100, Loss: 1703839.99875\n",
      "Evaluation Loss: 11.1336\n",
      "Evaluation Loss: 17.8463\n",
      "Evaluation Loss: 14.6878\n",
      "Evaluation Loss: 16.1858\n",
      "Evaluation Loss: 12.4532\n",
      "Epoch 28, Step 200, Loss: 1703985.941875\n",
      "Evaluation Loss: 12.8945\n",
      "Evaluation Loss: 12.2258\n",
      "Evaluation Loss: 14.4884\n",
      "Task 1 Epoch 28 completed. Average Loss: 1704009.2215\n",
      "Evaluation Loss: 10.7075\n",
      "Evaluation Loss: 9.8978\n",
      "Evaluation Loss: 13.7783\n",
      "Evaluation Loss: 11.1433\n",
      "Epoch 29, Step 100, Loss: 1703960.5975\n",
      "Evaluation Loss: 13.7028\n",
      "Evaluation Loss: 11.2873\n",
      "Evaluation Loss: 16.4347\n",
      "Evaluation Loss: 13.7244\n",
      "Evaluation Loss: 12.4253\n",
      "Epoch 29, Step 200, Loss: 1703979.8875\n",
      "Evaluation Loss: 21.6502\n",
      "Evaluation Loss: 13.3091\n",
      "Evaluation Loss: 12.1739\n",
      "Task 1 Epoch 29 completed. Average Loss: 1703978.1525\n",
      "Evaluation Loss: 12.6996\n",
      "Evaluation Loss: 13.1022\n",
      "Evaluation Loss: 8.4451\n",
      "Evaluation Loss: 13.9373\n",
      "Epoch 30, Step 100, Loss: 1704138.92875\n",
      "Evaluation Loss: 15.9523\n",
      "Evaluation Loss: 14.9823\n",
      "Evaluation Loss: 12.3289\n",
      "Evaluation Loss: 12.8593\n",
      "Evaluation Loss: 11.2707\n",
      "Epoch 30, Step 200, Loss: 1703904.90125\n",
      "Evaluation Loss: 14.8163\n",
      "Evaluation Loss: 13.8290\n",
      "Evaluation Loss: 11.1593\n",
      "Task 1 Epoch 30 completed. Average Loss: 1703917.569\n",
      "Evaluation Loss: 13.2408\n",
      "Evaluation Loss: 13.2345\n",
      "Evaluation Loss: 13.6212\n",
      "Evaluation Loss: 15.8801\n",
      "Epoch 31, Step 100, Loss: 1703873.04375\n",
      "Evaluation Loss: 14.0845\n",
      "Evaluation Loss: 12.5272\n",
      "Evaluation Loss: 15.0562\n",
      "Evaluation Loss: 13.8302\n",
      "Evaluation Loss: 12.5768\n",
      "Epoch 31, Step 200, Loss: 1703915.288125\n",
      "Evaluation Loss: 14.4833\n",
      "Evaluation Loss: 14.5043\n",
      "Evaluation Loss: 14.0199\n",
      "Task 1 Epoch 31 completed. Average Loss: 1703938.638\n",
      "Evaluation Loss: 14.5250\n",
      "Evaluation Loss: 15.3156\n",
      "Evaluation Loss: 17.4797\n",
      "Evaluation Loss: 11.9340\n",
      "Epoch 32, Step 100, Loss: 1704070.01\n",
      "Evaluation Loss: 11.9213\n",
      "Evaluation Loss: 17.4943\n",
      "Evaluation Loss: 14.8060\n",
      "Evaluation Loss: 14.3632\n",
      "Evaluation Loss: 9.0453\n",
      "Epoch 32, Step 200, Loss: 1704034.3125\n",
      "Evaluation Loss: 12.5226\n",
      "Evaluation Loss: 15.0409\n",
      "Evaluation Loss: 13.4330\n",
      "Task 1 Epoch 32 completed. Average Loss: 1704015.3765\n",
      "Evaluation Loss: 14.6467\n",
      "Evaluation Loss: 13.3764\n",
      "Evaluation Loss: 13.7091\n",
      "Evaluation Loss: 12.7387\n",
      "Epoch 33, Step 100, Loss: 1703706.3925\n",
      "Evaluation Loss: 12.6071\n",
      "Evaluation Loss: 12.2551\n",
      "Evaluation Loss: 12.6806\n",
      "Evaluation Loss: 14.0967\n",
      "Evaluation Loss: 15.2903\n",
      "Epoch 33, Step 200, Loss: 1703874.548125\n",
      "Evaluation Loss: 11.5786\n",
      "Evaluation Loss: 13.2130\n",
      "Evaluation Loss: 13.0230\n",
      "Task 1 Epoch 33 completed. Average Loss: 1703869.2525\n",
      "Evaluation Loss: 11.5735\n",
      "Evaluation Loss: 14.4840\n",
      "Evaluation Loss: 17.0808\n",
      "Evaluation Loss: 13.9850\n",
      "Epoch 34, Step 100, Loss: 1703790.805\n",
      "Evaluation Loss: 15.7897\n",
      "Evaluation Loss: 13.9302\n",
      "Evaluation Loss: 16.2733\n",
      "Evaluation Loss: 12.4398\n",
      "Evaluation Loss: 15.5990\n",
      "Epoch 34, Step 200, Loss: 1703953.73125\n",
      "Evaluation Loss: 11.5335\n",
      "Evaluation Loss: 11.5209\n",
      "Evaluation Loss: 14.8011\n",
      "Task 1 Epoch 34 completed. Average Loss: 1703958.449\n",
      "Evaluation Loss: 13.7188\n",
      "Evaluation Loss: 17.6871\n",
      "Evaluation Loss: 15.3220\n",
      "Evaluation Loss: 10.0489\n",
      "Epoch 35, Step 100, Loss: 1703657.2425\n",
      "Evaluation Loss: 15.0599\n",
      "Evaluation Loss: 13.0411\n",
      "Evaluation Loss: 12.9447\n",
      "Evaluation Loss: 14.3009\n",
      "Evaluation Loss: 12.8463\n",
      "Epoch 35, Step 200, Loss: 1703891.396875\n",
      "Evaluation Loss: 12.8590\n",
      "Evaluation Loss: 12.4108\n",
      "Evaluation Loss: 11.7794\n",
      "Task 1 Epoch 35 completed. Average Loss: 1703933.4285\n",
      "Evaluation Loss: 14.6468\n",
      "Evaluation Loss: 11.2041\n",
      "Evaluation Loss: 15.3299\n",
      "Evaluation Loss: 14.1805\n",
      "Epoch 36, Step 100, Loss: 1703886.21125\n",
      "Evaluation Loss: 11.8555\n",
      "Evaluation Loss: 14.0210\n",
      "Evaluation Loss: 11.0400\n",
      "Evaluation Loss: 14.6860\n",
      "Evaluation Loss: 13.0687\n",
      "Epoch 36, Step 200, Loss: 1703864.06875\n",
      "Evaluation Loss: 8.6751\n",
      "Evaluation Loss: 11.8928\n",
      "Evaluation Loss: 9.9681\n",
      "Task 1 Epoch 36 completed. Average Loss: 1703860.0135\n",
      "Evaluation Loss: 13.3223\n",
      "Evaluation Loss: 14.6797\n",
      "Evaluation Loss: 14.0944\n",
      "Evaluation Loss: 14.7848\n",
      "Epoch 37, Step 100, Loss: 1703758.80125\n",
      "Evaluation Loss: 13.4989\n",
      "Evaluation Loss: 11.5225\n",
      "Evaluation Loss: 15.6183\n",
      "Evaluation Loss: 9.0032\n",
      "Evaluation Loss: 12.5072\n",
      "Epoch 37, Step 200, Loss: 1703929.56375\n",
      "Evaluation Loss: 16.5249\n",
      "Evaluation Loss: 11.2491\n",
      "Evaluation Loss: 15.7534\n",
      "Task 1 Epoch 37 completed. Average Loss: 1703933.4145\n",
      "Evaluation Loss: 17.0981\n",
      "Evaluation Loss: 14.6633\n",
      "Evaluation Loss: 13.2208\n",
      "Evaluation Loss: 11.6795\n",
      "Epoch 38, Step 100, Loss: 1704018.69125\n",
      "Evaluation Loss: 12.6450\n",
      "Evaluation Loss: 19.0219\n",
      "Evaluation Loss: 14.1750\n",
      "Evaluation Loss: 14.1508\n",
      "Evaluation Loss: 14.5115\n",
      "Epoch 38, Step 200, Loss: 1703984.53625\n",
      "Evaluation Loss: 15.5889\n",
      "Evaluation Loss: 11.2326\n",
      "Evaluation Loss: 16.5988\n",
      "Task 1 Epoch 38 completed. Average Loss: 1703950.736\n",
      "Evaluation Loss: 16.3782\n",
      "Evaluation Loss: 14.7445\n",
      "Evaluation Loss: 12.3800\n",
      "Evaluation Loss: 13.7045\n",
      "Epoch 39, Step 100, Loss: 1703779.045\n",
      "Evaluation Loss: 16.2677\n",
      "Evaluation Loss: 14.7795\n",
      "Evaluation Loss: 12.1758\n",
      "Evaluation Loss: 14.1106\n",
      "Evaluation Loss: 13.7689\n",
      "Epoch 39, Step 200, Loss: 1703915.65625\n",
      "Evaluation Loss: 16.3723\n",
      "Evaluation Loss: 13.7979\n",
      "Evaluation Loss: 13.5508\n",
      "Task 1 Epoch 39 completed. Average Loss: 1703941.973\n",
      "Evaluation Loss: 10.6431\n",
      "Evaluation Loss: 13.3946\n",
      "Evaluation Loss: 15.1667\n",
      "Evaluation Loss: 11.3376\n",
      "Epoch 40, Step 100, Loss: 1703934.525\n",
      "Evaluation Loss: 13.3363\n",
      "Evaluation Loss: 12.2038\n",
      "Evaluation Loss: 13.8585\n",
      "Evaluation Loss: 16.0106\n",
      "Evaluation Loss: 15.7060\n",
      "Epoch 40, Step 200, Loss: 1703877.874375\n",
      "Evaluation Loss: 14.7520\n",
      "Evaluation Loss: 11.5877\n",
      "Evaluation Loss: 13.1997\n",
      "Task 1 Epoch 40 completed. Average Loss: 1703803.0105\n",
      "Evaluation Loss: 14.0857\n",
      "Evaluation Loss: 12.4210\n",
      "Evaluation Loss: 16.5993\n",
      "Evaluation Loss: 9.8613\n",
      "Epoch 41, Step 100, Loss: 1704051.1225\n",
      "Evaluation Loss: 15.7026\n",
      "Evaluation Loss: 17.5438\n",
      "Evaluation Loss: 13.9959\n",
      "Evaluation Loss: 15.0359\n",
      "Evaluation Loss: 11.4371\n",
      "Epoch 41, Step 200, Loss: 1703935.183125\n",
      "Evaluation Loss: 17.9696\n",
      "Evaluation Loss: 17.4512\n",
      "Evaluation Loss: 8.4827\n",
      "Task 1 Epoch 41 completed. Average Loss: 1703927.1275\n",
      "Evaluation Loss: 13.8690\n",
      "Evaluation Loss: 15.3449\n",
      "Evaluation Loss: 12.1761\n",
      "Evaluation Loss: 11.3247\n",
      "Epoch 42, Step 100, Loss: 1703941.97\n",
      "Evaluation Loss: 14.6560\n",
      "Evaluation Loss: 14.6546\n",
      "Evaluation Loss: 11.6376\n",
      "Evaluation Loss: 15.2114\n",
      "Evaluation Loss: 16.1544\n",
      "Epoch 42, Step 200, Loss: 1704019.29\n",
      "Evaluation Loss: 13.7551\n",
      "Evaluation Loss: 12.3948\n",
      "Evaluation Loss: 19.0544\n",
      "Task 1 Epoch 42 completed. Average Loss: 1703998.4855\n",
      "Evaluation Loss: 14.3849\n",
      "Evaluation Loss: 11.5270\n",
      "Evaluation Loss: 13.8120\n",
      "Evaluation Loss: 13.2278\n",
      "Epoch 43, Step 100, Loss: 1703940.60125\n",
      "Evaluation Loss: 16.2808\n",
      "Evaluation Loss: 13.4929\n",
      "Evaluation Loss: 13.2076\n",
      "Evaluation Loss: 15.0779\n",
      "Evaluation Loss: 14.7607\n",
      "Epoch 43, Step 200, Loss: 1703843.4575\n",
      "Evaluation Loss: 14.8918\n",
      "Evaluation Loss: 13.2073\n",
      "Evaluation Loss: 15.2010\n",
      "Task 1 Epoch 43 completed. Average Loss: 1703903.816\n",
      "Evaluation Loss: 14.9303\n",
      "Evaluation Loss: 16.4724\n",
      "Evaluation Loss: 12.4423\n",
      "Evaluation Loss: 13.8719\n",
      "Epoch 44, Step 100, Loss: 1703868.6425\n",
      "Evaluation Loss: 13.2324\n",
      "Evaluation Loss: 18.1747\n",
      "Evaluation Loss: 14.1915\n",
      "Evaluation Loss: 13.0469\n",
      "Evaluation Loss: 13.7563\n",
      "Epoch 44, Step 200, Loss: 1703761.465625\n",
      "Evaluation Loss: 13.1237\n",
      "Evaluation Loss: 13.3028\n",
      "Evaluation Loss: 12.7451\n",
      "Task 1 Epoch 44 completed. Average Loss: 1703797.651\n",
      "Evaluation Loss: 12.4172\n",
      "Evaluation Loss: 15.1284\n",
      "Evaluation Loss: 14.7768\n",
      "Evaluation Loss: 10.1926\n",
      "Epoch 45, Step 100, Loss: 1703958.1825\n",
      "Evaluation Loss: 12.4813\n",
      "Evaluation Loss: 13.9149\n",
      "Evaluation Loss: 14.2460\n",
      "Evaluation Loss: 18.6695\n",
      "Evaluation Loss: 16.0256\n",
      "Epoch 45, Step 200, Loss: 1703968.088125\n",
      "Evaluation Loss: 12.4341\n",
      "Evaluation Loss: 11.5669\n",
      "Evaluation Loss: 16.3572\n",
      "Task 1 Epoch 45 completed. Average Loss: 1703915.0215\n",
      "Evaluation Loss: 16.6338\n",
      "Evaluation Loss: 13.4800\n",
      "Evaluation Loss: 14.8456\n",
      "Evaluation Loss: 11.8893\n",
      "Epoch 46, Step 100, Loss: 1703851.34375\n",
      "Evaluation Loss: 14.3380\n",
      "Evaluation Loss: 12.7368\n",
      "Evaluation Loss: 14.7305\n",
      "Evaluation Loss: 14.0558\n",
      "Evaluation Loss: 16.0192\n",
      "Epoch 46, Step 200, Loss: 1703997.348125\n",
      "Evaluation Loss: 15.4986\n",
      "Evaluation Loss: 14.4183\n",
      "Evaluation Loss: 13.3807\n",
      "Task 1 Epoch 46 completed. Average Loss: 1704040.913\n",
      "Evaluation Loss: 13.6499\n",
      "Evaluation Loss: 15.2472\n",
      "Evaluation Loss: 16.2914\n",
      "Evaluation Loss: 12.3552\n",
      "Epoch 47, Step 100, Loss: 1703796.295\n",
      "Evaluation Loss: 13.7087\n",
      "Evaluation Loss: 19.6243\n",
      "Evaluation Loss: 15.3675\n",
      "Evaluation Loss: 11.2084\n",
      "Evaluation Loss: 8.8249\n",
      "Epoch 47, Step 200, Loss: 1703846.934375\n",
      "Evaluation Loss: 13.9061\n",
      "Evaluation Loss: 14.7327\n",
      "Evaluation Loss: 15.2655\n",
      "Task 1 Epoch 47 completed. Average Loss: 1703881.4085\n",
      "Evaluation Loss: 11.2995\n",
      "Evaluation Loss: 12.8961\n",
      "Evaluation Loss: 13.0704\n",
      "Evaluation Loss: 9.2111\n",
      "Epoch 48, Step 100, Loss: 1704236.5\n",
      "Evaluation Loss: 12.8504\n",
      "Evaluation Loss: 14.1629\n",
      "Evaluation Loss: 14.4425\n",
      "Evaluation Loss: 15.1752\n",
      "Evaluation Loss: 14.2205\n",
      "Epoch 48, Step 200, Loss: 1704183.52\n",
      "Evaluation Loss: 11.9212\n",
      "Evaluation Loss: 11.0992\n",
      "Evaluation Loss: 12.6964\n",
      "Task 1 Epoch 48 completed. Average Loss: 1704216.339\n",
      "Evaluation Loss: 11.2927\n",
      "Evaluation Loss: 10.3461\n",
      "Evaluation Loss: 17.8334\n",
      "Evaluation Loss: 10.0185\n",
      "Epoch 49, Step 100, Loss: 1703904.05375\n",
      "Evaluation Loss: 12.1976\n",
      "Evaluation Loss: 15.1120\n",
      "Evaluation Loss: 19.7097\n",
      "Evaluation Loss: 18.0039\n",
      "Evaluation Loss: 10.8283\n",
      "Epoch 49, Step 200, Loss: 1703916.8475\n",
      "Evaluation Loss: 13.2321\n",
      "Evaluation Loss: 16.7023\n",
      "Evaluation Loss: 13.1537\n",
      "Task 1 Epoch 49 completed. Average Loss: 1703890.803\n",
      "Evaluation Loss: 12.7473\n",
      "Evaluation Loss: 15.1347\n",
      "Evaluation Loss: 12.8334\n",
      "Evaluation Loss: 13.1299\n",
      "Epoch 50, Step 100, Loss: 1703848.80375\n",
      "Evaluation Loss: 13.4181\n",
      "Evaluation Loss: 12.2541\n",
      "Evaluation Loss: 16.2887\n",
      "Evaluation Loss: 17.5841\n",
      "Evaluation Loss: 14.0779\n",
      "Epoch 50, Step 200, Loss: 1703949.17\n",
      "Evaluation Loss: 11.6141\n",
      "Evaluation Loss: 16.1421\n",
      "Evaluation Loss: 12.3245\n",
      "Task 1 Epoch 50 completed. Average Loss: 1703920.788\n",
      "Evaluation Loss: 13.4966\n",
      "Evaluation Loss: 10.3863\n",
      "Evaluation Loss: 11.1941\n",
      "Evaluation Loss: 16.4937\n",
      "Epoch 51, Step 100, Loss: 1704194.5025\n",
      "Evaluation Loss: 16.2741\n",
      "Evaluation Loss: 16.1646\n",
      "Evaluation Loss: 15.5296\n",
      "Evaluation Loss: 15.6391\n",
      "Evaluation Loss: 15.9411\n",
      "Epoch 51, Step 200, Loss: 1704005.43375\n",
      "Evaluation Loss: 11.6559\n",
      "Evaluation Loss: 15.7300\n",
      "Evaluation Loss: 15.6226\n",
      "Task 1 Epoch 51 completed. Average Loss: 1703992.8175\n",
      "Evaluation Loss: 11.9203\n",
      "Evaluation Loss: 15.2744\n",
      "Evaluation Loss: 15.3177\n",
      "Evaluation Loss: 15.8109\n",
      "Epoch 52, Step 100, Loss: 1703816.03\n",
      "Evaluation Loss: 13.6339\n",
      "Evaluation Loss: 11.5647\n",
      "Evaluation Loss: 19.5802\n",
      "Evaluation Loss: 13.3069\n",
      "Evaluation Loss: 13.0945\n",
      "Epoch 52, Step 200, Loss: 1703909.861875\n",
      "Evaluation Loss: 17.2245\n",
      "Evaluation Loss: 9.8298\n",
      "Evaluation Loss: 11.9370\n",
      "Task 1 Epoch 52 completed. Average Loss: 1703920.0195\n",
      "Evaluation Loss: 14.9232\n",
      "Evaluation Loss: 11.2075\n",
      "Evaluation Loss: 15.4829\n",
      "Evaluation Loss: 10.4420\n",
      "Epoch 53, Step 100, Loss: 1703875.4025\n",
      "Evaluation Loss: 15.1929\n",
      "Evaluation Loss: 15.8772\n",
      "Evaluation Loss: 17.8242\n",
      "Evaluation Loss: 15.5486\n",
      "Evaluation Loss: 14.4228\n",
      "Epoch 53, Step 200, Loss: 1703885.8175\n",
      "Evaluation Loss: 15.6630\n",
      "Evaluation Loss: 12.9933\n",
      "Evaluation Loss: 13.8729\n",
      "Task 1 Epoch 53 completed. Average Loss: 1703890.0445\n",
      "Evaluation Loss: 13.6607\n",
      "Evaluation Loss: 11.7262\n",
      "Evaluation Loss: 12.4179\n",
      "Evaluation Loss: 11.9612\n",
      "Epoch 54, Step 100, Loss: 1703959.20875\n",
      "Evaluation Loss: 15.0443\n",
      "Evaluation Loss: 16.8098\n",
      "Evaluation Loss: 11.7845\n",
      "Evaluation Loss: 12.3363\n",
      "Evaluation Loss: 13.4845\n",
      "Epoch 54, Step 200, Loss: 1703971.429375\n",
      "Evaluation Loss: 16.2342\n",
      "Evaluation Loss: 11.6065\n",
      "Evaluation Loss: 12.2193\n",
      "Task 1 Epoch 54 completed. Average Loss: 1704005.8705\n",
      "Evaluation Loss: 10.0270\n",
      "Evaluation Loss: 15.7822\n",
      "Evaluation Loss: 14.8178\n",
      "Evaluation Loss: 15.6067\n",
      "Epoch 55, Step 100, Loss: 1704269.04125\n",
      "Evaluation Loss: 11.0077\n",
      "Evaluation Loss: 10.5055\n",
      "Evaluation Loss: 13.9000\n",
      "Evaluation Loss: 14.8616\n",
      "Evaluation Loss: 8.1074\n",
      "Epoch 55, Step 200, Loss: 1704078.498125\n",
      "Evaluation Loss: 12.8401\n",
      "Evaluation Loss: 17.0616\n",
      "Evaluation Loss: 12.7944\n",
      "Task 1 Epoch 55 completed. Average Loss: 1704046.641\n",
      "Evaluation Loss: 13.6964\n",
      "Evaluation Loss: 11.4589\n",
      "Evaluation Loss: 13.5458\n",
      "Evaluation Loss: 10.4134\n",
      "Epoch 56, Step 100, Loss: 1703795.06875\n",
      "Evaluation Loss: 14.2479\n",
      "Evaluation Loss: 13.8213\n",
      "Evaluation Loss: 13.8321\n",
      "Evaluation Loss: 13.1137\n",
      "Evaluation Loss: 19.0016\n",
      "Epoch 56, Step 200, Loss: 1703799.52125\n",
      "Evaluation Loss: 15.5299\n",
      "Evaluation Loss: 15.9749\n",
      "Evaluation Loss: 15.5795\n",
      "Task 1 Epoch 56 completed. Average Loss: 1703831.362\n",
      "Evaluation Loss: 11.5466\n",
      "Evaluation Loss: 14.8219\n",
      "Evaluation Loss: 8.5380\n",
      "Evaluation Loss: 13.1111\n",
      "Epoch 57, Step 100, Loss: 1703936.06875\n",
      "Evaluation Loss: 14.7383\n",
      "Evaluation Loss: 13.7517\n",
      "Evaluation Loss: 14.7175\n",
      "Evaluation Loss: 14.0070\n",
      "Evaluation Loss: 14.5000\n",
      "Epoch 57, Step 200, Loss: 1703969.5325\n",
      "Evaluation Loss: 11.5143\n",
      "Evaluation Loss: 13.4206\n",
      "Evaluation Loss: 15.1492\n",
      "Task 1 Epoch 57 completed. Average Loss: 1703934.301\n",
      "Evaluation Loss: 12.5527\n",
      "Evaluation Loss: 11.1969\n",
      "Evaluation Loss: 16.5121\n",
      "Evaluation Loss: 13.5478\n",
      "Epoch 58, Step 100, Loss: 1703999.62875\n",
      "Evaluation Loss: 11.0246\n",
      "Evaluation Loss: 12.5022\n",
      "Evaluation Loss: 12.2877\n",
      "Evaluation Loss: 19.3957\n",
      "Evaluation Loss: 9.0083\n",
      "Epoch 58, Step 200, Loss: 1703991.718125\n",
      "Evaluation Loss: 14.4731\n",
      "Evaluation Loss: 12.2713\n",
      "Evaluation Loss: 14.0024\n",
      "Task 1 Epoch 58 completed. Average Loss: 1703973.161\n",
      "Evaluation Loss: 13.7518\n",
      "Evaluation Loss: 12.1207\n",
      "Evaluation Loss: 14.4800\n",
      "Evaluation Loss: 12.3572\n",
      "Epoch 59, Step 100, Loss: 1703918.43125\n",
      "Evaluation Loss: 11.3368\n",
      "Evaluation Loss: 13.3777\n",
      "Evaluation Loss: 14.1889\n",
      "Evaluation Loss: 11.9292\n",
      "Evaluation Loss: 16.8770\n",
      "Epoch 59, Step 200, Loss: 1703967.673125\n",
      "Evaluation Loss: 11.1625\n",
      "Evaluation Loss: 11.1121\n",
      "Evaluation Loss: 13.3528\n",
      "Task 1 Epoch 59 completed. Average Loss: 1703929.817\n",
      "Evaluation Loss: 12.5429\n",
      "Evaluation Loss: 11.2650\n",
      "Evaluation Loss: 10.8144\n",
      "Evaluation Loss: 16.0625\n",
      "Epoch 60, Step 100, Loss: 1703967.02\n",
      "Evaluation Loss: 14.0074\n",
      "Evaluation Loss: 13.0586\n",
      "Evaluation Loss: 16.5660\n",
      "Evaluation Loss: 14.1838\n",
      "Evaluation Loss: 14.3261\n",
      "Epoch 60, Step 200, Loss: 1703967.71375\n",
      "Evaluation Loss: 16.0330\n",
      "Evaluation Loss: 11.0661\n",
      "Evaluation Loss: 12.9616\n",
      "Task 1 Epoch 60 completed. Average Loss: 1703932.9315\n",
      "Evaluation Loss: 12.5672\n",
      "Evaluation Loss: 13.6190\n",
      "Evaluation Loss: 16.5314\n",
      "Evaluation Loss: 14.2579\n",
      "Epoch 61, Step 100, Loss: 1703932.49125\n",
      "Evaluation Loss: 14.7223\n",
      "Evaluation Loss: 12.6209\n",
      "Evaluation Loss: 15.8049\n",
      "Evaluation Loss: 14.6191\n",
      "Evaluation Loss: 16.5448\n",
      "Epoch 61, Step 200, Loss: 1703857.47875\n",
      "Evaluation Loss: 17.8001\n",
      "Evaluation Loss: 10.5757\n",
      "Evaluation Loss: 16.5635\n",
      "Task 1 Epoch 61 completed. Average Loss: 1703892.3335\n",
      "Evaluation Loss: 12.9461\n",
      "Evaluation Loss: 12.9143\n",
      "Evaluation Loss: 11.7748\n",
      "Evaluation Loss: 11.8319\n",
      "Epoch 62, Step 100, Loss: 1703953.18125\n",
      "Evaluation Loss: 15.3440\n",
      "Evaluation Loss: 13.2397\n",
      "Evaluation Loss: 13.6815\n",
      "Evaluation Loss: 15.4092\n",
      "Evaluation Loss: 18.5124\n",
      "Epoch 62, Step 200, Loss: 1704055.61125\n",
      "Evaluation Loss: 14.6362\n",
      "Evaluation Loss: 14.0117\n",
      "Evaluation Loss: 15.0286\n",
      "Task 1 Epoch 62 completed. Average Loss: 1703992.762\n",
      "Evaluation Loss: 12.2563\n",
      "Evaluation Loss: 12.4059\n",
      "Evaluation Loss: 17.5712\n",
      "Evaluation Loss: 15.7207\n",
      "Epoch 63, Step 100, Loss: 1704005.96\n",
      "Evaluation Loss: 13.3333\n",
      "Evaluation Loss: 13.1430\n",
      "Evaluation Loss: 16.3880\n",
      "Evaluation Loss: 14.4840\n",
      "Evaluation Loss: 17.4569\n",
      "Epoch 63, Step 200, Loss: 1703997.244375\n",
      "Evaluation Loss: 13.5149\n",
      "Evaluation Loss: 10.6698\n",
      "Evaluation Loss: 13.1415\n",
      "Task 1 Epoch 63 completed. Average Loss: 1704025.742\n",
      "Evaluation Loss: 13.3425\n",
      "Evaluation Loss: 12.4814\n",
      "Evaluation Loss: 15.1745\n",
      "Evaluation Loss: 12.1867\n",
      "Epoch 64, Step 100, Loss: 1703979.45125\n",
      "Evaluation Loss: 11.8468\n",
      "Evaluation Loss: 18.2459\n",
      "Evaluation Loss: 12.5520\n",
      "Evaluation Loss: 18.1896\n",
      "Evaluation Loss: 9.3414\n",
      "Epoch 64, Step 200, Loss: 1703948.6575\n",
      "Evaluation Loss: 14.2941\n",
      "Evaluation Loss: 14.6783\n",
      "Evaluation Loss: 13.8896\n",
      "Task 1 Epoch 64 completed. Average Loss: 1703973.7725\n",
      "Evaluation Loss: 14.2398\n",
      "Evaluation Loss: 11.7541\n",
      "Evaluation Loss: 11.9703\n",
      "Evaluation Loss: 14.2003\n",
      "Epoch 65, Step 100, Loss: 1704103.7375\n",
      "Evaluation Loss: 11.2249\n",
      "Evaluation Loss: 12.5347\n",
      "Evaluation Loss: 12.4083\n",
      "Evaluation Loss: 17.0560\n",
      "Evaluation Loss: 13.1770\n",
      "Epoch 65, Step 200, Loss: 1703992.58875\n",
      "Evaluation Loss: 14.7875\n",
      "Evaluation Loss: 13.9568\n",
      "Evaluation Loss: 11.9285\n",
      "Task 1 Epoch 65 completed. Average Loss: 1703933.871\n",
      "Evaluation Loss: 12.4972\n",
      "Evaluation Loss: 16.6389\n",
      "Evaluation Loss: 14.8642\n",
      "Evaluation Loss: 13.6611\n",
      "Epoch 66, Step 100, Loss: 1703829.95625\n",
      "Evaluation Loss: 13.3976\n",
      "Evaluation Loss: 15.8019\n",
      "Evaluation Loss: 12.2115\n",
      "Evaluation Loss: 15.6058\n",
      "Evaluation Loss: 16.7623\n",
      "Epoch 66, Step 200, Loss: 1703843.900625\n",
      "Evaluation Loss: 14.6577\n",
      "Evaluation Loss: 10.6734\n",
      "Evaluation Loss: 14.6623\n",
      "Task 1 Epoch 66 completed. Average Loss: 1703806.2985\n",
      "Evaluation Loss: 14.5328\n",
      "Evaluation Loss: 12.9246\n",
      "Evaluation Loss: 14.9157\n",
      "Evaluation Loss: 13.7535\n",
      "Epoch 67, Step 100, Loss: 1704168.0725\n",
      "Evaluation Loss: 13.1561\n",
      "Evaluation Loss: 11.7578\n",
      "Evaluation Loss: 15.3267\n",
      "Evaluation Loss: 12.9837\n",
      "Evaluation Loss: 13.2444\n",
      "Epoch 67, Step 200, Loss: 1704110.395\n",
      "Evaluation Loss: 15.7405\n",
      "Evaluation Loss: 16.3999\n",
      "Evaluation Loss: 10.0698\n",
      "Task 1 Epoch 67 completed. Average Loss: 1704111.9395\n",
      "Evaluation Loss: 13.7711\n",
      "Evaluation Loss: 11.7880\n",
      "Evaluation Loss: 16.5259\n",
      "Evaluation Loss: 13.4937\n",
      "Epoch 68, Step 100, Loss: 1703988.88\n",
      "Evaluation Loss: 11.1465\n",
      "Evaluation Loss: 15.2100\n",
      "Evaluation Loss: 14.6656\n",
      "Evaluation Loss: 15.1490\n",
      "Evaluation Loss: 12.7064\n",
      "Epoch 68, Step 200, Loss: 1703903.91625\n",
      "Evaluation Loss: 12.2711\n",
      "Evaluation Loss: 14.7879\n",
      "Evaluation Loss: 17.6535\n",
      "Task 1 Epoch 68 completed. Average Loss: 1703939.08\n",
      "Evaluation Loss: 14.8147\n",
      "Evaluation Loss: 11.4196\n",
      "Evaluation Loss: 14.3675\n",
      "Evaluation Loss: 13.4748\n",
      "Epoch 69, Step 100, Loss: 1704057.34375\n",
      "Evaluation Loss: 13.8968\n",
      "Evaluation Loss: 18.2798\n",
      "Evaluation Loss: 14.3837\n",
      "Evaluation Loss: 16.2233\n",
      "Evaluation Loss: 15.6295\n",
      "Epoch 69, Step 200, Loss: 1704025.49\n",
      "Evaluation Loss: 14.3343\n",
      "Evaluation Loss: 12.0292\n",
      "Evaluation Loss: 16.0391\n",
      "Task 1 Epoch 69 completed. Average Loss: 1704047.335\n",
      "Evaluation Loss: 17.4016\n",
      "Evaluation Loss: 14.0462\n",
      "Evaluation Loss: 11.3784\n",
      "Evaluation Loss: 18.7011\n",
      "Epoch 70, Step 100, Loss: 1703949.6\n",
      "Evaluation Loss: 10.4404\n",
      "Evaluation Loss: 11.9194\n",
      "Evaluation Loss: 12.1523\n",
      "Evaluation Loss: 13.7572\n",
      "Evaluation Loss: 14.6205\n",
      "Epoch 70, Step 200, Loss: 1704046.2075\n",
      "Evaluation Loss: 14.8898\n",
      "Evaluation Loss: 14.8201\n",
      "Evaluation Loss: 10.1119\n",
      "Task 1 Epoch 70 completed. Average Loss: 1704000.218\n",
      "Evaluation Loss: 14.6147\n",
      "Evaluation Loss: 13.7211\n",
      "Evaluation Loss: 12.6305\n",
      "Evaluation Loss: 11.8325\n",
      "Epoch 71, Step 100, Loss: 1703840.635\n",
      "Evaluation Loss: 15.4614\n",
      "Evaluation Loss: 15.2177\n",
      "Evaluation Loss: 11.6477\n",
      "Evaluation Loss: 13.8086\n",
      "Evaluation Loss: 15.3704\n",
      "Epoch 71, Step 200, Loss: 1703826.60125\n",
      "Evaluation Loss: 13.6912\n",
      "Evaluation Loss: 13.5428\n",
      "Evaluation Loss: 13.8750\n",
      "Task 1 Epoch 71 completed. Average Loss: 1703870.863\n",
      "Evaluation Loss: 15.4393\n",
      "Evaluation Loss: 12.9531\n",
      "Evaluation Loss: 13.8021\n",
      "Evaluation Loss: 14.9195\n",
      "Epoch 72, Step 100, Loss: 1703616.6\n",
      "Evaluation Loss: 14.7387\n",
      "Evaluation Loss: 12.0622\n",
      "Evaluation Loss: 17.1089\n",
      "Evaluation Loss: 14.5156\n",
      "Evaluation Loss: 14.4085\n",
      "Epoch 72, Step 200, Loss: 1703747.440625\n",
      "Evaluation Loss: 14.9512\n",
      "Evaluation Loss: 15.1049\n",
      "Evaluation Loss: 13.3935\n",
      "Task 1 Epoch 72 completed. Average Loss: 1703769.8465\n",
      "Evaluation Loss: 15.0643\n",
      "Evaluation Loss: 13.9832\n",
      "Evaluation Loss: 15.6447\n",
      "Evaluation Loss: 8.5796\n",
      "Epoch 73, Step 100, Loss: 1703878.865\n",
      "Evaluation Loss: 14.1092\n",
      "Evaluation Loss: 16.3950\n",
      "Evaluation Loss: 15.4071\n",
      "Evaluation Loss: 13.5576\n",
      "Evaluation Loss: 15.3994\n",
      "Epoch 73, Step 200, Loss: 1703803.57375\n",
      "Evaluation Loss: 16.2023\n",
      "Evaluation Loss: 12.2456\n",
      "Evaluation Loss: 14.4198\n",
      "Task 1 Epoch 73 completed. Average Loss: 1703849.208\n",
      "Evaluation Loss: 7.5202\n",
      "Evaluation Loss: 13.8919\n",
      "Evaluation Loss: 14.1672\n",
      "Evaluation Loss: 14.1731\n",
      "Epoch 74, Step 100, Loss: 1704106.51375\n",
      "Evaluation Loss: 17.5945\n",
      "Evaluation Loss: 14.1779\n",
      "Evaluation Loss: 14.6338\n",
      "Evaluation Loss: 15.7038\n",
      "Evaluation Loss: 13.4329\n",
      "Epoch 74, Step 200, Loss: 1704076.1425\n",
      "Evaluation Loss: 15.6105\n",
      "Evaluation Loss: 17.8128\n",
      "Evaluation Loss: 14.7547\n",
      "Task 1 Epoch 74 completed. Average Loss: 1704007.0435\n",
      "Evaluation Loss: 11.5409\n",
      "Evaluation Loss: 15.0017\n",
      "Evaluation Loss: 11.9500\n",
      "Evaluation Loss: 15.6421\n",
      "Epoch 75, Step 100, Loss: 1703876.35625\n",
      "Evaluation Loss: 16.1491\n",
      "Evaluation Loss: 11.8932\n",
      "Evaluation Loss: 8.6210\n",
      "Evaluation Loss: 12.5211\n",
      "Evaluation Loss: 10.3037\n",
      "Epoch 75, Step 200, Loss: 1703933.386875\n",
      "Evaluation Loss: 15.3096\n",
      "Evaluation Loss: 12.0637\n",
      "Evaluation Loss: 13.0569\n",
      "Task 1 Epoch 75 completed. Average Loss: 1703866.3055\n",
      "Evaluation Loss: 16.4716\n",
      "Evaluation Loss: 13.8851\n",
      "Evaluation Loss: 11.4112\n",
      "Evaluation Loss: 17.1646\n",
      "Epoch 76, Step 100, Loss: 1703883.8875\n",
      "Evaluation Loss: 15.0406\n",
      "Evaluation Loss: 13.6021\n",
      "Evaluation Loss: 17.2875\n",
      "Evaluation Loss: 14.3068\n",
      "Evaluation Loss: 18.8193\n",
      "Epoch 76, Step 200, Loss: 1703879.4775\n",
      "Evaluation Loss: 14.8828\n",
      "Evaluation Loss: 14.3444\n",
      "Evaluation Loss: 11.3250\n",
      "Task 1 Epoch 76 completed. Average Loss: 1703891.426\n",
      "Evaluation Loss: 13.2219\n",
      "Evaluation Loss: 11.7100\n",
      "Evaluation Loss: 9.2231\n",
      "Evaluation Loss: 13.1632\n",
      "Epoch 77, Step 100, Loss: 1703848.455\n",
      "Evaluation Loss: 12.4885\n",
      "Evaluation Loss: 12.0561\n",
      "Evaluation Loss: 15.8875\n",
      "Evaluation Loss: 13.1553\n",
      "Evaluation Loss: 12.0536\n",
      "Epoch 77, Step 200, Loss: 1703842.46625\n",
      "Evaluation Loss: 14.7837\n",
      "Evaluation Loss: 17.4401\n",
      "Evaluation Loss: 15.5536\n",
      "Task 1 Epoch 77 completed. Average Loss: 1703849.4945\n",
      "Evaluation Loss: 12.1274\n",
      "Evaluation Loss: 15.0674\n",
      "Evaluation Loss: 16.2570\n",
      "Evaluation Loss: 11.5757\n",
      "Epoch 78, Step 100, Loss: 1703892.1075\n",
      "Evaluation Loss: 15.5670\n",
      "Evaluation Loss: 13.3124\n",
      "Evaluation Loss: 13.3033\n",
      "Evaluation Loss: 11.9953\n",
      "Evaluation Loss: 13.1617\n",
      "Epoch 78, Step 200, Loss: 1703955.23625\n",
      "Evaluation Loss: 12.4193\n",
      "Evaluation Loss: 14.8321\n",
      "Evaluation Loss: 15.2050\n",
      "Task 1 Epoch 78 completed. Average Loss: 1703955.075\n",
      "Evaluation Loss: 10.2475\n",
      "Evaluation Loss: 12.1721\n",
      "Evaluation Loss: 12.4546\n",
      "Evaluation Loss: 14.7873\n",
      "Epoch 79, Step 100, Loss: 1704017.22875\n",
      "Evaluation Loss: 16.6848\n",
      "Evaluation Loss: 10.0585\n",
      "Evaluation Loss: 12.1109\n",
      "Evaluation Loss: 12.5694\n",
      "Evaluation Loss: 12.8712\n",
      "Epoch 79, Step 200, Loss: 1703968.80875\n",
      "Evaluation Loss: 16.7365\n",
      "Evaluation Loss: 15.5263\n",
      "Evaluation Loss: 14.1164\n",
      "Task 1 Epoch 79 completed. Average Loss: 1703922.71\n",
      "Evaluation Loss: 13.3055\n",
      "Evaluation Loss: 16.3735\n",
      "Evaluation Loss: 14.8001\n",
      "Evaluation Loss: 10.2114\n",
      "Epoch 80, Step 100, Loss: 1704038.41125\n",
      "Evaluation Loss: 16.9744\n",
      "Evaluation Loss: 17.0778\n",
      "Evaluation Loss: 14.8629\n",
      "Evaluation Loss: 13.4563\n",
      "Evaluation Loss: 15.0610\n",
      "Epoch 80, Step 200, Loss: 1703984.325625\n",
      "Evaluation Loss: 14.4177\n",
      "Evaluation Loss: 11.9915\n",
      "Evaluation Loss: 11.5103\n",
      "Task 1 Epoch 80 completed. Average Loss: 1703971.631\n",
      "Evaluation Loss: 13.7969\n",
      "Evaluation Loss: 10.4343\n",
      "Evaluation Loss: 11.8680\n",
      "Evaluation Loss: 14.4254\n",
      "Epoch 81, Step 100, Loss: 1703900.94875\n",
      "Evaluation Loss: 15.2388\n",
      "Evaluation Loss: 11.5634\n",
      "Evaluation Loss: 10.0115\n",
      "Evaluation Loss: 16.3314\n",
      "Evaluation Loss: 9.5620\n",
      "Epoch 81, Step 200, Loss: 1703930.014375\n",
      "Evaluation Loss: 17.2387\n",
      "Evaluation Loss: 13.2810\n",
      "Evaluation Loss: 15.9213\n",
      "Task 1 Epoch 81 completed. Average Loss: 1703912.717\n",
      "Evaluation Loss: 14.2354\n",
      "Evaluation Loss: 10.4691\n",
      "Evaluation Loss: 13.5595\n",
      "Evaluation Loss: 13.5639\n",
      "Epoch 82, Step 100, Loss: 1703877.02375\n",
      "Evaluation Loss: 16.1026\n",
      "Evaluation Loss: 17.1425\n",
      "Evaluation Loss: 12.6417\n",
      "Evaluation Loss: 14.8904\n",
      "Evaluation Loss: 16.5288\n",
      "Epoch 82, Step 200, Loss: 1703935.2575\n",
      "Evaluation Loss: 15.5662\n",
      "Evaluation Loss: 14.1823\n",
      "Evaluation Loss: 10.9366\n",
      "Task 1 Epoch 82 completed. Average Loss: 1703948.8895\n",
      "Evaluation Loss: 17.5044\n",
      "Evaluation Loss: 14.5073\n",
      "Evaluation Loss: 16.0234\n",
      "Evaluation Loss: 14.7987\n",
      "Epoch 83, Step 100, Loss: 1703875.08125\n",
      "Evaluation Loss: 13.4257\n",
      "Evaluation Loss: 13.3851\n",
      "Evaluation Loss: 14.0060\n",
      "Evaluation Loss: 11.7248\n",
      "Evaluation Loss: 15.3338\n",
      "Epoch 83, Step 200, Loss: 1703983.869375\n",
      "Evaluation Loss: 10.1471\n",
      "Evaluation Loss: 14.1271\n",
      "Evaluation Loss: 14.0970\n",
      "Task 1 Epoch 83 completed. Average Loss: 1703888.3505\n",
      "Evaluation Loss: 14.2356\n",
      "Evaluation Loss: 11.6995\n",
      "Evaluation Loss: 14.9601\n",
      "Evaluation Loss: 14.3906\n",
      "Epoch 84, Step 100, Loss: 1703676.4125\n",
      "Evaluation Loss: 14.7095\n",
      "Evaluation Loss: 11.0174\n",
      "Evaluation Loss: 12.1883\n",
      "Evaluation Loss: 10.8729\n",
      "Evaluation Loss: 17.2735\n",
      "Epoch 84, Step 200, Loss: 1703801.378125\n",
      "Evaluation Loss: 10.6405\n",
      "Evaluation Loss: 15.5882\n",
      "Evaluation Loss: 16.6576\n",
      "Task 1 Epoch 84 completed. Average Loss: 1703790.1985\n",
      "Evaluation Loss: 15.2705\n",
      "Evaluation Loss: 13.0415\n",
      "Evaluation Loss: 12.3996\n",
      "Evaluation Loss: 13.2409\n",
      "Epoch 85, Step 100, Loss: 1703945.2725\n",
      "Evaluation Loss: 10.5428\n",
      "Evaluation Loss: 12.1692\n",
      "Evaluation Loss: 14.9680\n",
      "Evaluation Loss: 13.5613\n",
      "Evaluation Loss: 10.2671\n",
      "Epoch 85, Step 200, Loss: 1703951.14375\n",
      "Evaluation Loss: 12.3143\n",
      "Evaluation Loss: 11.6071\n",
      "Evaluation Loss: 13.1945\n",
      "Task 1 Epoch 85 completed. Average Loss: 1703966.514\n",
      "Evaluation Loss: 11.4184\n",
      "Evaluation Loss: 14.1538\n",
      "Evaluation Loss: 12.8176\n",
      "Evaluation Loss: 11.8407\n",
      "Epoch 86, Step 100, Loss: 1703993.26875\n",
      "Evaluation Loss: 11.9204\n",
      "Evaluation Loss: 10.1635\n",
      "Evaluation Loss: 15.1444\n",
      "Evaluation Loss: 11.4322\n",
      "Evaluation Loss: 13.5092\n",
      "Epoch 86, Step 200, Loss: 1703789.67125\n",
      "Evaluation Loss: 15.3432\n",
      "Evaluation Loss: 18.5654\n",
      "Evaluation Loss: 18.3163\n",
      "Task 1 Epoch 86 completed. Average Loss: 1703853.1595\n",
      "Evaluation Loss: 9.6104\n",
      "Evaluation Loss: 10.7722\n",
      "Evaluation Loss: 13.5734\n",
      "Evaluation Loss: 15.7659\n",
      "Epoch 87, Step 100, Loss: 1703886.07375\n",
      "Evaluation Loss: 15.3018\n",
      "Evaluation Loss: 13.9572\n",
      "Evaluation Loss: 14.6866\n",
      "Evaluation Loss: 14.2645\n",
      "Evaluation Loss: 13.2625\n",
      "Epoch 87, Step 200, Loss: 1703871.591875\n",
      "Evaluation Loss: 15.3862\n",
      "Evaluation Loss: 10.9312\n",
      "Evaluation Loss: 16.2522\n",
      "Task 1 Epoch 87 completed. Average Loss: 1703884.0105\n",
      "Evaluation Loss: 16.3954\n",
      "Evaluation Loss: 12.8108\n",
      "Evaluation Loss: 14.4551\n",
      "Evaluation Loss: 13.8393\n",
      "Epoch 88, Step 100, Loss: 1703603.8125\n",
      "Evaluation Loss: 17.9542\n",
      "Evaluation Loss: 12.7085\n",
      "Evaluation Loss: 12.7133\n",
      "Evaluation Loss: 16.6170\n",
      "Evaluation Loss: 14.4906\n",
      "Epoch 88, Step 200, Loss: 1703680.325625\n",
      "Evaluation Loss: 16.8094\n",
      "Evaluation Loss: 14.8206\n",
      "Evaluation Loss: 16.2739\n",
      "Task 1 Epoch 88 completed. Average Loss: 1703713.6845\n",
      "Evaluation Loss: 12.5688\n",
      "Evaluation Loss: 12.2801\n",
      "Evaluation Loss: 13.5678\n",
      "Evaluation Loss: 11.5576\n",
      "Epoch 89, Step 100, Loss: 1703663.5975\n",
      "Evaluation Loss: 15.9335\n",
      "Evaluation Loss: 10.4099\n",
      "Evaluation Loss: 13.2336\n",
      "Evaluation Loss: 16.2041\n",
      "Evaluation Loss: 11.3541\n",
      "Epoch 89, Step 200, Loss: 1703848.65\n",
      "Evaluation Loss: 15.1437\n",
      "Evaluation Loss: 16.2717\n",
      "Evaluation Loss: 12.0639\n",
      "Task 1 Epoch 89 completed. Average Loss: 1703892.5165\n",
      "Evaluation Loss: 18.8483\n",
      "Evaluation Loss: 12.3195\n",
      "Evaluation Loss: 15.2768\n",
      "Evaluation Loss: 10.7790\n",
      "Epoch 90, Step 100, Loss: 1703958.80375\n",
      "Evaluation Loss: 12.9700\n",
      "Evaluation Loss: 11.3906\n",
      "Evaluation Loss: 17.1243\n",
      "Evaluation Loss: 14.8616\n",
      "Evaluation Loss: 14.4350\n",
      "Epoch 90, Step 200, Loss: 1703813.211875\n",
      "Evaluation Loss: 15.9481\n",
      "Evaluation Loss: 14.2154\n",
      "Evaluation Loss: 14.5113\n",
      "Task 1 Epoch 90 completed. Average Loss: 1703854.9525\n",
      "Evaluation Loss: 17.2313\n",
      "Evaluation Loss: 9.7977\n",
      "Evaluation Loss: 10.6116\n",
      "Evaluation Loss: 14.2796\n",
      "Epoch 91, Step 100, Loss: 1704079.05375\n",
      "Evaluation Loss: 15.5478\n",
      "Evaluation Loss: 12.6846\n",
      "Evaluation Loss: 10.7782\n",
      "Evaluation Loss: 12.6416\n",
      "Evaluation Loss: 16.4258\n",
      "Epoch 91, Step 200, Loss: 1704107.089375\n",
      "Evaluation Loss: 5.8119\n",
      "Evaluation Loss: 14.7595\n",
      "Evaluation Loss: 14.1885\n",
      "Task 1 Epoch 91 completed. Average Loss: 1704144.357\n",
      "Evaluation Loss: 14.2027\n",
      "Evaluation Loss: 13.6808\n",
      "Evaluation Loss: 13.2405\n",
      "Evaluation Loss: 16.2455\n",
      "Epoch 92, Step 100, Loss: 1703923.73625\n",
      "Evaluation Loss: 14.7008\n",
      "Evaluation Loss: 14.4773\n",
      "Evaluation Loss: 11.3148\n",
      "Evaluation Loss: 15.0132\n",
      "Evaluation Loss: 13.2507\n",
      "Epoch 92, Step 200, Loss: 1703909.98875\n",
      "Evaluation Loss: 17.9006\n",
      "Evaluation Loss: 12.6599\n",
      "Evaluation Loss: 13.2879\n",
      "Task 1 Epoch 92 completed. Average Loss: 1703947.354\n",
      "Evaluation Loss: 13.4364\n",
      "Evaluation Loss: 15.1631\n",
      "Evaluation Loss: 15.5040\n",
      "Evaluation Loss: 14.8978\n",
      "Epoch 93, Step 100, Loss: 1703846.025\n",
      "Evaluation Loss: 13.7295\n",
      "Evaluation Loss: 15.7410\n",
      "Evaluation Loss: 14.0520\n",
      "Evaluation Loss: 11.3972\n",
      "Evaluation Loss: 13.9902\n",
      "Epoch 93, Step 200, Loss: 1703960.4425\n",
      "Evaluation Loss: 14.7420\n",
      "Evaluation Loss: 15.1686\n",
      "Evaluation Loss: 11.9380\n",
      "Task 1 Epoch 93 completed. Average Loss: 1703965.6545\n",
      "Evaluation Loss: 15.8461\n",
      "Evaluation Loss: 15.5078\n",
      "Evaluation Loss: 12.2589\n",
      "Evaluation Loss: 15.7200\n",
      "Epoch 94, Step 100, Loss: 1703890.32875\n",
      "Evaluation Loss: 11.7646\n",
      "Evaluation Loss: 12.9941\n",
      "Evaluation Loss: 14.2101\n",
      "Evaluation Loss: 12.7863\n",
      "Evaluation Loss: 11.5849\n",
      "Epoch 94, Step 200, Loss: 1703869.77875\n",
      "Evaluation Loss: 19.2159\n",
      "Evaluation Loss: 13.1455\n",
      "Evaluation Loss: 15.9735\n",
      "Task 1 Epoch 94 completed. Average Loss: 1703838.4665\n",
      "Evaluation Loss: 13.3763\n",
      "Evaluation Loss: 16.1793\n",
      "Evaluation Loss: 10.1935\n",
      "Evaluation Loss: 13.7769\n",
      "Epoch 95, Step 100, Loss: 1703819.5775\n",
      "Evaluation Loss: 15.1098\n",
      "Evaluation Loss: 13.6228\n",
      "Evaluation Loss: 11.7464\n",
      "Evaluation Loss: 17.1467\n",
      "Evaluation Loss: 18.6725\n",
      "Epoch 95, Step 200, Loss: 1703962.731875\n",
      "Evaluation Loss: 12.6494\n",
      "Evaluation Loss: 13.1903\n",
      "Evaluation Loss: 16.7007\n",
      "Task 1 Epoch 95 completed. Average Loss: 1703971.482\n",
      "Evaluation Loss: 15.9736\n",
      "Evaluation Loss: 14.1552\n",
      "Evaluation Loss: 13.6307\n",
      "Evaluation Loss: 13.0982\n",
      "Epoch 96, Step 100, Loss: 1703866.17375\n",
      "Evaluation Loss: 14.1161\n",
      "Evaluation Loss: 17.5282\n",
      "Evaluation Loss: 7.1761\n",
      "Evaluation Loss: 13.1566\n",
      "Evaluation Loss: 15.2464\n",
      "Epoch 96, Step 200, Loss: 1703876.4625\n",
      "Evaluation Loss: 16.4337\n",
      "Evaluation Loss: 13.0542\n",
      "Evaluation Loss: 16.2394\n",
      "Task 1 Epoch 96 completed. Average Loss: 1703831.297\n",
      "Evaluation Loss: 11.7787\n",
      "Evaluation Loss: 13.4157\n",
      "Evaluation Loss: 13.3548\n",
      "Evaluation Loss: 14.0029\n",
      "Epoch 97, Step 100, Loss: 1703959.38125\n",
      "Evaluation Loss: 17.3237\n",
      "Evaluation Loss: 15.6660\n",
      "Evaluation Loss: 12.7982\n",
      "Evaluation Loss: 17.6700\n",
      "Evaluation Loss: 14.8584\n",
      "Epoch 97, Step 200, Loss: 1704022.5\n",
      "Evaluation Loss: 16.7852\n",
      "Evaluation Loss: 13.6630\n",
      "Evaluation Loss: 14.9252\n",
      "Task 1 Epoch 97 completed. Average Loss: 1704004.346\n",
      "Evaluation Loss: 15.1400\n",
      "Evaluation Loss: 13.6061\n",
      "Evaluation Loss: 11.6007\n",
      "Evaluation Loss: 16.1388\n",
      "Epoch 98, Step 100, Loss: 1703720.735\n",
      "Evaluation Loss: 12.9973\n",
      "Evaluation Loss: 11.9242\n",
      "Evaluation Loss: 17.5248\n",
      "Evaluation Loss: 16.6755\n",
      "Evaluation Loss: 14.4049\n",
      "Epoch 98, Step 200, Loss: 1703865.16875\n",
      "Evaluation Loss: 13.8557\n",
      "Evaluation Loss: 12.5970\n",
      "Evaluation Loss: 14.3631\n",
      "Task 1 Epoch 98 completed. Average Loss: 1703926.4155\n",
      "Evaluation Loss: 12.3046\n",
      "Evaluation Loss: 14.6692\n",
      "Evaluation Loss: 9.3875\n",
      "Evaluation Loss: 13.2499\n",
      "Epoch 99, Step 100, Loss: 1703950.33125\n",
      "Evaluation Loss: 15.3551\n",
      "Evaluation Loss: 13.0703\n",
      "Evaluation Loss: 9.2805\n",
      "Evaluation Loss: 13.7682\n",
      "Evaluation Loss: 10.1523\n",
      "Epoch 99, Step 200, Loss: 1703913.354375\n",
      "Evaluation Loss: 13.5581\n",
      "Evaluation Loss: 14.8226\n",
      "Evaluation Loss: 15.9500\n",
      "Task 1 Epoch 99 completed. Average Loss: 1703907.015\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model=run_lora_evcl_1(\n",
    "        num_epochs=100,\n",
    "        base_model_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "        batch_size=2,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=100,\n",
    "        eval_steps=20,\n",
    "        save_steps=500,\n",
    "        output_dir=\"finetuned-weights-LoRA-EVCL\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64aee245-7e13-4698-8b75-7d72f09421c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e135318333c486dae882dc79bcf57a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\",\n",
    "            device_map=\"auto\",\n",
    "            offload_folder='/home/pranav24/cs-546-project/llama_offload_evcl',\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "lora_model_path = \"/home/pranav24/cs-546-project/finetuned-weights-LoRA-EVCL\"\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602a70a2-af6d-4078-98bf-473da225ec18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f3a69c3-3a02-4521-a882-a55b1fcca627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/1\n",
      "Processing batch 1\n",
      "Completed batch 1\n",
      "Processing batch 2\n",
      "Completed batch 2\n",
      "Processing batch 3\n",
      "Completed batch 3\n",
      "Processing batch 4\n",
      "Completed batch 4\n",
      "Processing batch 5\n",
      "Completed batch 5\n",
      "Processing batch 6\n",
      "Completed batch 6\n",
      "Processing batch 7\n",
      "Completed batch 7\n",
      "Processing batch 8\n",
      "Completed batch 8\n",
      "Processing batch 9\n",
      "Completed batch 9\n",
      "Processing batch 10\n",
      "Completed batch 10\n",
      "Processing batch 11\n",
      "Completed batch 11\n",
      "Processing batch 12\n",
      "Completed batch 12\n",
      "Processing batch 13\n",
      "Completed batch 13\n",
      "Processing batch 14\n",
      "Completed batch 14\n",
      "Processing batch 15\n",
      "Completed batch 15\n",
      "Processing batch 16\n",
      "Completed batch 16\n",
      "Processing batch 17\n",
      "Completed batch 17\n",
      "Processing batch 18\n",
      "Completed batch 18\n",
      "Processing batch 19\n",
      "Completed batch 19\n",
      "Processing batch 20\n",
      "Completed batch 20\n",
      "Processing batch 21\n",
      "Completed batch 21\n",
      "Processing batch 22\n",
      "Completed batch 22\n",
      "Processing batch 23\n",
      "Completed batch 23\n",
      "Processing batch 24\n",
      "Completed batch 24\n",
      "Processing batch 25\n",
      "Completed batch 25\n",
      "Processing batch 26\n",
      "Completed batch 26\n",
      "Processing batch 27\n",
      "Completed batch 27\n",
      "Processing batch 28\n",
      "Completed batch 28\n",
      "Processing batch 29\n",
      "Completed batch 29\n",
      "Processing batch 30\n",
      "Completed batch 30\n",
      "Processing batch 31\n",
      "Completed batch 31\n",
      "Processing batch 32\n",
      "Completed batch 32\n",
      "Processing batch 33\n",
      "Completed batch 33\n",
      "Processing batch 34\n",
      "Completed batch 34\n",
      "Processing batch 35\n",
      "Completed batch 35\n",
      "Processing batch 36\n",
      "Completed batch 36\n",
      "Processing batch 37\n",
      "Completed batch 37\n",
      "Processing batch 38\n",
      "Completed batch 38\n",
      "Processing batch 39\n",
      "Completed batch 39\n",
      "Processing batch 40\n",
      "Completed batch 40\n",
      "Processing batch 41\n",
      "Completed batch 41\n",
      "Processing batch 42\n",
      "Completed batch 42\n",
      "Processing batch 43\n",
      "Completed batch 43\n",
      "Processing batch 44\n",
      "Completed batch 44\n",
      "Processing batch 45\n",
      "Completed batch 45\n",
      "Processing batch 46\n",
      "Completed batch 46\n",
      "Processing batch 47\n",
      "Completed batch 47\n",
      "Processing batch 48\n",
      "Completed batch 48\n",
      "Processing batch 49\n",
      "Completed batch 49\n",
      "Processing batch 50\n",
      "Completed batch 50\n",
      "Processing batch 51\n",
      "Completed batch 51\n",
      "Processing batch 52\n",
      "Completed batch 52\n",
      "Processing batch 53\n",
      "Completed batch 53\n",
      "Processing batch 54\n",
      "Completed batch 54\n",
      "Processing batch 55\n",
      "Completed batch 55\n",
      "Processing batch 56\n",
      "Completed batch 56\n",
      "Processing batch 57\n",
      "Completed batch 57\n",
      "Processing batch 58\n",
      "Completed batch 58\n",
      "Processing batch 59\n",
      "Completed batch 59\n",
      "Processing batch 60\n",
      "Completed batch 60\n",
      "Processing batch 61\n",
      "Completed batch 61\n",
      "Processing batch 62\n",
      "Completed batch 62\n",
      "Processing batch 63\n",
      "Completed batch 63\n",
      "Processing batch 64\n",
      "Completed batch 64\n",
      "Processing batch 65\n",
      "Completed batch 65\n",
      "Processing batch 66\n",
      "Completed batch 66\n",
      "Processing batch 67\n",
      "Completed batch 67\n",
      "Processing batch 68\n",
      "Completed batch 68\n",
      "Processing batch 69\n",
      "Completed batch 69\n",
      "Processing batch 70\n",
      "Completed batch 70\n",
      "Processing batch 71\n",
      "Completed batch 71\n",
      "Processing batch 72\n",
      "Completed batch 72\n",
      "Processing batch 73\n",
      "Completed batch 73\n",
      "Processing batch 74\n",
      "Completed batch 74\n",
      "Processing batch 75\n",
      "Completed batch 75\n",
      "Processing batch 76\n",
      "Completed batch 76\n",
      "Processing batch 77\n",
      "Completed batch 77\n",
      "Processing batch 78\n",
      "Completed batch 78\n",
      "Processing batch 79\n",
      "Completed batch 79\n",
      "Processing batch 80\n",
      "Completed batch 80\n",
      "Processing batch 81\n",
      "Completed batch 81\n",
      "Processing batch 82\n",
      "Completed batch 82\n",
      "Processing batch 83\n",
      "Completed batch 83\n",
      "Processing batch 84\n",
      "Completed batch 84\n",
      "Processing batch 85\n",
      "Completed batch 85\n",
      "Processing batch 86\n",
      "Completed batch 86\n",
      "Processing batch 87\n",
      "Completed batch 87\n",
      "Processing batch 88\n",
      "Completed batch 88\n",
      "Processing batch 89\n",
      "Completed batch 89\n",
      "Processing batch 90\n",
      "Completed batch 90\n",
      "Processing batch 91\n",
      "Completed batch 91\n",
      "Processing batch 92\n",
      "Completed batch 92\n",
      "Processing batch 93\n",
      "Completed batch 93\n",
      "Processing batch 94\n",
      "Completed batch 94\n",
      "Processing batch 95\n",
      "Completed batch 95\n",
      "Processing batch 96\n",
      "Completed batch 96\n",
      "Processing batch 97\n",
      "Completed batch 97\n",
      "Processing batch 98\n",
      "Completed batch 98\n",
      "Processing batch 99\n",
      "Completed batch 99\n",
      "Processing batch 100\n",
      "Completed batch 100\n",
      "Processing batch 101\n",
      "Completed batch 101\n",
      "Processing batch 102\n",
      "Completed batch 102\n",
      "Processing batch 103\n",
      "Completed batch 103\n",
      "Processing batch 104\n",
      "Completed batch 104\n",
      "Processing batch 105\n",
      "Completed batch 105\n",
      "Processing batch 106\n",
      "Completed batch 106\n",
      "Processing batch 107\n",
      "Completed batch 107\n",
      "Processing batch 108\n",
      "Completed batch 108\n",
      "Processing batch 109\n",
      "Completed batch 109\n",
      "Processing batch 110\n",
      "Completed batch 110\n",
      "Processing batch 111\n",
      "Completed batch 111\n",
      "Processing batch 112\n",
      "Completed batch 112\n",
      "Processing batch 113\n",
      "Completed batch 113\n",
      "Processing batch 114\n",
      "Completed batch 114\n",
      "Processing batch 115\n",
      "Completed batch 115\n",
      "Processing batch 116\n",
      "Completed batch 116\n",
      "Processing batch 117\n",
      "Completed batch 117\n",
      "Processing batch 118\n",
      "Completed batch 118\n",
      "Processing batch 119\n",
      "Completed batch 119\n",
      "Processing batch 120\n",
      "Completed batch 120\n",
      "Processing batch 121\n",
      "Completed batch 121\n",
      "Processing batch 122\n",
      "Completed batch 122\n",
      "Processing batch 123\n",
      "Completed batch 123\n",
      "Processing batch 124\n",
      "Completed batch 124\n",
      "Processing batch 125\n",
      "Completed batch 125\n",
      "Processing batch 126\n",
      "Completed batch 126\n",
      "Processing batch 127\n",
      "Completed batch 127\n",
      "Processing batch 128\n",
      "Completed batch 128\n",
      "Processing batch 129\n",
      "Completed batch 129\n",
      "Processing batch 130\n",
      "Completed batch 130\n",
      "Processing batch 131\n",
      "Completed batch 131\n",
      "Processing batch 132\n",
      "Completed batch 132\n",
      "Processing batch 133\n",
      "Completed batch 133\n",
      "Processing batch 134\n",
      "Completed batch 134\n",
      "Processing batch 135\n",
      "Completed batch 135\n",
      "Processing batch 136\n",
      "Completed batch 136\n",
      "Processing batch 137\n",
      "Completed batch 137\n",
      "Processing batch 138\n",
      "Completed batch 138\n",
      "Processing batch 139\n",
      "Completed batch 139\n",
      "Processing batch 140\n",
      "Completed batch 140\n",
      "Processing batch 141\n",
      "Completed batch 141\n",
      "Processing batch 142\n",
      "Completed batch 142\n",
      "Processing batch 143\n",
      "Completed batch 143\n",
      "Processing batch 144\n",
      "Completed batch 144\n",
      "Processing batch 145\n",
      "Completed batch 145\n",
      "Processing batch 146\n",
      "Completed batch 146\n",
      "Processing batch 147\n",
      "Completed batch 147\n",
      "Processing batch 148\n",
      "Completed batch 148\n",
      "Processing batch 149\n",
      "Completed batch 149\n",
      "Processing batch 150\n",
      "Completed batch 150\n",
      "Processing batch 151\n",
      "Completed batch 151\n",
      "Processing batch 152\n",
      "Completed batch 152\n",
      "Processing batch 153\n",
      "Completed batch 153\n",
      "Processing batch 154\n",
      "Completed batch 154\n",
      "Processing batch 155\n",
      "Completed batch 155\n",
      "Processing batch 156\n",
      "Completed batch 156\n",
      "Processing batch 157\n",
      "Completed batch 157\n",
      "Processing batch 158\n",
      "Completed batch 158\n",
      "Processing batch 159\n",
      "Completed batch 159\n",
      "Processing batch 160\n",
      "Completed batch 160\n",
      "Processing batch 161\n",
      "Completed batch 161\n",
      "Processing batch 162\n",
      "Completed batch 162\n",
      "Processing batch 163\n",
      "Completed batch 163\n",
      "Processing batch 164\n",
      "Completed batch 164\n",
      "Processing batch 165\n",
      "Completed batch 165\n",
      "Processing batch 166\n",
      "Completed batch 166\n",
      "Processing batch 167\n",
      "Completed batch 167\n",
      "Processing batch 168\n",
      "Completed batch 168\n",
      "Processing batch 169\n",
      "Completed batch 169\n",
      "Processing batch 170\n",
      "Completed batch 170\n",
      "Processing batch 171\n",
      "Completed batch 171\n",
      "Processing batch 172\n",
      "Completed batch 172\n",
      "Processing batch 173\n",
      "Completed batch 173\n",
      "Processing batch 174\n",
      "Completed batch 174\n",
      "Processing batch 175\n",
      "Completed batch 175\n",
      "Processing batch 176\n",
      "Completed batch 176\n",
      "Processing batch 177\n",
      "Completed batch 177\n",
      "Processing batch 178\n",
      "Completed batch 178\n",
      "Processing batch 179\n",
      "Completed batch 179\n",
      "Processing batch 180\n",
      "Completed batch 180\n",
      "Processing batch 181\n",
      "Completed batch 181\n",
      "Processing batch 182\n",
      "Completed batch 182\n",
      "Processing batch 183\n",
      "Completed batch 183\n",
      "Processing batch 184\n",
      "Completed batch 184\n",
      "Processing batch 185\n",
      "Completed batch 185\n",
      "Processing batch 186\n",
      "Completed batch 186\n",
      "Processing batch 187\n",
      "Completed batch 187\n",
      "Processing batch 188\n",
      "Completed batch 188\n",
      "Processing batch 189\n",
      "Completed batch 189\n",
      "Processing batch 190\n",
      "Completed batch 190\n",
      "Processing batch 191\n",
      "Completed batch 191\n",
      "Processing batch 192\n",
      "Completed batch 192\n",
      "Processing batch 193\n",
      "Completed batch 193\n",
      "Processing batch 194\n",
      "Completed batch 194\n",
      "Processing batch 195\n",
      "Completed batch 195\n",
      "Processing batch 196\n",
      "Completed batch 196\n",
      "Processing batch 197\n",
      "Completed batch 197\n",
      "Processing batch 198\n",
      "Completed batch 198\n",
      "Processing batch 199\n",
      "Completed batch 199\n",
      "Processing batch 200\n",
      "Completed batch 200\n",
      "Processing batch 201\n",
      "Completed batch 201\n",
      "Processing batch 202\n",
      "Completed batch 202\n",
      "Processing batch 203\n",
      "Completed batch 203\n",
      "Processing batch 204\n",
      "Completed batch 204\n",
      "Processing batch 205\n",
      "Completed batch 205\n",
      "Processing batch 206\n",
      "Completed batch 206\n",
      "Processing batch 207\n",
      "Completed batch 207\n",
      "Processing batch 208\n",
      "Completed batch 208\n",
      "Processing batch 209\n",
      "Completed batch 209\n",
      "Processing batch 210\n",
      "Completed batch 210\n",
      "Processing batch 211\n",
      "Completed batch 211\n",
      "Processing batch 212\n",
      "Completed batch 212\n",
      "Processing batch 213\n",
      "Completed batch 213\n",
      "Processing batch 214\n",
      "Completed batch 214\n",
      "Processing batch 215\n",
      "Completed batch 215\n",
      "Processing batch 216\n",
      "Completed batch 216\n",
      "Processing batch 217\n",
      "Completed batch 217\n",
      "Processing batch 218\n",
      "Completed batch 218\n",
      "Processing batch 219\n",
      "Completed batch 219\n",
      "Processing batch 220\n",
      "Completed batch 220\n",
      "Processing batch 221\n",
      "Completed batch 221\n",
      "Processing batch 222\n",
      "Completed batch 222\n",
      "Processing batch 223\n",
      "Completed batch 223\n",
      "Processing batch 224\n",
      "Completed batch 224\n",
      "Processing batch 225\n",
      "Completed batch 225\n",
      "Processing batch 226\n",
      "Completed batch 226\n",
      "Processing batch 227\n",
      "Completed batch 227\n",
      "Processing batch 228\n",
      "Completed batch 228\n",
      "Processing batch 229\n",
      "Completed batch 229\n",
      "Processing batch 230\n",
      "Completed batch 230\n",
      "Processing batch 231\n",
      "Completed batch 231\n",
      "Processing batch 232\n",
      "Completed batch 232\n",
      "Processing batch 233\n",
      "Completed batch 233\n",
      "Processing batch 234\n",
      "Completed batch 234\n",
      "Processing batch 235\n",
      "Completed batch 235\n",
      "Processing batch 236\n",
      "Completed batch 236\n",
      "Processing batch 237\n",
      "Completed batch 237\n",
      "Processing batch 238\n",
      "Completed batch 238\n",
      "Processing batch 239\n",
      "Completed batch 239\n",
      "Processing batch 240\n",
      "Completed batch 240\n",
      "Processing batch 241\n",
      "Completed batch 241\n",
      "Processing batch 242\n",
      "Completed batch 242\n",
      "Processing batch 243\n",
      "Completed batch 243\n",
      "Processing batch 244\n",
      "Completed batch 244\n",
      "Processing batch 245\n",
      "Completed batch 245\n",
      "Processing batch 246\n",
      "Completed batch 246\n",
      "Processing batch 247\n",
      "Completed batch 247\n",
      "Processing batch 248\n",
      "Completed batch 248\n",
      "Processing batch 249\n",
      "Completed batch 249\n",
      "Processing batch 250\n",
      "Completed batch 250\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "prev_fisher_info = None\n",
    "prev_params = None\n",
    "ewc_gamma = 1.0  \n",
    "\n",
    "fisher_info = compute_fisher_info(\n",
    "    model=model,\n",
    "    data_loader=train_loader,\n",
    "    prev_fisher_info=prev_fisher_info,\n",
    "    ewc_gamma=ewc_gamma,\n",
    "    num_epochs=1,  \n",
    "    head_modules=None,  \n",
    "    n_samples=None  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7e157fa-3d0a-4f37-a264-8c6ada0f30aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.0005135599058121443\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 0.0010042006615549326\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.029310958459973335\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.07284019887447357\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.00012028848868794739\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.44399270741269e-05\n",
      "Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0160883367061615\n",
      "Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.07432784140110016\n",
      "Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.00014660028682556003\n",
      "Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.724917976593133e-05\n",
      "Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.01853732392191887\n",
      "Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.027849949896335602\n",
      "Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 6.263380055315793e-05\n",
      "Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 6.651174771832302e-05\n",
      "Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0032166452147066593\n",
      "Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.02938860096037388\n",
      "Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 6.102528004703345e-06\n",
      "Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 4.9254340410698205e-06\n",
      "Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.007425458170473576\n",
      "Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.007124226540327072\n",
      "Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.1080824151576962e-05\n",
      "Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 9.383667929796502e-06\n",
      "Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00208555837161839\n",
      "Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.018010050058364868\n",
      "Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.381498117960291e-06\n",
      "Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.8912448871997185e-06\n",
      "Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0034343204461038113\n",
      "Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.007514249533414841\n",
      "Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.3519834940088913e-05\n",
      "Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 9.293102266383357e-06\n",
      "Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0011356242466717958\n",
      "Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.009284822270274162\n",
      "Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.762624485010747e-06\n",
      "Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 8.811045518086758e-06\n",
      "Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0010721504222601652\n",
      "Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.005346088670194149\n",
      "Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.1054527223896002e-06\n",
      "Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 7.679725968046114e-06\n",
      "Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.002220537979155779\n",
      "Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.005603664554655552\n",
      "Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.111691850994248e-06\n",
      "Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.4383977941179182e-05\n",
      "Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0019892742857337\n",
      "Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.004897716920822859\n",
      "Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.2640726936297142e-06\n",
      "Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 5.0797430048987735e-06\n",
      "Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.001047244993969798\n",
      "Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.004034414421766996\n",
      "Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.3561094621982193e-06\n",
      "Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.514296056688181e-06\n",
      "Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00047469406854361296\n",
      "Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.004460825119167566\n",
      "Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 9.425190000911243e-06\n",
      "Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.5808325264952146e-06\n",
      "Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0010196338407695293\n",
      "Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.005350570194423199\n",
      "Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.789291895373026e-06\n",
      "Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 8.83506629634212e-07\n",
      "Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0011822390370070934\n",
      "Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.005570509470999241\n",
      "Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.169946810681722e-06\n",
      "Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 7.385384606095613e-07\n",
      "Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0010968780843541026\n",
      "Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0025079380720853806\n",
      "Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.907239599560853e-06\n",
      "Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.715106575124082e-06\n",
      "Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0007925725658424199\n",
      "Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.007638889830559492\n",
      "Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.9054822359976242e-06\n",
      "Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.621270484974957e-06\n",
      "Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0005891437176615\n",
      "Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0031002433970570564\n",
      "Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.618912671754515e-07\n",
      "Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.7630733282203437e-06\n",
      "Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.002080557169392705\n",
      "Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0024559572339057922\n",
      "Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.3366947086979053e-06\n",
      "Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.705000497371657e-06\n",
      "Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0013511718716472387\n",
      "Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0040166983380913734\n",
      "Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 9.67640971794026e-07\n",
      "Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.3631024557980709e-06\n",
      "Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0007465659291483462\n",
      "Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.006023302674293518\n",
      "Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.729576126763277e-07\n",
      "Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.705068311570358e-07\n",
      "Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0012518700677901506\n",
      "Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0031773275695741177\n",
      "Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.9731429524181294e-07\n",
      "Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.0596622814773582e-06\n",
      "Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0013591318856924772\n",
      "Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.002549517899751663\n",
      "Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 7.208799956970324e-07\n",
      "Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 5.329301302481326e-07\n",
      "Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0006280883681029081\n",
      "Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.002614453434944153\n",
      "Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 6.808315902162576e-07\n",
      "Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.498294406905188e-07\n",
      "Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0004691544163506478\n",
      "Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0018144214991480112\n",
      "Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.131150384026114e-06\n",
      "Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 9.289548756896693e-07\n",
      "Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.000891244737431407\n",
      "Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0050424253568053246\n",
      "Layer: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.1073492462164722e-06\n",
      "Layer: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.163129011023557e-06\n",
      "Layer: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00037781090941280127\n",
      "Layer: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.002978071803227067\n",
      "Layer: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.840800218313234e-06\n",
      "Layer: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.5072357604803983e-06\n",
      "Layer: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0007891942514106631\n",
      "Layer: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.002095266943797469\n",
      "Layer: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 5.903692681386019e-07\n",
      "Layer: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.1036710247935844e-06\n",
      "Layer: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0005908538587391376\n",
      "Layer: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0025230818428099155\n",
      "Layer: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.476827605983999e-06\n",
      "Layer: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.676709755178308e-06\n",
      "Layer: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.000971137429587543\n",
      "Layer: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0068094367161393166\n",
      "Layer: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 6.30104750598548e-07\n",
      "Layer: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 4.097591954632662e-07\n",
      "Layer: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0018235808238387108\n",
      "Layer: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.004527417477220297\n",
      "Layer: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.529903611048212e-07\n",
      "Layer: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.493404108212417e-07\n",
      "Layer: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00044795440044254065\n",
      "Layer: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0023827587720006704\n"
     ]
    }
   ],
   "source": [
    "for name, fisher_matrix in fisher_info.items():\n",
    "    print(f\"Layer: {name}, Fisher Info Mean: {fisher_matrix.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d1b4cfa-ef40-47e7-9f88-1940c51e0c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "prev_posterior_means = get_variational_posterior_means(model)\n",
    "torch.save(prev_posterior_means, f'posterior_means_task_{1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf1d6a1f-9a1c-4f64-923b-3a02a008e407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_posterior_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234530c-a2c0-4698-b7f2-8a9e390585de",
   "metadata": {},
   "source": [
    "### Task 2: QA+QG EVCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e0bd22e-e909-48d8-ac98-23e524827601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "import torch\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO\n",
    "\n",
    "def run_lora_evcl_2(\n",
    "    num_epochs: int = 3,\n",
    "    batch_size: int = 2,\n",
    "    learning_rate: float = 1e-5,\n",
    "    logging_steps: int = 100,\n",
    "    eval_steps: int = 200,\n",
    "    save_steps: int = 500,\n",
    "    output_dir: str = \"finetuned-weights-LoRA-EVCL-2\",\n",
    "    prev_fisher_info: dict = None,            \n",
    "    prev_posterior_means: dict = None,        \n",
    "    ewc_lambda: float = 0.0,                  \n",
    "    synthetic_data_loader=None,               # Synthetic data from Task 1\n",
    "    combined_loader=None,                     # Data loader for Task 2\n",
    "    eval_loader=None,                         # Evaluation data loader\n",
    "    tokenizer=None,\n",
    "    model=None\n",
    "):\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Ensure all parameters require gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False  # Freeze non-LoRA parameters\n",
    "\n",
    "    def bayesian_guide(input_ids, attention_mask, labels):\n",
    "        # Define variational distributions over the LoRA parameters\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, 'lora_A'):\n",
    "                for key in module.lora_A:\n",
    "                    param_name = f\"{name}.lora_A.{key}\"\n",
    "                    lora_A_param = module.lora_A[key].weight\n",
    "                    device = lora_A_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_A_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_A_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_A_param.dim())\n",
    "                    )\n",
    "            if hasattr(module, 'lora_B'):\n",
    "                for key in module.lora_B:\n",
    "                    param_name = f\"{name}.lora_B.{key}\"\n",
    "                    lora_B_param = module.lora_B[key].weight\n",
    "                    device = lora_B_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_B_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_B_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_B_param.dim())\n",
    "                    )\n",
    "                        \n",
    "    def bayesian_model(input_ids, attention_mask, labels):\n",
    "        # pyro.module(\"model\", model)  # Removed\n",
    "\n",
    "        # Define a function to sample and substitute LoRA parameters\n",
    "        def model_with_sampled_lora():\n",
    "            # Sample LoRA parameters and set them in the model\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, 'lora_A'):\n",
    "                    for key in module.lora_A:\n",
    "                        param_name = f\"{name}.lora_A.{key}\"\n",
    "                        lora_A_module = module.lora_A[key]\n",
    "                        device = lora_A_module.weight.device\n",
    "\n",
    "                        # Use posterior mean from Task 1 as prior mean\n",
    "                        prior_mean = prev_posterior_means.get(param_name, lora_A_module.weight.detach().clone()).to(device)\n",
    "                        prior_std = (0.1 * torch.ones_like(lora_A_module.weight)).to(device)\n",
    "\n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                prior_mean,\n",
    "                                prior_std\n",
    "                            ).to_event(lora_A_module.weight.dim())\n",
    "                        )\n",
    "\n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_A_module.weight.copy_(sampled_weight)\n",
    "\n",
    "                if hasattr(module, 'lora_B'):\n",
    "                    for key in module.lora_B:\n",
    "                        param_name = f\"{name}.lora_B.{key}\"\n",
    "                        lora_B_module = module.lora_B[key]\n",
    "                        device = lora_B_module.weight.device\n",
    "\n",
    "                        # Use posterior mean from Task 1 as prior mean\n",
    "                        prior_mean = prev_posterior_means.get(param_name, lora_B_module.weight.detach().clone()).to(device)\n",
    "                        prior_std = (0.1 * torch.ones_like(lora_B_module.weight)).to(device)\n",
    "\n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                prior_mean,\n",
    "                                prior_std\n",
    "                            ).to_event(lora_B_module.weight.dim())\n",
    "                        )\n",
    "\n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_B_module.weight.copy_(sampled_weight)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Add EWC penalty if previous Fisher info and posterior means are provided\n",
    "            if prev_fisher_info is not None and prev_posterior_means is not None and ewc_lambda > 0.0:\n",
    "                ewc_penalty = 0.0\n",
    "                for name, param in model.named_parameters():\n",
    "                    if 'lora' in name and name in prev_fisher_info:\n",
    "                        fisher = prev_fisher_info[name].to(DEVICE)\n",
    "                        prev_mean = prev_posterior_means[name].to(DEVICE)\n",
    "                        ewc_penalty += (fisher * (param - prev_mean) ** 2).sum()\n",
    "                loss += ewc_lambda * ewc_penalty\n",
    "\n",
    "            return loss\n",
    "\n",
    "        # Use the modified model with sampled LoRA parameters\n",
    "        return model_with_sampled_lora()\n",
    "\n",
    "    # Set up SVI\n",
    "    pyro.clear_param_store()\n",
    "    optim = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "    elbo = TraceMeanField_ELBO()\n",
    "    svi = SVI(bayesian_model, bayesian_guide, optim, loss=elbo)\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training on new task with EWC and synthetic data from previous task...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for num_batches, batch in enumerate(combined_loader, 1):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            loss = svi.step(input_ids, attention_mask, labels)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Logging\n",
    "            if num_batches % logging_steps == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch {epoch + 1}, Step {num_batches}, Loss: {avg_loss}\")\n",
    "\n",
    "            # Evaluation\n",
    "            if num_batches % eval_steps == 0:\n",
    "                evaluate_model(model, eval_loader)\n",
    "\n",
    "            # Save checkpoints\n",
    "            if num_batches % save_steps == 0:\n",
    "                save_trained_model(model, tokenizer, output_dir)\n",
    "\n",
    "        avg_epoch_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    # Save the final trained model after the task\n",
    "    save_trained_model(model, tokenizer, output_dir)\n",
    "    pyro.get_param_store().save('pyro_param_store_task2.pt')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dcdfc7d-7765-4548-aeef-6c7032d5961c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1bc610257e4318b23428dab1820dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Latest_Weights/QA_QG_ Weights')\n",
    "target_file = \"task074_squad1.1_question_generation.json\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "instances = json_data['Instances'][0:2500]\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"output\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"attention_mask\"] = model_inputs.get(\"attention_mask\", None)\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization and set format\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Split dataset into train and eval\n",
    "train_size = int(0.9 * len(tokenized_datasets))\n",
    "train_dataset = tokenized_datasets.select(range(train_size))\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8  \n",
    "train_loader_2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader_2 = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1370b9d7-8f04-4a2c-a48f-f6f0b8500c56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: json_repair in /opt/conda/lib/python3.11/site-packages (0.30.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install json_repair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd52b9-8cc9-4709-aa6d-7342da74ecb5",
   "metadata": {},
   "source": [
    "#### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65bd9af2-9f5c-45b4-b1ba-7e8ea4c2ee50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9f622ae83946cbbb15db925d58838c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json_repair \n",
    "os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Synthethic_Data_Generation')\n",
    "target_file = \"qa.train.final_sampled.jsonl\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json_repair.loads(f.read())\n",
    "\n",
    "instances = json_data\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_size = int(1.0 * len(tokenized_datasets))\n",
    "synthetic_train_dataset = tokenized_datasets.select(range(train_size))\n",
    "batch_size = 8  \n",
    "synthetic_loader_1 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98e0db01-be03-419b-abde-11df8e635b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Synthethic_Data_Generation\n",
      "/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8ad22aa-217f-4d30-9ef2-c882368a26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "# Combine datasets\n",
    "if synthetic_loader_1 is not None:\n",
    "    combined_dataset = ConcatDataset([train_loader_2.dataset, synthetic_loader_1.dataset])\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
    "else:\n",
    "    combined_loader = train_loader_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5211d71-2108-4c2e-b790-758068ced9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on new task with EWC and synthetic data from previous task...\n",
      "Epoch 1, Step 100, Loss: 843486.794375\n",
      "Epoch 1, Step 200, Loss: 847730.5909375\n",
      "Evaluation Loss: 12.8290\n",
      "Epoch 1, Step 300, Loss: 849076.033125\n",
      "Epoch 1, Step 400, Loss: 849807.50875\n",
      "Evaluation Loss: 15.4223\n",
      "Epoch 1, Step 500, Loss: 850279.091625\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Task2\n",
      "Epoch 1 completed. Average Loss: 850449.5449600355\n",
      "Epoch 2, Step 100, Loss: 851994.78625\n",
      "Epoch 2, Step 200, Loss: 852064.1734375\n",
      "Evaluation Loss: 10.8797\n",
      "Epoch 2, Step 300, Loss: 852052.5872916667\n",
      "Epoch 2, Step 400, Loss: 852069.47703125\n",
      "Evaluation Loss: 14.5681\n",
      "Epoch 2, Step 500, Loss: 852065.829125\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Task2\n",
      "Epoch 2 completed. Average Loss: 852064.1171181173\n",
      "Epoch 3, Step 100, Loss: 851905.533125\n",
      "Epoch 3, Step 200, Loss: 851910.8978125\n",
      "Evaluation Loss: 15.9732\n",
      "Epoch 3, Step 300, Loss: 851894.6825\n",
      "Epoch 3, Step 400, Loss: 851918.4740625\n",
      "Evaluation Loss: 15.9056\n",
      "Epoch 3, Step 500, Loss: 851909.012125\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Task2\n",
      "Epoch 3 completed. Average Loss: 851946.198490231\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Task2\n"
     ]
    }
   ],
   "source": [
    "ewc_lambda = 100.0\n",
    "model_task_2=run_lora_evcl_2(\n",
    "    num_epochs=3,\n",
    "    batch_size=2,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=100,\n",
    "    eval_steps=200,\n",
    "    save_steps=500,\n",
    "    output_dir=\"finetuned-weights-LoRA-EVCL-Task2\",\n",
    "    prev_fisher_info=fisher_info,\n",
    "    prev_posterior_means=prev_posterior_means,\n",
    "    ewc_lambda=ewc_lambda,\n",
    "    synthetic_data_loader=synthetic_loader_1,\n",
    "    combined_loader=combined_loader,\n",
    "    eval_loader=eval_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6457d3f-72ee-4eca-9d75-84b53d953adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89919ef9-216e-44d2-a83a-c2458951d58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844525ac-9677-4cc2-846d-bae8ebb87667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_546)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
