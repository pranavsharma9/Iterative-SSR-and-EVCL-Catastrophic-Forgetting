{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3df9f10-1efb-4437-b04e-e6c2020be1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import tyxe\n",
    "\n",
    "import random\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO, Trace_ELBO\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, TensorDataset\n",
    "\n",
    "from datasets import load_dataset  # Added to load SuperNI dataset\n",
    "\n",
    "from typing import Optional, List\n",
    "from model.mle_prior import MLEPrior\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaff032f-ae3f-4be6-a016-d5ee7237f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current Device Index: 0\n",
      "Current Device Name: NVIDIA A100-SXM4-80GB\n",
      "Number of GPUs: 1\n",
      "Device 0: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get the current device index\n",
    "current_device = torch.cuda.current_device()\n",
    "print(\"Current Device Index:\", current_device)\n",
    "\n",
    "# Get the name of the current device\n",
    "device_name = torch.cuda.get_device_name(current_device)\n",
    "print(\"Current Device Name:\", device_name)\n",
    "\n",
    "# Get the number of GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "# List all GPUs\n",
    "for device_id in range(num_gpus):\n",
    "    print(f\"Device {device_id}: {torch.cuda.get_device_name(device_id)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3192f-96f4-44da-9eb4-aea2856aab4f",
   "metadata": {},
   "source": [
    "### Task1 -QA LoRA+EVCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18d43227-5365-4537-a585-1f370b504d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_info(\n",
    "    model, \n",
    "    data_loader, \n",
    "    prev_fisher_info=None, \n",
    "    ewc_gamma=1.0, \n",
    "    num_epochs=1, \n",
    "    head_modules=None, \n",
    "    n_samples=None\n",
    "):\n",
    "\n",
    "    fisher = {}\n",
    "    \n",
    "    # Initialize Fisher matrix for LoRA parameters, excluding head modules if provided\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name and (head_modules is None or not any(name.startswith(head) for head in head_modules)):\n",
    "            fisher[name] = torch.zeros_like(param).to(DEVICE)\n",
    "    \n",
    "    # Save the model's current training state and set to eval\n",
    "    old_training_state = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    batch_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            if n_samples is not None and batch_count >= n_samples:\n",
    "                break\n",
    "\n",
    "            print(f\"Processing batch {batch_count + 1}\")\n",
    "            model.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            try:\n",
    "                # with autocast(device_type='cuda'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "            # scaler.scale(loss).backward()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {batch_count + 1}: {e}\")\n",
    "                break\n",
    "\n",
    "            # Accumulate Fisher information for LoRA parameters\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'lora' in name and param.grad is not None and (head_modules is None or not any(name.startswith(head) for head in head_modules)):\n",
    "                    fisher[name] += param.grad.data ** 2\n",
    "\n",
    "            print(f\"Completed batch {batch_count + 1}\")\n",
    "            batch_count += 1\n",
    "\n",
    "    # Normalize Fisher information by the number of processed batches or samples\n",
    "    normalization_factor = batch_count if n_samples is None else min(batch_count, n_samples)\n",
    "    for name in fisher:\n",
    "        fisher[name] = fisher[name] / normalization_factor\n",
    "\n",
    "    # Integrate previous Fisher information with EWC scaling\n",
    "    if prev_fisher_info is not None:\n",
    "        for name in fisher:\n",
    "            if name in prev_fisher_info:\n",
    "                fisher[name] += ewc_gamma * prev_fisher_info[name]\n",
    "\n",
    "    # Restore the model's original training state\n",
    "    model.train(old_training_state)\n",
    "    \n",
    "    return fisher\n",
    "\n",
    "# Function to get variational posterior means\n",
    "def get_variational_posterior_means(model):\n",
    "    posterior_means = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'lora_A'):\n",
    "            for key in module.lora_A:\n",
    "                param_name = f\"{name}.lora_A.{key}\"\n",
    "                loc_name = f\"{param_name}_loc\"\n",
    "                if loc_name in pyro.get_param_store():\n",
    "                    lora_A_loc = pyro.param(loc_name).detach().clone()\n",
    "                    # Add '.weight' to the parameter name\n",
    "                    posterior_means[f\"{param_name}.weight\"] = lora_A_loc\n",
    "        if hasattr(module, 'lora_B'):\n",
    "            for key in module.lora_B:\n",
    "                param_name = f\"{name}.lora_B.{key}\"\n",
    "                loc_name = f\"{param_name}_loc\"\n",
    "                if loc_name in pyro.get_param_store():\n",
    "                    lora_B_loc = pyro.param(loc_name).detach().clone()\n",
    "                    # Add '.weight' to the parameter name\n",
    "                    posterior_means[f\"{param_name}.weight\"] = lora_B_loc\n",
    "    return posterior_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4265f3da-cf93-4c14-b872-82c7f2817cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import LoraLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc12ed2-8b42-4e1e-87f5-ed00d1eb2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import init_empty_weights\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "\n",
    "\n",
    "def initialize_lora():\n",
    "    login(\"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\")\n",
    "    # Set environment variable to manage memory fragmentation\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    \n",
    "     \n",
    "    # Specify directories and the path to the zip file\n",
    "    offload_dir = os.path.expanduser(\"llama_offload_evcl/\")\n",
    "     \n",
    "    os.makedirs(offload_dir, exist_ok=True)\n",
    "     \n",
    "    # Extract only the specified JSON file from the zip archive\n",
    "    os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Latest_Weights/QA_Weights')\n",
    "    target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "     \n",
    "    # Load tokenizer from Hugging Face\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "    # Load the model with accelerate's offloading and device map auto-setup\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B\",\n",
    "            device_map=\"auto\",\n",
    "            # max_memory=max_memory,\n",
    "            offload_folder=offload_dir,\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True\n",
    "        )\n",
    "     \n",
    "    # Configure LoRA with reduced rank\n",
    "    lora_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    #printing the trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if 'lora' in name:\n",
    "    #         print(name)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d62ff472-4fa4-47c2-b2bf-f39fd4fbfbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd35e6eb24a48539cef8d7b2fb8ea7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,703,936 || all params: 8,031,965,184 || trainable%: 0.0212\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading base model...\")\n",
    "model,tokenizer=initialize_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57385901-11a1-491b-abb3-1b924f4d6cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830f39530182478795a15a0f282d0621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Latest_Weights/QA_Weights')\n",
    "target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "instances = json_data['Instances'][0:2223]\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"output\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"attention_mask\"] = model_inputs.get(\"attention_mask\", None)\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization and set format\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Split dataset into train and eval\n",
    "train_size = int(0.9 * len(tokenized_datasets))\n",
    "train_dataset = tokenized_datasets.select(range(train_size))\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6015f59-7b73-46b6-940a-9f1ec22c3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(model, tokenizer, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93cecb98-c125-4a32-a87f-05e4addea699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            outputs = model(input_ids, labels=labels, attention_mask=attention_mask)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Evaluation Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ae06200-7093-45ff-bec3-abc1e93834af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "def run_lora_evcl_1(\n",
    "    num_epochs: int = 3,\n",
    "    base_model_name: str = \"meta-llama/Meta-Llama-3-8B\",\n",
    "    batch_size: int = 2,\n",
    "    learning_rate: float = 1e-5,\n",
    "    logging_steps: int = 100,\n",
    "    eval_steps: int = 200,\n",
    "    save_steps: int = 500,\n",
    "    output_dir: str = \"finetuned-weights-LoRA-EVCL\",\n",
    "):\n",
    "\n",
    "\n",
    "    def bayesian_guide(input_ids, attention_mask, labels):\n",
    "        # Define variational distributions over the LoRA parameters\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, 'lora_A'):\n",
    "                for key in module.lora_A:\n",
    "                    param_name = f\"{name}.lora_A.{key}\"\n",
    "                    lora_A_param = module.lora_A[key].weight\n",
    "                    device = lora_A_param.device\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        lora_A_param.clone().detach().to(device)\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        (0.1 * torch.ones_like(lora_A_param)).to(device),\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_A_param.dim())\n",
    "                    )\n",
    "            if hasattr(module, 'lora_B'):\n",
    "                for key in module.lora_B:\n",
    "                    param_name = f\"{name}.lora_B.{key}\"\n",
    "                    lora_B_param = module.lora_B[key].weight\n",
    "                    device = lora_B_param.device\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        lora_B_param.clone().detach().to(device)\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        (0.1 * torch.ones_like(lora_B_param)).to(device),\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_B_param.dim())\n",
    "                    )\n",
    "                    \n",
    "    def bayesian_model(input_ids, attention_mask, labels):\n",
    "        # Define a function to sample and substitute LoRA parameters\n",
    "        def model_with_sampled_lora():\n",
    "            # Sample LoRA parameters and set them in the model\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, 'lora_A'):\n",
    "                    for key in module.lora_A:\n",
    "                        param_name = f\"{name}.lora_A.{key}\"\n",
    "                        lora_A_module = module.lora_A[key]\n",
    "                        device = lora_A_module.weight.device\n",
    "    \n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                lora_A_module.weight.detach().to(device),\n",
    "                                (0.1 * torch.ones_like(lora_A_module.weight)).to(device)\n",
    "                            ).to_event(lora_A_module.weight.dim())\n",
    "                        )\n",
    "    \n",
    "                        # Assign the sampled weight to the module\n",
    "                        # lora_A_module.weight = torch.nn.Parameter(sampled_weight)\n",
    "                        lora_A_module.weight.data.copy_(sampled_weight)\n",
    "    \n",
    "                if hasattr(module, 'lora_B'):\n",
    "                    for key in module.lora_B:\n",
    "                        param_name = f\"{name}.lora_B.{key}\"\n",
    "                        lora_B_module = module.lora_B[key]\n",
    "                        device = lora_B_module.weight.device\n",
    "    \n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                lora_B_module.weight.detach().to(device),\n",
    "                                (0.1 * torch.ones_like(lora_B_module.weight)).to(device)\n",
    "                            ).to_event(lora_B_module.weight.dim())\n",
    "                        )\n",
    "    \n",
    "                        # Assign the sampled weight to the module\n",
    "                        # lora_B_module.weight = torch.nn.Parameter(sampled_weight)\n",
    "                        lora_B_module.weight.data.copy_(sampled_weight)\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            return loss\n",
    "    \n",
    "        # Use the modified model with sampled LoRA parameters\n",
    "        return model_with_sampled_lora()\n",
    "\n",
    "\n",
    "    # Set up SVI\n",
    "    pyro.clear_param_store()\n",
    "    optim = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "    elbo = TraceMeanField_ELBO()\n",
    "    svi = SVI(bayesian_model, bayesian_guide, optim, loss=elbo)\n",
    "\n",
    "    \n",
    "    # Training loop for Task 1\n",
    "    print(f\"Training on Task 1...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for num_batches, batch in enumerate(train_loader, 1):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            loss = svi.step(input_ids, attention_mask, labels)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Logging\n",
    "            if num_batches % logging_steps == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch {epoch}, Step {num_batches}, Loss: {avg_loss}\")\n",
    "\n",
    "            # Evaluation\n",
    "            if num_batches % eval_steps == 0:\n",
    "                evaluate_model(model, eval_loader)\n",
    "\n",
    "            # Save checkpoints\n",
    "            if num_batches % save_steps == 0:\n",
    "                save_trained_model(model, tokenizer, output_dir)\n",
    "\n",
    "        avg_epoch_loss = total_loss / num_batches\n",
    "        print(f\"Task 1 Epoch {epoch} completed. Average Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    # Save the final trained model after Task 1\n",
    "    save_trained_model(model, tokenizer, output_dir)\n",
    "    pyro.get_param_store().save('pyro_param_store_task1.pt')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a88408ab-cd21-43bf-a8bb-6c9f4e8e3b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Latest_Weights/QA_Weights\n",
      "/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ed87466-51c2-43f4-bccc-e0710e0b8fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Task 1...\n",
      "Epoch 0, Step 100, Loss: 851907.06\n",
      "Epoch 0, Step 200, Loss: 852017.54875\n",
      "Evaluation Loss: 15.8451\n",
      "Task 1 Epoch 0 completed. Average Loss: 852014.324\n",
      "Epoch 1, Step 100, Loss: 851873.406875\n",
      "Epoch 1, Step 200, Loss: 851949.38875\n",
      "Evaluation Loss: 15.7820\n",
      "Task 1 Epoch 1 completed. Average Loss: 851946.13\n",
      "Epoch 2, Step 100, Loss: 852003.260625\n",
      "Epoch 2, Step 200, Loss: 851964.3621875\n",
      "Evaluation Loss: 16.7253\n",
      "Task 1 Epoch 2 completed. Average Loss: 851949.122\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model=run_lora_evcl_1(\n",
    "        num_epochs=3,\n",
    "        base_model_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "        batch_size=2,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=100,\n",
    "        eval_steps=200,\n",
    "        save_steps=500,\n",
    "        output_dir=\"finetuned-weights-LoRA-EVCL\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64aee245-7e13-4698-8b75-7d72f09421c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdf5d6310bb4ff695fa04ecf89ac24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\",\n",
    "            device_map=\"auto\",\n",
    "            offload_folder='/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/llama_offload_evcl',\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "lora_model_path = \"/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/finetuned-weights-LoRA-EVCL\"\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "602a70a2-af6d-4078-98bf-473da225ec18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f3a69c3-3a02-4521-a882-a55b1fcca627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/1\n",
      "Processing batch 1\n",
      "Completed batch 1\n",
      "Processing batch 2\n",
      "Completed batch 2\n",
      "Processing batch 3\n",
      "Completed batch 3\n",
      "Processing batch 4\n",
      "Completed batch 4\n",
      "Processing batch 5\n",
      "Completed batch 5\n",
      "Processing batch 6\n",
      "Completed batch 6\n",
      "Processing batch 7\n",
      "Completed batch 7\n",
      "Processing batch 8\n",
      "Completed batch 8\n",
      "Processing batch 9\n",
      "Completed batch 9\n",
      "Processing batch 10\n",
      "Completed batch 10\n",
      "Processing batch 11\n",
      "Completed batch 11\n",
      "Processing batch 12\n",
      "Completed batch 12\n",
      "Processing batch 13\n",
      "Completed batch 13\n",
      "Processing batch 14\n",
      "Completed batch 14\n",
      "Processing batch 15\n",
      "Completed batch 15\n",
      "Processing batch 16\n",
      "Completed batch 16\n",
      "Processing batch 17\n",
      "Completed batch 17\n",
      "Processing batch 18\n",
      "Completed batch 18\n",
      "Processing batch 19\n",
      "Completed batch 19\n",
      "Processing batch 20\n",
      "Completed batch 20\n",
      "Processing batch 21\n",
      "Completed batch 21\n",
      "Processing batch 22\n",
      "Completed batch 22\n",
      "Processing batch 23\n",
      "Completed batch 23\n",
      "Processing batch 24\n",
      "Completed batch 24\n",
      "Processing batch 25\n",
      "Completed batch 25\n",
      "Processing batch 26\n",
      "Completed batch 26\n",
      "Processing batch 27\n",
      "Completed batch 27\n",
      "Processing batch 28\n",
      "Completed batch 28\n",
      "Processing batch 29\n",
      "Completed batch 29\n",
      "Processing batch 30\n",
      "Completed batch 30\n",
      "Processing batch 31\n",
      "Completed batch 31\n",
      "Processing batch 32\n",
      "Completed batch 32\n",
      "Processing batch 33\n",
      "Completed batch 33\n",
      "Processing batch 34\n",
      "Completed batch 34\n",
      "Processing batch 35\n",
      "Completed batch 35\n",
      "Processing batch 36\n",
      "Completed batch 36\n",
      "Processing batch 37\n",
      "Completed batch 37\n",
      "Processing batch 38\n",
      "Completed batch 38\n",
      "Processing batch 39\n",
      "Completed batch 39\n",
      "Processing batch 40\n",
      "Completed batch 40\n",
      "Processing batch 41\n",
      "Completed batch 41\n",
      "Processing batch 42\n",
      "Completed batch 42\n",
      "Processing batch 43\n",
      "Completed batch 43\n",
      "Processing batch 44\n",
      "Completed batch 44\n",
      "Processing batch 45\n",
      "Completed batch 45\n",
      "Processing batch 46\n",
      "Completed batch 46\n",
      "Processing batch 47\n",
      "Completed batch 47\n",
      "Processing batch 48\n",
      "Completed batch 48\n",
      "Processing batch 49\n",
      "Completed batch 49\n",
      "Processing batch 50\n",
      "Completed batch 50\n",
      "Processing batch 51\n",
      "Completed batch 51\n",
      "Processing batch 52\n",
      "Completed batch 52\n",
      "Processing batch 53\n",
      "Completed batch 53\n",
      "Processing batch 54\n",
      "Completed batch 54\n",
      "Processing batch 55\n",
      "Completed batch 55\n",
      "Processing batch 56\n",
      "Completed batch 56\n",
      "Processing batch 57\n",
      "Completed batch 57\n",
      "Processing batch 58\n",
      "Completed batch 58\n",
      "Processing batch 59\n",
      "Completed batch 59\n",
      "Processing batch 60\n",
      "Completed batch 60\n",
      "Processing batch 61\n",
      "Completed batch 61\n",
      "Processing batch 62\n",
      "Completed batch 62\n",
      "Processing batch 63\n",
      "Completed batch 63\n",
      "Processing batch 64\n",
      "Completed batch 64\n",
      "Processing batch 65\n",
      "Completed batch 65\n",
      "Processing batch 66\n",
      "Completed batch 66\n",
      "Processing batch 67\n",
      "Completed batch 67\n",
      "Processing batch 68\n",
      "Completed batch 68\n",
      "Processing batch 69\n",
      "Completed batch 69\n",
      "Processing batch 70\n",
      "Completed batch 70\n",
      "Processing batch 71\n",
      "Completed batch 71\n",
      "Processing batch 72\n",
      "Completed batch 72\n",
      "Processing batch 73\n",
      "Completed batch 73\n",
      "Processing batch 74\n",
      "Completed batch 74\n",
      "Processing batch 75\n",
      "Completed batch 75\n",
      "Processing batch 76\n",
      "Completed batch 76\n",
      "Processing batch 77\n",
      "Completed batch 77\n",
      "Processing batch 78\n",
      "Completed batch 78\n",
      "Processing batch 79\n",
      "Completed batch 79\n",
      "Processing batch 80\n",
      "Completed batch 80\n",
      "Processing batch 81\n",
      "Completed batch 81\n",
      "Processing batch 82\n",
      "Completed batch 82\n",
      "Processing batch 83\n",
      "Completed batch 83\n",
      "Processing batch 84\n",
      "Completed batch 84\n",
      "Processing batch 85\n",
      "Completed batch 85\n",
      "Processing batch 86\n",
      "Completed batch 86\n",
      "Processing batch 87\n",
      "Completed batch 87\n",
      "Processing batch 88\n",
      "Completed batch 88\n",
      "Processing batch 89\n",
      "Completed batch 89\n",
      "Processing batch 90\n",
      "Completed batch 90\n",
      "Processing batch 91\n",
      "Completed batch 91\n",
      "Processing batch 92\n",
      "Completed batch 92\n",
      "Processing batch 93\n",
      "Completed batch 93\n",
      "Processing batch 94\n",
      "Completed batch 94\n",
      "Processing batch 95\n",
      "Completed batch 95\n",
      "Processing batch 96\n",
      "Completed batch 96\n",
      "Processing batch 97\n",
      "Completed batch 97\n",
      "Processing batch 98\n",
      "Completed batch 98\n",
      "Processing batch 99\n",
      "Completed batch 99\n",
      "Processing batch 100\n",
      "Completed batch 100\n",
      "Processing batch 101\n",
      "Completed batch 101\n",
      "Processing batch 102\n",
      "Completed batch 102\n",
      "Processing batch 103\n",
      "Completed batch 103\n",
      "Processing batch 104\n",
      "Completed batch 104\n",
      "Processing batch 105\n",
      "Completed batch 105\n",
      "Processing batch 106\n",
      "Completed batch 106\n",
      "Processing batch 107\n",
      "Completed batch 107\n",
      "Processing batch 108\n",
      "Completed batch 108\n",
      "Processing batch 109\n",
      "Completed batch 109\n",
      "Processing batch 110\n",
      "Completed batch 110\n",
      "Processing batch 111\n",
      "Completed batch 111\n",
      "Processing batch 112\n",
      "Completed batch 112\n",
      "Processing batch 113\n",
      "Completed batch 113\n",
      "Processing batch 114\n",
      "Completed batch 114\n",
      "Processing batch 115\n",
      "Completed batch 115\n",
      "Processing batch 116\n",
      "Completed batch 116\n",
      "Processing batch 117\n",
      "Completed batch 117\n",
      "Processing batch 118\n",
      "Completed batch 118\n",
      "Processing batch 119\n",
      "Completed batch 119\n",
      "Processing batch 120\n",
      "Completed batch 120\n",
      "Processing batch 121\n",
      "Completed batch 121\n",
      "Processing batch 122\n",
      "Completed batch 122\n",
      "Processing batch 123\n",
      "Completed batch 123\n",
      "Processing batch 124\n",
      "Completed batch 124\n",
      "Processing batch 125\n",
      "Completed batch 125\n",
      "Processing batch 126\n",
      "Completed batch 126\n",
      "Processing batch 127\n",
      "Completed batch 127\n",
      "Processing batch 128\n",
      "Completed batch 128\n",
      "Processing batch 129\n",
      "Completed batch 129\n",
      "Processing batch 130\n",
      "Completed batch 130\n",
      "Processing batch 131\n",
      "Completed batch 131\n",
      "Processing batch 132\n",
      "Completed batch 132\n",
      "Processing batch 133\n",
      "Completed batch 133\n",
      "Processing batch 134\n",
      "Completed batch 134\n",
      "Processing batch 135\n",
      "Completed batch 135\n",
      "Processing batch 136\n",
      "Completed batch 136\n",
      "Processing batch 137\n",
      "Completed batch 137\n",
      "Processing batch 138\n",
      "Completed batch 138\n",
      "Processing batch 139\n",
      "Completed batch 139\n",
      "Processing batch 140\n",
      "Completed batch 140\n",
      "Processing batch 141\n",
      "Completed batch 141\n",
      "Processing batch 142\n",
      "Completed batch 142\n",
      "Processing batch 143\n",
      "Completed batch 143\n",
      "Processing batch 144\n",
      "Completed batch 144\n",
      "Processing batch 145\n",
      "Completed batch 145\n",
      "Processing batch 146\n",
      "Completed batch 146\n",
      "Processing batch 147\n",
      "Completed batch 147\n",
      "Processing batch 148\n",
      "Completed batch 148\n",
      "Processing batch 149\n",
      "Completed batch 149\n",
      "Processing batch 150\n",
      "Completed batch 150\n",
      "Processing batch 151\n",
      "Completed batch 151\n",
      "Processing batch 152\n",
      "Completed batch 152\n",
      "Processing batch 153\n",
      "Completed batch 153\n",
      "Processing batch 154\n",
      "Completed batch 154\n",
      "Processing batch 155\n",
      "Completed batch 155\n",
      "Processing batch 156\n",
      "Completed batch 156\n",
      "Processing batch 157\n",
      "Completed batch 157\n",
      "Processing batch 158\n",
      "Completed batch 158\n",
      "Processing batch 159\n",
      "Completed batch 159\n",
      "Processing batch 160\n",
      "Completed batch 160\n",
      "Processing batch 161\n",
      "Completed batch 161\n",
      "Processing batch 162\n",
      "Completed batch 162\n",
      "Processing batch 163\n",
      "Completed batch 163\n",
      "Processing batch 164\n",
      "Completed batch 164\n",
      "Processing batch 165\n",
      "Completed batch 165\n",
      "Processing batch 166\n",
      "Completed batch 166\n",
      "Processing batch 167\n",
      "Completed batch 167\n",
      "Processing batch 168\n",
      "Completed batch 168\n",
      "Processing batch 169\n",
      "Completed batch 169\n",
      "Processing batch 170\n",
      "Completed batch 170\n",
      "Processing batch 171\n",
      "Completed batch 171\n",
      "Processing batch 172\n",
      "Completed batch 172\n",
      "Processing batch 173\n",
      "Completed batch 173\n",
      "Processing batch 174\n",
      "Completed batch 174\n",
      "Processing batch 175\n",
      "Completed batch 175\n",
      "Processing batch 176\n",
      "Completed batch 176\n",
      "Processing batch 177\n",
      "Completed batch 177\n",
      "Processing batch 178\n",
      "Completed batch 178\n",
      "Processing batch 179\n",
      "Completed batch 179\n",
      "Processing batch 180\n",
      "Completed batch 180\n",
      "Processing batch 181\n",
      "Completed batch 181\n",
      "Processing batch 182\n",
      "Completed batch 182\n",
      "Processing batch 183\n",
      "Completed batch 183\n",
      "Processing batch 184\n",
      "Completed batch 184\n",
      "Processing batch 185\n",
      "Completed batch 185\n",
      "Processing batch 186\n",
      "Completed batch 186\n",
      "Processing batch 187\n",
      "Completed batch 187\n",
      "Processing batch 188\n",
      "Completed batch 188\n",
      "Processing batch 189\n",
      "Completed batch 189\n",
      "Processing batch 190\n",
      "Completed batch 190\n",
      "Processing batch 191\n",
      "Completed batch 191\n",
      "Processing batch 192\n",
      "Completed batch 192\n",
      "Processing batch 193\n",
      "Completed batch 193\n",
      "Processing batch 194\n",
      "Completed batch 194\n",
      "Processing batch 195\n",
      "Completed batch 195\n",
      "Processing batch 196\n",
      "Completed batch 196\n",
      "Processing batch 197\n",
      "Completed batch 197\n",
      "Processing batch 198\n",
      "Completed batch 198\n",
      "Processing batch 199\n",
      "Completed batch 199\n",
      "Processing batch 200\n",
      "Completed batch 200\n",
      "Processing batch 201\n",
      "Completed batch 201\n",
      "Processing batch 202\n",
      "Completed batch 202\n",
      "Processing batch 203\n",
      "Completed batch 203\n",
      "Processing batch 204\n",
      "Completed batch 204\n",
      "Processing batch 205\n",
      "Completed batch 205\n",
      "Processing batch 206\n",
      "Completed batch 206\n",
      "Processing batch 207\n",
      "Completed batch 207\n",
      "Processing batch 208\n",
      "Completed batch 208\n",
      "Processing batch 209\n",
      "Completed batch 209\n",
      "Processing batch 210\n",
      "Completed batch 210\n",
      "Processing batch 211\n",
      "Completed batch 211\n",
      "Processing batch 212\n",
      "Completed batch 212\n",
      "Processing batch 213\n",
      "Completed batch 213\n",
      "Processing batch 214\n",
      "Completed batch 214\n",
      "Processing batch 215\n",
      "Completed batch 215\n",
      "Processing batch 216\n",
      "Completed batch 216\n",
      "Processing batch 217\n",
      "Completed batch 217\n",
      "Processing batch 218\n",
      "Completed batch 218\n",
      "Processing batch 219\n",
      "Completed batch 219\n",
      "Processing batch 220\n",
      "Completed batch 220\n",
      "Processing batch 221\n",
      "Completed batch 221\n",
      "Processing batch 222\n",
      "Completed batch 222\n",
      "Processing batch 223\n",
      "Completed batch 223\n",
      "Processing batch 224\n",
      "Completed batch 224\n",
      "Processing batch 225\n",
      "Completed batch 225\n",
      "Processing batch 226\n",
      "Completed batch 226\n",
      "Processing batch 227\n",
      "Completed batch 227\n",
      "Processing batch 228\n",
      "Completed batch 228\n",
      "Processing batch 229\n",
      "Completed batch 229\n",
      "Processing batch 230\n",
      "Completed batch 230\n",
      "Processing batch 231\n",
      "Completed batch 231\n",
      "Processing batch 232\n",
      "Completed batch 232\n",
      "Processing batch 233\n",
      "Completed batch 233\n",
      "Processing batch 234\n",
      "Completed batch 234\n",
      "Processing batch 235\n",
      "Completed batch 235\n",
      "Processing batch 236\n",
      "Completed batch 236\n",
      "Processing batch 237\n",
      "Completed batch 237\n",
      "Processing batch 238\n",
      "Completed batch 238\n",
      "Processing batch 239\n",
      "Completed batch 239\n",
      "Processing batch 240\n",
      "Completed batch 240\n",
      "Processing batch 241\n",
      "Completed batch 241\n",
      "Processing batch 242\n",
      "Completed batch 242\n",
      "Processing batch 243\n",
      "Completed batch 243\n",
      "Processing batch 244\n",
      "Completed batch 244\n",
      "Processing batch 245\n",
      "Completed batch 245\n",
      "Processing batch 246\n",
      "Completed batch 246\n",
      "Processing batch 247\n",
      "Completed batch 247\n",
      "Processing batch 248\n",
      "Completed batch 248\n",
      "Processing batch 249\n",
      "Completed batch 249\n",
      "Processing batch 250\n",
      "Completed batch 250\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "prev_fisher_info = None\n",
    "prev_params = None\n",
    "ewc_gamma = 1.0  \n",
    "\n",
    "fisher_info = compute_fisher_info(\n",
    "    model=model,\n",
    "    data_loader=train_loader,\n",
    "    prev_fisher_info=prev_fisher_info,\n",
    "    ewc_gamma=ewc_gamma,\n",
    "    num_epochs=1,  \n",
    "    head_modules=None,  \n",
    "    n_samples=None  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7e157fa-3d0a-4f37-a264-8c6ada0f30aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.1619853973388672\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 0.8357223272323608\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 1.3880329132080078\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 3.1735033988952637\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.08619121462106705\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 0.1591525375843048\n",
      "Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.2895389795303345\n",
      "Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.6863220930099487\n",
      "Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.16036146879196167\n",
      "Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 0.09793616831302643\n",
      "Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.066110759973526\n",
      "Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.268744558095932\n",
      "Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.026460085064172745\n",
      "Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 0.0212889164686203\n",
      "Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.014594235457479954\n",
      "Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.07105840742588043\n",
      "Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.004461842589080334\n",
      "Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 0.005488422699272633\n",
      "Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.005418797489255667\n",
      "Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.02852371335029602\n",
      "Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.002480185590684414\n",
      "Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 0.0023140606936067343\n",
      "Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.004014892503619194\n",
      "Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.023763561621308327\n",
      "Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.0010368108050897717\n",
      "Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 0.0009359726682305336\n",
      "Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0027071181684732437\n",
      "Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.013135138899087906\n",
      "Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.000526268093381077\n",
      "Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 0.0005207362701185048\n",
      "Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0054238904267549515\n",
      "Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.01038832776248455\n",
      "Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.0003425923641771078\n",
      "Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 0.0003174656885676086\n",
      "Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0013240089174360037\n",
      "Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.002642915351316333\n",
      "Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.00016818512813188136\n",
      "Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 0.00017405685503035784\n",
      "Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0025184631813317537\n",
      "Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0046377768740057945\n",
      "Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.00014204152103047818\n",
      "Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 0.00011034922499675304\n",
      "Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0016181734390556812\n",
      "Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.013829050585627556\n",
      "Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 9.750960452947766e-05\n",
      "Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 5.9774152759928256e-05\n",
      "Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0015882240841165185\n",
      "Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0010413520503789186\n",
      "Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 5.108379264129326e-05\n",
      "Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 5.23762937518768e-05\n",
      "Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0010037904139608145\n",
      "Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0047394270077347755\n",
      "Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 6.338524690363556e-05\n",
      "Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 4.656410601455718e-05\n",
      "Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.001314318971708417\n",
      "Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0009595432202331722\n",
      "Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.4115327909821644e-05\n",
      "Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.797069464577362e-05\n",
      "Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0015953361289575696\n",
      "Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0008537362446077168\n",
      "Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.8410193661111407e-05\n",
      "Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 4.623194399755448e-05\n",
      "Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0013543511740863323\n",
      "Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.00964837521314621\n",
      "Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.4214998373063281e-05\n",
      "Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.4711378500796854e-05\n",
      "Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0015801668632775545\n",
      "Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0027688778936862946\n",
      "Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.2725186024908908e-05\n",
      "Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.787656194414012e-05\n",
      "Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0025851456448435783\n",
      "Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0021199649199843407\n",
      "Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.6041594790294766e-05\n",
      "Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.2143851310829632e-05\n",
      "Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.002788061508908868\n",
      "Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.003259469522163272\n",
      "Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 9.235092875314876e-06\n",
      "Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.4822726370766759e-05\n",
      "Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 8.909039752325043e-05\n",
      "Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.00021769662271253765\n",
      "Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 6.237165507627651e-06\n",
      "Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 8.928001989261247e-06\n",
      "Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.000384600309189409\n",
      "Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0011030799942091107\n",
      "Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 7.1544536695000716e-06\n",
      "Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 7.540050319221336e-06\n",
      "Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0006668958812952042\n",
      "Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.003481835825368762\n",
      "Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.776003152073827e-06\n",
      "Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 9.957211659639142e-06\n",
      "Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0005040307296440005\n",
      "Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.00016792905807960778\n",
      "Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 5.910546860832255e-06\n",
      "Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.1226607966818847e-05\n",
      "Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0002415233029751107\n",
      "Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0034670864697545767\n",
      "Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.492476818995783e-06\n",
      "Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 6.848814336990472e-06\n",
      "Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00020009253057651222\n",
      "Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0018748356960713863\n",
      "Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.659174010157585e-06\n",
      "Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 6.321123692032415e-06\n",
      "Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0003365395241416991\n",
      "Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0025686025619506836\n",
      "Layer: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.386492153367726e-06\n",
      "Layer: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.830432438509888e-06\n",
      "Layer: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0019025587243959308\n",
      "Layer: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.00040373147930949926\n",
      "Layer: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.6300252759247087e-06\n",
      "Layer: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.16013210976962e-06\n",
      "Layer: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 7.657281093997881e-05\n",
      "Layer: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0002455596113577485\n",
      "Layer: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.4134103518445045e-06\n",
      "Layer: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.4895439310057554e-06\n",
      "Layer: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00031436755671165884\n",
      "Layer: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.003309624269604683\n",
      "Layer: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.2445223041577265e-06\n",
      "Layer: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.5199190076818923e-06\n",
      "Layer: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0005246722139418125\n",
      "Layer: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0008916029473766685\n",
      "Layer: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.8401037777948659e-06\n",
      "Layer: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.0870329535682686e-06\n",
      "Layer: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00033507461193948984\n",
      "Layer: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.00266165635548532\n",
      "Layer: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 5.842869086336577e-07\n",
      "Layer: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 9.470020359003684e-07\n",
      "Layer: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00028304284205660224\n",
      "Layer: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.006284800358116627\n"
     ]
    }
   ],
   "source": [
    "for name, fisher_matrix in fisher_info.items():\n",
    "    print(f\"Layer: {name}, Fisher Info Mean: {fisher_matrix.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d1b4cfa-ef40-47e7-9f88-1940c51e0c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj\n",
      "base_model.model.model.layers.0.self_attn.v_proj\n",
      "base_model.model.model.layers.1.self_attn.q_proj\n",
      "base_model.model.model.layers.1.self_attn.v_proj\n",
      "base_model.model.model.layers.2.self_attn.q_proj\n",
      "base_model.model.model.layers.2.self_attn.v_proj\n",
      "base_model.model.model.layers.3.self_attn.q_proj\n",
      "base_model.model.model.layers.3.self_attn.v_proj\n",
      "base_model.model.model.layers.4.self_attn.q_proj\n",
      "base_model.model.model.layers.4.self_attn.v_proj\n",
      "base_model.model.model.layers.5.self_attn.q_proj\n",
      "base_model.model.model.layers.5.self_attn.v_proj\n",
      "base_model.model.model.layers.6.self_attn.q_proj\n",
      "base_model.model.model.layers.6.self_attn.v_proj\n",
      "base_model.model.model.layers.7.self_attn.q_proj\n",
      "base_model.model.model.layers.7.self_attn.v_proj\n",
      "base_model.model.model.layers.8.self_attn.q_proj\n",
      "base_model.model.model.layers.8.self_attn.v_proj\n",
      "base_model.model.model.layers.9.self_attn.q_proj\n",
      "base_model.model.model.layers.9.self_attn.v_proj\n",
      "base_model.model.model.layers.10.self_attn.q_proj\n",
      "base_model.model.model.layers.10.self_attn.v_proj\n",
      "base_model.model.model.layers.11.self_attn.q_proj\n",
      "base_model.model.model.layers.11.self_attn.v_proj\n",
      "base_model.model.model.layers.12.self_attn.q_proj\n",
      "base_model.model.model.layers.12.self_attn.v_proj\n",
      "base_model.model.model.layers.13.self_attn.q_proj\n",
      "base_model.model.model.layers.13.self_attn.v_proj\n",
      "base_model.model.model.layers.14.self_attn.q_proj\n",
      "base_model.model.model.layers.14.self_attn.v_proj\n",
      "base_model.model.model.layers.15.self_attn.q_proj\n",
      "base_model.model.model.layers.15.self_attn.v_proj\n",
      "base_model.model.model.layers.16.self_attn.q_proj\n",
      "base_model.model.model.layers.16.self_attn.v_proj\n",
      "base_model.model.model.layers.17.self_attn.q_proj\n",
      "base_model.model.model.layers.17.self_attn.v_proj\n",
      "base_model.model.model.layers.18.self_attn.q_proj\n",
      "base_model.model.model.layers.18.self_attn.v_proj\n",
      "base_model.model.model.layers.19.self_attn.q_proj\n",
      "base_model.model.model.layers.19.self_attn.v_proj\n",
      "base_model.model.model.layers.20.self_attn.q_proj\n",
      "base_model.model.model.layers.20.self_attn.v_proj\n",
      "base_model.model.model.layers.21.self_attn.q_proj\n",
      "base_model.model.model.layers.21.self_attn.v_proj\n",
      "base_model.model.model.layers.22.self_attn.q_proj\n",
      "base_model.model.model.layers.22.self_attn.v_proj\n",
      "base_model.model.model.layers.23.self_attn.q_proj\n",
      "base_model.model.model.layers.23.self_attn.v_proj\n",
      "base_model.model.model.layers.24.self_attn.q_proj\n",
      "base_model.model.model.layers.24.self_attn.v_proj\n",
      "base_model.model.model.layers.25.self_attn.q_proj\n",
      "base_model.model.model.layers.25.self_attn.v_proj\n",
      "base_model.model.model.layers.26.self_attn.q_proj\n",
      "base_model.model.model.layers.26.self_attn.v_proj\n",
      "base_model.model.model.layers.27.self_attn.q_proj\n",
      "base_model.model.model.layers.27.self_attn.v_proj\n",
      "base_model.model.model.layers.28.self_attn.q_proj\n",
      "base_model.model.model.layers.28.self_attn.v_proj\n",
      "base_model.model.model.layers.29.self_attn.q_proj\n",
      "base_model.model.model.layers.29.self_attn.v_proj\n",
      "base_model.model.model.layers.30.self_attn.q_proj\n",
      "base_model.model.model.layers.30.self_attn.v_proj\n",
      "base_model.model.model.layers.31.self_attn.q_proj\n",
      "base_model.model.model.layers.31.self_attn.v_proj\n"
     ]
    }
   ],
   "source": [
    "prev_posterior_means = get_variational_posterior_means(model)\n",
    "# torch.save(prev_posterior_means, f'posterior_means_task_{1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf1d6a1f-9a1c-4f64-923b-3a02a008e407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0273, -0.0820,  0.0260,  ...,  0.1615, -0.0500, -0.5330],\n",
       "         [-0.0019, -0.1523, -0.0506,  ..., -0.1359,  0.2239,  0.2128],\n",
       "         [-0.0098,  0.2263,  0.1196,  ...,  0.2271, -0.0216, -0.1201],\n",
       "         [ 0.1446,  0.0056,  0.0917,  ...,  0.2553,  0.2915,  0.0272]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.2107,  0.0365,  0.0640, -0.0585],\n",
       "         [ 0.1612,  0.1358,  0.0684, -0.3468],\n",
       "         [-0.0530,  0.0436, -0.1327,  0.1919],\n",
       "         ...,\n",
       "         [ 0.1370, -0.0671, -0.1661, -0.0700],\n",
       "         [-0.2560,  0.0196,  0.3673,  0.1771],\n",
       "         [-0.0113,  0.2186,  0.1336,  0.0868]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0365,  0.1549,  0.1681,  ..., -0.1015, -0.1428,  0.0453],\n",
       "         [ 0.2244, -0.1443,  0.0856,  ...,  0.2902, -0.0542,  0.1508],\n",
       "         [ 0.1003,  0.0826,  0.1011,  ...,  0.1032, -0.1306,  0.0238],\n",
       "         [-0.2078,  0.1873, -0.1199,  ...,  0.0608, -0.0093,  0.0269]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0182,  0.1159, -0.0277,  0.2725],\n",
       "         [ 0.2045, -0.0244,  0.0442,  0.1902],\n",
       "         [ 0.3584, -0.0397, -0.0556, -0.0710],\n",
       "         ...,\n",
       "         [-0.1688,  0.2691,  0.4824,  0.1356],\n",
       "         [-0.2149,  0.2495, -0.0253, -0.2126],\n",
       "         [ 0.0260, -0.2029, -0.1147, -0.2853]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0475, -0.0104, -0.2227,  ..., -0.1186, -0.0990,  0.0694],\n",
       "         [-0.0381, -0.0668,  0.0394,  ...,  0.3435,  0.0981, -0.0797],\n",
       "         [ 0.0817,  0.1342,  0.2123,  ..., -0.1766,  0.1388, -0.1921],\n",
       "         [ 0.0079, -0.2485,  0.1006,  ..., -0.3666, -0.1504,  0.2031]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.0165,  0.2578, -0.0065,  0.2460],\n",
       "         [ 0.2099,  0.0888, -0.2792,  0.2565],\n",
       "         [-0.2809, -0.2914, -0.0279, -0.0166],\n",
       "         ...,\n",
       "         [-0.2116,  0.0298,  0.0708, -0.1118],\n",
       "         [-0.1132,  0.0781, -0.0377, -0.1319],\n",
       "         [-0.0261, -0.1371, -0.1622,  0.1947]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0650,  0.1010, -0.0644,  ...,  0.2346, -0.0568,  0.1606],\n",
       "         [ 0.2091, -0.1245,  0.2441,  ...,  0.0250, -0.1166, -0.2747],\n",
       "         [ 0.0743, -0.1379,  0.1115,  ..., -0.2803,  0.2011, -0.0324],\n",
       "         [-0.1421,  0.2252,  0.1700,  ...,  0.2548,  0.0722,  0.0832]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0494,  0.0156, -0.3317, -0.1635],\n",
       "         [-0.1320,  0.3089, -0.3227,  0.2072],\n",
       "         [-0.2627, -0.1004, -0.1197,  0.0647],\n",
       "         ...,\n",
       "         [-0.2488, -0.0454,  0.0877, -0.2715],\n",
       "         [ 0.0280,  0.1297, -0.1666,  0.0572],\n",
       "         [-0.1402,  0.1213,  0.2421, -0.5557]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0687, -0.2711,  0.0190,  ...,  0.2212,  0.0269,  0.0319],\n",
       "         [ 0.0361, -0.1524,  0.2544,  ...,  0.3266,  0.2619,  0.0488],\n",
       "         [-0.0186, -0.3252,  0.4149,  ..., -0.0574,  0.2135, -0.1149],\n",
       "         [ 0.1044,  0.2702, -0.0282,  ...,  0.0636, -0.1443, -0.2490]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight': tensor([[-0.2101, -0.2000, -0.1000,  0.0669],\n",
       "         [ 0.0527,  0.2157,  0.1984,  0.1637],\n",
       "         [-0.1218, -0.0236, -0.1758,  0.0091],\n",
       "         ...,\n",
       "         [-0.1708,  0.1094, -0.1447, -0.1071],\n",
       "         [-0.0105, -0.0775,  0.0137, -0.0417],\n",
       "         [-0.0178, -0.1158, -0.0584, -0.4134]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0016, -0.4770,  0.0850,  ..., -0.3270, -0.0723, -0.0821],\n",
       "         [-0.2074, -0.0231,  0.0027,  ...,  0.0489, -0.1740,  0.1979],\n",
       "         [ 0.1509,  0.0704, -0.0108,  ..., -0.2164, -0.1519,  0.0815],\n",
       "         [ 0.4048,  0.0455,  0.0492,  ...,  0.0677, -0.1130,  0.3757]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0140, -0.1770, -0.2012,  0.2602],\n",
       "         [ 0.0652, -0.0604, -0.1902,  0.0998],\n",
       "         [ 0.2450, -0.0675,  0.1566, -0.1014],\n",
       "         ...,\n",
       "         [-0.0200,  0.1971,  0.0719,  0.1309],\n",
       "         [-0.0297,  0.0906, -0.0991,  0.2513],\n",
       "         [-0.2153, -0.0279, -0.0509, -0.1573]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.1034, -0.2247,  0.2266,  ..., -0.1278, -0.0053,  0.2690],\n",
       "         [-0.1129, -0.2272, -0.0007,  ...,  0.2087,  0.0123,  0.1268],\n",
       "         [-0.2159, -0.1083,  0.1811,  ..., -0.0067, -0.2317, -0.0181],\n",
       "         [-0.0056, -0.0333, -0.0990,  ..., -0.0070,  0.1761,  0.0748]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0210,  0.1624,  0.3469,  0.3118],\n",
       "         [ 0.1130, -0.0225,  0.0616,  0.0771],\n",
       "         [ 0.0740, -0.0115, -0.0672,  0.4027],\n",
       "         ...,\n",
       "         [ 0.1691, -0.1971, -0.2963,  0.0114],\n",
       "         [ 0.0247, -0.1693, -0.1490,  0.0744],\n",
       "         [ 0.0944,  0.0749,  0.0880, -0.2533]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0010,  0.2049,  0.2874,  ...,  0.0417, -0.2184,  0.2078],\n",
       "         [ 0.2532,  0.0913,  0.2108,  ..., -0.0855,  0.1748, -0.2031],\n",
       "         [ 0.0513, -0.0512, -0.0116,  ..., -0.2045, -0.0951, -0.2196],\n",
       "         [-0.1613, -0.0726,  0.0206,  ...,  0.0057, -0.0530,  0.1515]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0170,  0.1677, -0.1516,  0.1706],\n",
       "         [ 0.0945,  0.0918, -0.1539, -0.1226],\n",
       "         [-0.1069,  0.0465,  0.2400,  0.0665],\n",
       "         ...,\n",
       "         [ 0.0241, -0.0074, -0.2379, -0.0851],\n",
       "         [ 0.1363, -0.0865,  0.0047,  0.2208],\n",
       "         [ 0.3298,  0.2629,  0.1691, -0.0408]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight': tensor([[-0.1446, -0.4364, -0.0491,  ..., -0.0021,  0.0539, -0.1354],\n",
       "         [ 0.2491,  0.0588, -0.0483,  ...,  0.1404, -0.0681,  0.2360],\n",
       "         [ 0.2418,  0.1556,  0.2551,  ...,  0.5667, -0.0279, -0.1028],\n",
       "         [-0.2883,  0.1458, -0.2481,  ..., -0.1398,  0.1391, -0.1167]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.0226, -0.0590,  0.0173,  0.2818],\n",
       "         [ 0.0491,  0.1359, -0.3164,  0.1795],\n",
       "         [ 0.0233, -0.1280,  0.0636,  0.1161],\n",
       "         ...,\n",
       "         [ 0.2796,  0.2829,  0.2905,  0.0409],\n",
       "         [ 0.3031, -0.1642,  0.2469,  0.0361],\n",
       "         [ 0.2059, -0.3018, -0.0651,  0.3217]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight': tensor([[-0.3936,  0.2353, -0.0646,  ..., -0.0055, -0.1185,  0.0049],\n",
       "         [ 0.0948, -0.1797, -0.0675,  ..., -0.2327, -0.2807,  0.1482],\n",
       "         [-0.2603, -0.0359,  0.0101,  ..., -0.0795,  0.1624, -0.0665],\n",
       "         [-0.0815, -0.1185,  0.0455,  ...,  0.0193,  0.0388,  0.1269]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight': tensor([[-0.3065,  0.0579,  0.1371,  0.0154],\n",
       "         [ 0.0636, -0.3062, -0.0453,  0.0341],\n",
       "         [-0.0699, -0.3043, -0.2256, -0.0668],\n",
       "         ...,\n",
       "         [-0.0762, -0.0541, -0.1186,  0.0742],\n",
       "         [ 0.0139, -0.0390, -0.1171,  0.0778],\n",
       "         [ 0.2914,  0.0233, -0.1711, -0.0351]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0016, -0.1044, -0.0185,  ...,  0.2318,  0.0387, -0.0727],\n",
       "         [-0.2047,  0.1540, -0.1117,  ..., -0.3765,  0.3050,  0.1149],\n",
       "         [ 0.0841,  0.0444, -0.1743,  ...,  0.0322,  0.0339,  0.1610],\n",
       "         [-0.0457,  0.1528,  0.0239,  ..., -0.0658,  0.0273,  0.0725]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.1710, -0.1724,  0.0212, -0.1844],\n",
       "         [-0.0859,  0.1442, -0.1518,  0.2123],\n",
       "         [-0.1477,  0.1310, -0.1412,  0.2200],\n",
       "         ...,\n",
       "         [-0.0580,  0.2769, -0.0126,  0.1479],\n",
       "         [ 0.1701,  0.1400, -0.3790,  0.1437],\n",
       "         [-0.4033,  0.0238,  0.2007, -0.0925]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight': tensor([[ 1.6986e-01,  3.6853e-04, -9.4819e-02,  ...,  2.0137e-01,\n",
       "           1.4950e-01, -3.1260e-01],\n",
       "         [ 1.9670e-02, -2.7573e-02,  8.7403e-02,  ...,  2.1961e-01,\n",
       "          -3.7352e-01,  8.9012e-03],\n",
       "         [-1.0750e-01, -7.2991e-02,  7.0114e-02,  ..., -7.2490e-02,\n",
       "           3.7392e-02,  1.2897e-01],\n",
       "         [ 1.1999e-02, -2.0639e-01,  1.6525e-01,  ...,  1.0853e-02,\n",
       "          -7.6123e-02,  1.4081e-01]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0342, -0.3239, -0.3010,  0.1470],\n",
       "         [ 0.0480, -0.1772,  0.1551, -0.2698],\n",
       "         [ 0.0106, -0.1958, -0.0445, -0.0920],\n",
       "         ...,\n",
       "         [-0.1917,  0.3304, -0.2874,  0.1183],\n",
       "         [ 0.1682, -0.2353, -0.0786,  0.0757],\n",
       "         [-0.0265, -0.0562,  0.1192,  0.0977]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight': tensor([[-0.1717,  0.1029, -0.1716,  ..., -0.4066,  0.0395,  0.1511],\n",
       "         [ 0.2626, -0.0990,  0.1788,  ...,  0.0662,  0.1690, -0.1510],\n",
       "         [-0.0326, -0.1670,  0.0303,  ...,  0.0822, -0.0442, -0.0631],\n",
       "         [ 0.0796,  0.2079, -0.0573,  ..., -0.1448,  0.0360,  0.0436]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0902, -0.2950, -0.1163,  0.1090],\n",
       "         [ 0.0671, -0.0233,  0.2193, -0.2219],\n",
       "         [-0.1953, -0.0484, -0.0742, -0.1367],\n",
       "         ...,\n",
       "         [-0.0057, -0.4116, -0.0716,  0.1288],\n",
       "         [ 0.1330, -0.1677, -0.1882, -0.1979],\n",
       "         [-0.1633,  0.2661,  0.3294, -0.1555]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0020, -0.1581,  0.1905,  ..., -0.0547, -0.2136, -0.0945],\n",
       "         [-0.0893, -0.1157, -0.1638,  ..., -0.4089, -0.1322, -0.0817],\n",
       "         [-0.2879,  0.0065, -0.1057,  ...,  0.2032, -0.4201,  0.1359],\n",
       "         [-0.0140,  0.0135,  0.1973,  ...,  0.0137,  0.0904,  0.0869]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.2756, -0.0732, -0.0250, -0.2836],\n",
       "         [-0.1696, -0.5625, -0.2520, -0.0321],\n",
       "         [ 0.0840, -0.2273,  0.0430,  0.0529],\n",
       "         ...,\n",
       "         [-0.4128, -0.1040,  0.0225,  0.0706],\n",
       "         [ 0.3231,  0.1406,  0.1246,  0.0073],\n",
       "         [ 0.1398,  0.0503,  0.0010,  0.0616]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0961, -0.1079, -0.4342,  ..., -0.0711,  0.2908, -0.2034],\n",
       "         [-0.2326,  0.0311, -0.0507,  ...,  0.1803,  0.4090,  0.3901],\n",
       "         [ 0.2360,  0.1833, -0.1425,  ..., -0.2634, -0.0126, -0.3248],\n",
       "         [-0.3092, -0.0821,  0.1307,  ..., -0.0917,  0.1840, -0.1459]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0857, -0.1661, -0.0386,  0.0928],\n",
       "         [-0.1467,  0.1131,  0.2542,  0.0849],\n",
       "         [-0.3407, -0.0279, -0.1318,  0.0705],\n",
       "         ...,\n",
       "         [-0.0713, -0.0911, -0.1624, -0.1747],\n",
       "         [-0.1989,  0.2126, -0.1726, -0.0372],\n",
       "         [ 0.0802, -0.1304,  0.1348,  0.0622]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0229,  0.1054,  0.1088,  ...,  0.0964,  0.0795, -0.0120],\n",
       "         [-0.1243,  0.1967, -0.0014,  ...,  0.0773,  0.0392,  0.0922],\n",
       "         [-0.3203,  0.3261,  0.0753,  ...,  0.0375,  0.2365,  0.0116],\n",
       "         [-0.1751, -0.0640, -0.0069,  ..., -0.0356,  0.3121,  0.3749]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0638, -0.0809,  0.0698, -0.2904],\n",
       "         [-0.0871, -0.0112, -0.0343,  0.1131],\n",
       "         [-0.2335, -0.0274, -0.2574, -0.2444],\n",
       "         ...,\n",
       "         [ 0.3253,  0.3048,  0.0651, -0.0424],\n",
       "         [-0.0254,  0.0578,  0.0792, -0.2428],\n",
       "         [-0.2571,  0.0338,  0.0480,  0.3197]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0497, -0.0160,  0.3176,  ..., -0.0109, -0.0691,  0.3792],\n",
       "         [-0.1999,  0.0553,  0.1041,  ...,  0.1203,  0.0712,  0.1115],\n",
       "         [ 0.1575,  0.0487, -0.0672,  ..., -0.1005, -0.2279,  0.2138],\n",
       "         [-0.0558,  0.1051,  0.0313,  ...,  0.1473,  0.0664,  0.0364]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.4254, -0.3534, -0.2208,  0.1007],\n",
       "         [-0.2184,  0.0852,  0.0129, -0.2662],\n",
       "         [ 0.0821,  0.0746, -0.2119, -0.1110],\n",
       "         ...,\n",
       "         [ 0.2499,  0.3175, -0.2532, -0.1075],\n",
       "         [-0.3963,  0.1198, -0.2077,  0.1069],\n",
       "         [-0.0112,  0.1029,  0.3245, -0.2771]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0601,  0.0559,  0.1535,  ...,  0.0504, -0.2805, -0.0404],\n",
       "         [ 0.0428,  0.0146,  0.1582,  ..., -0.0786,  0.0698,  0.0634],\n",
       "         [-0.0935,  0.1911, -0.5309,  ...,  0.1728, -0.1668, -0.1712],\n",
       "         [-0.0935,  0.0754,  0.0752,  ..., -0.2249, -0.2658, -0.0854]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0982,  0.1057, -0.1463,  0.0469],\n",
       "         [ 0.1553,  0.0741, -0.1659, -0.1209],\n",
       "         [-0.1728,  0.0608,  0.1301, -0.0548],\n",
       "         ...,\n",
       "         [-0.1002, -0.2291, -0.0074,  0.1473],\n",
       "         [ 0.0559, -0.4925,  0.1149,  0.0533],\n",
       "         [-0.1228,  0.0441,  0.0032,  0.0826]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight': tensor([[-0.2041, -0.0166, -0.0407,  ...,  0.0793, -0.0347,  0.1492],\n",
       "         [-0.1913,  0.1950,  0.0497,  ...,  0.0877,  0.0581, -0.0196],\n",
       "         [ 0.1151,  0.1397, -0.0396,  ...,  0.1186,  0.3329, -0.0348],\n",
       "         [-0.1893, -0.1494,  0.3366,  ...,  0.0832, -0.1987,  0.3824]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.2255, -0.1927, -0.2041, -0.0814],\n",
       "         [ 0.3055,  0.1012, -0.4358, -0.0630],\n",
       "         [-0.0123, -0.2614, -0.0209, -0.0113],\n",
       "         ...,\n",
       "         [ 0.1434,  0.1024,  0.2592,  0.2133],\n",
       "         [ 0.2924,  0.1307, -0.1737,  0.2638],\n",
       "         [-0.1668,  0.1281,  0.0724, -0.2571]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight': tensor([[-0.1559, -0.1557, -0.0601,  ...,  0.1441,  0.1888, -0.0916],\n",
       "         [ 0.1713,  0.1521,  0.1164,  ..., -0.1369, -0.0523,  0.0512],\n",
       "         [-0.1308, -0.0914, -0.2672,  ..., -0.2544, -0.1446, -0.0810],\n",
       "         [-0.0929, -0.1260,  0.2748,  ..., -0.1241,  0.0876,  0.0385]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0385,  0.1887,  0.0971, -0.0441],\n",
       "         [-0.1610, -0.0510, -0.0675, -0.1500],\n",
       "         [-0.1429, -0.2018,  0.0584,  0.0081],\n",
       "         ...,\n",
       "         [-0.0903,  0.1462,  0.0523,  0.1626],\n",
       "         [ 0.0449, -0.3561, -0.1698, -0.0334],\n",
       "         [-0.0307,  0.1595,  0.0866,  0.0889]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight': tensor([[-0.1550, -0.0710, -0.0018,  ..., -0.0841, -0.0696, -0.2311],\n",
       "         [-0.0890,  0.1117,  0.0874,  ...,  0.0571,  0.2168,  0.0846],\n",
       "         [ 0.1279, -0.1414, -0.0444,  ..., -0.0475,  0.3017,  0.0060],\n",
       "         [ 0.0881, -0.2343, -0.2096,  ..., -0.3112,  0.1148,  0.3025]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight': tensor([[-0.1529,  0.1107, -0.0734,  0.2344],\n",
       "         [-0.0614,  0.1019, -0.0916,  0.4376],\n",
       "         [-0.0867, -0.2718, -0.1129, -0.1198],\n",
       "         ...,\n",
       "         [-0.0693,  0.2295,  0.0742,  0.1236],\n",
       "         [ 0.1223,  0.1190, -0.0568, -0.0063],\n",
       "         [-0.2447,  0.0771,  0.1024,  0.1974]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0649, -0.2013, -0.0433,  ..., -0.1196, -0.0413, -0.3527],\n",
       "         [ 0.0220, -0.0126,  0.0251,  ..., -0.1549,  0.1384, -0.1595],\n",
       "         [ 0.3889, -0.0307, -0.1002,  ...,  0.2860, -0.2156, -0.0099],\n",
       "         [ 0.3919,  0.1771,  0.0667,  ...,  0.0647,  0.1842, -0.2561]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight': tensor([[-0.1801,  0.1886, -0.2815,  0.0280],\n",
       "         [-0.0986, -0.0508,  0.0270, -0.1002],\n",
       "         [ 0.0224, -0.0631, -0.2000, -0.2392],\n",
       "         ...,\n",
       "         [ 0.1752,  0.2082, -0.1420,  0.2678],\n",
       "         [ 0.0060,  0.0614, -0.1736,  0.0666],\n",
       "         [ 0.3004, -0.1453,  0.3176, -0.1719]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight': tensor([[-0.1333,  0.1685,  0.0163,  ...,  0.1526, -0.3929,  0.1483],\n",
       "         [-0.0394,  0.1669,  0.0594,  ..., -0.0955, -0.1368, -0.1314],\n",
       "         [-0.0548,  0.1005, -0.0593,  ...,  0.1081,  0.0318, -0.1063],\n",
       "         [-0.0995,  0.1195, -0.0748,  ..., -0.2319, -0.1878,  0.1097]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0237,  0.0075,  0.0805, -0.2288],\n",
       "         [-0.0370,  0.0612, -0.0849, -0.1679],\n",
       "         [-0.0472,  0.2520,  0.2118,  0.0023],\n",
       "         ...,\n",
       "         [-0.1177,  0.0021,  0.2166, -0.1165],\n",
       "         [ 0.2081, -0.2300, -0.0328,  0.1045],\n",
       "         [ 0.0649,  0.0295, -0.1460, -0.1592]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.1588, -0.0155,  0.1372,  ..., -0.0833, -0.0777,  0.4016],\n",
       "         [ 0.0192, -0.0627,  0.0354,  ..., -0.1455,  0.1192, -0.0385],\n",
       "         [-0.3356, -0.1450,  0.0051,  ...,  0.1345, -0.1055,  0.1137],\n",
       "         [-0.1565, -0.1327,  0.2360,  ..., -0.1336, -0.0428,  0.0541]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight': tensor([[-0.1282, -0.3199, -0.0550, -0.0105],\n",
       "         [-0.0534,  0.0371,  0.1222,  0.0780],\n",
       "         [ 0.4313, -0.2391, -0.2227, -0.2054],\n",
       "         ...,\n",
       "         [ 0.3481,  0.1888, -0.1452,  0.1650],\n",
       "         [ 0.2325, -0.2540,  0.2132,  0.2829],\n",
       "         [-0.3303, -0.1135, -0.1743,  0.0811]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0956,  0.3190, -0.0981,  ...,  0.1545, -0.0578, -0.2899],\n",
       "         [-0.3882, -0.0383,  0.1066,  ...,  0.0155, -0.0986, -0.0150],\n",
       "         [-0.1428, -0.0787,  0.0414,  ..., -0.0083,  0.0588, -0.1475],\n",
       "         [-0.0710,  0.1107, -0.1744,  ...,  0.0213, -0.1863,  0.0024]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight': tensor([[-0.1276, -0.0820, -0.4118, -0.3092],\n",
       "         [ 0.0366,  0.0294,  0.0368, -0.2040],\n",
       "         [-0.0841, -0.1181,  0.1213,  0.1466],\n",
       "         ...,\n",
       "         [ 0.1022,  0.0372, -0.3999, -0.3972],\n",
       "         [ 0.0378,  0.1801, -0.1674,  0.1059],\n",
       "         [ 0.1115,  0.0745,  0.0847, -0.0442]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0210, -0.0434, -0.3905,  ..., -0.0482,  0.2226,  0.1659],\n",
       "         [ 0.0233, -0.1487,  0.1080,  ...,  0.0623,  0.4523, -0.3081],\n",
       "         [ 0.3127, -0.1717, -0.0499,  ..., -0.1967, -0.0835, -0.0304],\n",
       "         [-0.1540,  0.1252,  0.0130,  ...,  0.1357, -0.0332,  0.2449]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.3705,  0.1444,  0.3101,  0.0114],\n",
       "         [ 0.0103,  0.0862, -0.1521, -0.0245],\n",
       "         [-0.0521, -0.0600,  0.3928,  0.3048],\n",
       "         ...,\n",
       "         [-0.1508,  0.0641, -0.1618, -0.0892],\n",
       "         [ 0.1720, -0.3135, -0.0884, -0.4490],\n",
       "         [-0.2019, -0.0439,  0.0695,  0.2122]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight': tensor([[-0.1568,  0.0920,  0.0043,  ...,  0.0790, -0.1448,  0.0548],\n",
       "         [ 0.0733, -0.0494, -0.0408,  ...,  0.1044,  0.0102, -0.0263],\n",
       "         [ 0.1703, -0.0497, -0.1332,  ..., -0.2211,  0.1163,  0.1767],\n",
       "         [-0.1724,  0.1852, -0.0297,  ...,  0.0241, -0.0567,  0.1347]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.3476,  0.1661, -0.0339,  0.0773],\n",
       "         [ 0.4740,  0.1303,  0.4382,  0.0390],\n",
       "         [ 0.2421,  0.0246, -0.0519,  0.0029],\n",
       "         ...,\n",
       "         [ 0.1515, -0.0625, -0.0387,  0.0524],\n",
       "         [-0.2993, -0.0289,  0.0582,  0.0596],\n",
       "         [-0.0661,  0.2543, -0.0547, -0.1086]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0004, -0.0148, -0.1118,  ...,  0.1058, -0.2966, -0.2750],\n",
       "         [-0.1032, -0.1570,  0.0966,  ..., -0.2473, -0.2274, -0.1599],\n",
       "         [ 0.0386, -0.1239,  0.0005,  ..., -0.1818, -0.2835, -0.0422],\n",
       "         [ 0.1937, -0.0030, -0.0701,  ..., -0.0185,  0.3122, -0.2610]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight': tensor([[-0.2606, -0.1577,  0.2563, -0.1553],\n",
       "         [ 0.2666,  0.2833, -0.0460, -0.1460],\n",
       "         [-0.1529,  0.2468,  0.2698, -0.4383],\n",
       "         ...,\n",
       "         [ 0.2156,  0.0740,  0.0932,  0.1938],\n",
       "         [ 0.2361,  0.2994, -0.0693,  0.0684],\n",
       "         [ 0.1115, -0.3392,  0.0307,  0.1175]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0669, -0.2645, -0.0590,  ..., -0.3366,  0.0315,  0.4115],\n",
       "         [-0.0328,  0.0343,  0.2198,  ..., -0.1430, -0.0702,  0.0452],\n",
       "         [ 0.1428, -0.1532, -0.0686,  ..., -0.1558, -0.1671, -0.0447],\n",
       "         [ 0.0191,  0.0113, -0.2464,  ...,  0.0183,  0.5554, -0.2171]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight': tensor([[-0.1822,  0.1247,  0.1462, -0.2287],\n",
       "         [-0.0552,  0.1156, -0.4175,  0.0103],\n",
       "         [-0.0610, -0.1047,  0.0290, -0.0234],\n",
       "         ...,\n",
       "         [-0.0270,  0.1148,  0.0252,  0.2090],\n",
       "         [ 0.2538, -0.0919, -0.0800,  0.0602],\n",
       "         [ 0.2113, -0.0392, -0.2601,  0.0786]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.2973,  0.1676, -0.2627,  ..., -0.1495, -0.2195,  0.4999],\n",
       "         [ 0.0324, -0.2994, -0.2211,  ...,  0.1123, -0.3579,  0.1531],\n",
       "         [-0.1464,  0.0665,  0.2225,  ...,  0.0526, -0.1726,  0.0786],\n",
       "         [-0.2743,  0.0566,  0.1702,  ..., -0.0455, -0.0475,  0.2680]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.1067,  0.1338,  0.2151,  0.2032],\n",
       "         [-0.0007, -0.2390, -0.2013,  0.0052],\n",
       "         [-0.1464,  0.1649, -0.1007,  0.2160],\n",
       "         ...,\n",
       "         [-0.1430, -0.0159, -0.2168, -0.2909],\n",
       "         [ 0.1903, -0.1877,  0.2675, -0.0918],\n",
       "         [ 0.2488,  0.1475,  0.0340,  0.1488]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0176,  0.1714, -0.0720,  ..., -0.0077,  0.0180, -0.4648],\n",
       "         [ 0.0319,  0.1309,  0.0584,  ..., -0.1396,  0.0512, -0.4979],\n",
       "         [ 0.0917, -0.2093,  0.0039,  ...,  0.2195, -0.3600,  0.0190],\n",
       "         [-0.1051, -0.1983,  0.0724,  ...,  0.3215,  0.0102,  0.1738]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0464,  0.0161,  0.2037, -0.0448],\n",
       "         [ 0.0493, -0.0229, -0.1797,  0.2328],\n",
       "         [ 0.0667, -0.3487,  0.0987, -0.0134],\n",
       "         ...,\n",
       "         [ 0.0763,  0.1123, -0.1448, -0.1079],\n",
       "         [-0.1101,  0.0097, -0.0827, -0.1438],\n",
       "         [-0.4422, -0.1042, -0.1296,  0.1292]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0516,  0.1613,  0.1215,  ...,  0.0450, -0.0681, -0.3396],\n",
       "         [-0.0819,  0.1364, -0.0426,  ..., -0.0374, -0.0723,  0.1853],\n",
       "         [-0.3027,  0.0496, -0.0142,  ..., -0.1076,  0.1609,  0.0051],\n",
       "         [-0.0587, -0.1790, -0.1359,  ..., -0.1739,  0.3731, -0.0208]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.1293, -0.0145, -0.2925, -0.2251],\n",
       "         [ 0.0706, -0.0917,  0.1972, -0.0166],\n",
       "         [ 0.0827,  0.0147, -0.1035, -0.0969],\n",
       "         ...,\n",
       "         [-0.0986,  0.2649, -0.0170,  0.2186],\n",
       "         [ 0.2343, -0.1697,  0.2277, -0.0112],\n",
       "         [-0.0960,  0.1323,  0.0483, -0.1045]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight': tensor([[-0.2291, -0.0016,  0.3732,  ..., -0.2594, -0.1469, -0.0238],\n",
       "         [ 0.0744, -0.1054, -0.0862,  ...,  0.1542, -0.0137, -0.1121],\n",
       "         [ 0.1195,  0.1320,  0.0195,  ...,  0.0655,  0.2360,  0.0333],\n",
       "         [ 0.1305, -0.1256, -0.0183,  ..., -0.1293,  0.2360, -0.0017]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight': tensor([[-0.1511,  0.2265,  0.4028,  0.0678],\n",
       "         [ 0.1338,  0.1400, -0.2032,  0.1263],\n",
       "         [-0.0498,  0.1900,  0.0029,  0.1273],\n",
       "         ...,\n",
       "         [ 0.0575,  0.2584,  0.1322, -0.0286],\n",
       "         [-0.2804,  0.1039, -0.2947, -0.0041],\n",
       "         [-0.0025,  0.2396, -0.2805,  0.1729]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight': tensor([[-0.1287, -0.0076,  0.4928,  ..., -0.0171, -0.2257, -0.0130],\n",
       "         [-0.2752,  0.1058, -0.2988,  ...,  0.1503, -0.0982,  0.0887],\n",
       "         [-0.0396, -0.2080, -0.1421,  ..., -0.0267,  0.2720,  0.0060],\n",
       "         [ 0.0052,  0.1708,  0.1349,  ..., -0.1758, -0.0703,  0.0873]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0061, -0.0430, -0.2005, -0.0990],\n",
       "         [ 0.1868, -0.2195,  0.0235,  0.1124],\n",
       "         [-0.0789,  0.0703, -0.0160,  0.0472],\n",
       "         ...,\n",
       "         [ 0.0650,  0.1758, -0.1586,  0.2188],\n",
       "         [ 0.0280,  0.1373, -0.4150, -0.2088],\n",
       "         [ 0.2185,  0.2082, -0.0679,  0.3162]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0691, -0.1307,  0.2813,  ...,  0.1238,  0.1040, -0.0933],\n",
       "         [ 0.1581,  0.0625,  0.2521,  ..., -0.0178, -0.1944,  0.1061],\n",
       "         [ 0.1132, -0.1921,  0.2153,  ...,  0.0783,  0.0550,  0.0617],\n",
       "         [ 0.0037, -0.1341, -0.0218,  ..., -0.1243,  0.1702,  0.0907]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0115,  0.1088, -0.2540,  0.2597],\n",
       "         [ 0.0414, -0.1481, -0.0693,  0.2455],\n",
       "         [-0.1902,  0.1783, -0.0688,  0.0816],\n",
       "         ...,\n",
       "         [ 0.2991,  0.1286,  0.1936,  0.1249],\n",
       "         [ 0.0969,  0.0687,  0.0830, -0.0739],\n",
       "         [-0.0482,  0.0057,  0.0060, -0.3272]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0841, -0.2551,  0.1482,  ..., -0.1573,  0.2506, -0.1213],\n",
       "         [-0.0499, -0.0092,  0.0998,  ...,  0.0467, -0.1009, -0.0036],\n",
       "         [-0.3253,  0.1106,  0.0293,  ..., -0.0172,  0.2364, -0.0921],\n",
       "         [ 0.3142,  0.0391,  0.0938,  ...,  0.1186,  0.2146,  0.1625]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight': tensor([[-0.2028,  0.2302, -0.1199,  0.2339],\n",
       "         [ 0.2153, -0.2225,  0.0634, -0.0641],\n",
       "         [ 0.0448, -0.2513,  0.1194,  0.0787],\n",
       "         ...,\n",
       "         [ 0.2139,  0.3004,  0.0111, -0.0123],\n",
       "         [ 0.3345,  0.1671,  0.1008, -0.1424],\n",
       "         [-0.0525,  0.0407,  0.1124, -0.1683]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.2232, -0.0741, -0.4542,  ...,  0.1695, -0.2018,  0.3293],\n",
       "         [ 0.2001, -0.0919,  0.0125,  ...,  0.1240,  0.0038, -0.0485],\n",
       "         [ 0.2167,  0.0664,  0.1305,  ..., -0.1493, -0.0324,  0.0335],\n",
       "         [ 0.0643,  0.1882, -0.0089,  ..., -0.2810,  0.0560, -0.0104]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.0266,  0.2438,  0.0193, -0.1513],\n",
       "         [ 0.0708,  0.1845, -0.2321,  0.1049],\n",
       "         [ 0.1672,  0.2139,  0.0884, -0.1471],\n",
       "         ...,\n",
       "         [ 0.3017,  0.1238, -0.0888, -0.1112],\n",
       "         [ 0.0803,  0.1300,  0.0074,  0.0665],\n",
       "         [ 0.2770,  0.2973, -0.2214,  0.1932]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.1256, -0.0479,  0.3480,  ...,  0.1562,  0.1643, -0.0318],\n",
       "         [ 0.1014, -0.0616, -0.0487,  ...,  0.2771,  0.1994, -0.0812],\n",
       "         [-0.1328,  0.0790, -0.1047,  ..., -0.4589, -0.1610, -0.3092],\n",
       "         [ 0.0388,  0.1147,  0.1291,  ..., -0.0437,  0.0888,  0.0590]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0578, -0.1134,  0.1075, -0.0646],\n",
       "         [ 0.1068,  0.1027, -0.0070,  0.1256],\n",
       "         [ 0.1090, -0.4052,  0.0056,  0.2461],\n",
       "         ...,\n",
       "         [ 0.1565,  0.1953,  0.0437, -0.1416],\n",
       "         [ 0.1783,  0.0230,  0.1513, -0.1520],\n",
       "         [-0.1568, -0.1267,  0.1056,  0.3604]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight': tensor([[-0.1652,  0.0013,  0.1031,  ..., -0.2114,  0.1182,  0.1528],\n",
       "         [-0.3486,  0.3850,  0.1610,  ..., -0.0627, -0.1177, -0.0842],\n",
       "         [-0.1227, -0.0310,  0.0373,  ...,  0.3912,  0.0984, -0.0372],\n",
       "         [-0.2132, -0.0472,  0.0662,  ..., -0.2421, -0.1523,  0.1206]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.4508,  0.1405,  0.0125,  0.0441],\n",
       "         [-0.0191, -0.0411, -0.1879,  0.2139],\n",
       "         [ 0.0902,  0.1602,  0.0389,  0.1468],\n",
       "         ...,\n",
       "         [ 0.2102,  0.3320, -0.1924,  0.2439],\n",
       "         [-0.1229,  0.1129,  0.2449,  0.0471],\n",
       "         [-0.0034, -0.0825,  0.1143,  0.2982]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.2704, -0.1461, -0.1078,  ..., -0.0451,  0.1136,  0.1770],\n",
       "         [ 0.0057, -0.0880,  0.1713,  ..., -0.1417, -0.0072,  0.1276],\n",
       "         [ 0.2485,  0.0736, -0.0029,  ..., -0.2816,  0.0365,  0.1385],\n",
       "         [-0.1799,  0.2471,  0.0634,  ...,  0.1583,  0.0723,  0.2489]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight': tensor([[-0.2008,  0.1614, -0.3036,  0.0357],\n",
       "         [-0.0335, -0.0996, -0.0663, -0.0275],\n",
       "         [ 0.1632,  0.0725,  0.0582,  0.1605],\n",
       "         ...,\n",
       "         [ 0.1291,  0.0365, -0.1976,  0.2673],\n",
       "         [-0.1341,  0.1601, -0.2556,  0.3782],\n",
       "         [ 0.0104, -0.1625, -0.2739,  0.2018]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0518, -0.0184, -0.0420,  ..., -0.0364,  0.2665, -0.0332],\n",
       "         [-0.1755,  0.0775,  0.0733,  ...,  0.0610, -0.0365, -0.0042],\n",
       "         [ 0.2147,  0.0155,  0.0435,  ...,  0.1095,  0.2698, -0.0901],\n",
       "         [ 0.3411,  0.0764, -0.3062,  ..., -0.1789,  0.1926,  0.1094]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.2645,  0.2442, -0.0984,  0.1424],\n",
       "         [-0.0800, -0.1659,  0.0574, -0.1272],\n",
       "         [ 0.1287, -0.1136, -0.0107, -0.0590],\n",
       "         ...,\n",
       "         [-0.2202, -0.2201,  0.1232,  0.0547],\n",
       "         [-0.1661, -0.1706,  0.1559,  0.0977],\n",
       "         [-0.0523, -0.3088,  0.1634, -0.2834]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.1340,  0.1911,  0.0254,  ..., -0.2351, -0.2451, -0.2203],\n",
       "         [-0.2706,  0.0193,  0.3609,  ...,  0.2442,  0.1993, -0.0203],\n",
       "         [-0.0256,  0.0208,  0.2109,  ..., -0.4451, -0.0261,  0.1093],\n",
       "         [ 0.4048,  0.0926,  0.2538,  ..., -0.2006, -0.1457, -0.2678]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0572,  0.0106, -0.1657,  0.0144],\n",
       "         [-0.2413,  0.2808, -0.0274,  0.0814],\n",
       "         [-0.1007,  0.1057, -0.0323,  0.0819],\n",
       "         ...,\n",
       "         [-0.0179,  0.0377,  0.0036,  0.0672],\n",
       "         [ 0.1707, -0.3459, -0.0208, -0.0674],\n",
       "         [-0.1811, -0.0564,  0.0580, -0.1182]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0378,  0.0517,  0.0056,  ...,  0.2262, -0.0356, -0.2802],\n",
       "         [-0.1814,  0.1186, -0.0031,  ...,  0.3270,  0.0596,  0.1157],\n",
       "         [-0.1846,  0.4519,  0.0557,  ..., -0.2226,  0.0539, -0.2100],\n",
       "         [-0.0753, -0.0541, -0.1645,  ...,  0.0867,  0.0108,  0.0139]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight': tensor([[-0.0775,  0.1286, -0.0460,  0.1966],\n",
       "         [-0.4202, -0.1997,  0.1294, -0.0713],\n",
       "         [ 0.1892,  0.0926, -0.0813, -0.1408],\n",
       "         ...,\n",
       "         [ 0.1048, -0.0056, -0.1016, -0.0780],\n",
       "         [-0.1733,  0.1120,  0.1524,  0.2596],\n",
       "         [-0.0559,  0.2203,  0.2772, -0.2738]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.1292, -0.2349,  0.1255,  ...,  0.0627, -0.2357,  0.1399],\n",
       "         [ 0.0209, -0.3096,  0.4469,  ...,  0.0516, -0.2483, -0.0351],\n",
       "         [ 0.0771,  0.1334,  0.1384,  ...,  0.2474, -0.1827,  0.1086],\n",
       "         [ 0.3574,  0.1014, -0.4074,  ..., -0.0408, -0.0574, -0.1027]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.3105, -0.2015, -0.1638,  0.2375],\n",
       "         [-0.1680, -0.2854, -0.1237, -0.0332],\n",
       "         [ 0.0256,  0.0693, -0.1040,  0.4245],\n",
       "         ...,\n",
       "         [ 0.1265,  0.2802,  0.0270, -0.1299],\n",
       "         [ 0.0590,  0.0399,  0.2121, -0.1188],\n",
       "         [-0.0780,  0.5646, -0.2045,  0.3248]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0498,  0.1407, -0.0085,  ...,  0.3843, -0.1702,  0.0116],\n",
       "         [-0.0132, -0.3028,  0.0418,  ..., -0.0400,  0.3518,  0.1111],\n",
       "         [ 0.0231, -0.0979,  0.2367,  ..., -0.0113, -0.1259,  0.1384],\n",
       "         [ 0.1611,  0.0215, -0.0386,  ..., -0.1833,  0.0804,  0.0999]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight': tensor([[ 1.1022e-01, -1.4816e-02,  1.2540e-01,  8.7760e-02],\n",
       "         [-1.7045e-01, -4.1741e-01,  1.3987e-01, -2.8536e-01],\n",
       "         [-5.7371e-02, -2.5921e-02,  1.0685e-01,  2.9140e-02],\n",
       "         ...,\n",
       "         [ 2.2892e-02,  2.4216e-01,  1.1198e-01,  7.0882e-02],\n",
       "         [ 1.1969e-01,  8.6766e-02,  5.9874e-02, -1.1828e-01],\n",
       "         [-2.3218e-02,  1.3355e-04, -2.5475e-01, -6.7372e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0638, -0.0008, -0.0841,  ..., -0.0124,  0.1967,  0.1020],\n",
       "         [-0.0561,  0.0143,  0.0728,  ..., -0.1633,  0.0735, -0.0411],\n",
       "         [ 0.1761, -0.1147,  0.0294,  ..., -0.0409, -0.0498,  0.0746],\n",
       "         [-0.3989, -0.0642, -0.3312,  ...,  0.2221, -0.0601,  0.2727]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight': tensor([[-0.1689, -0.1938, -0.0561, -0.3300],\n",
       "         [ 0.1606, -0.0357,  0.1015,  0.0832],\n",
       "         [ 0.1757, -0.0325, -0.0299,  0.1479],\n",
       "         ...,\n",
       "         [-0.0508,  0.1817, -0.0625, -0.0867],\n",
       "         [-0.2214, -0.1938,  0.1535,  0.2309],\n",
       "         [-0.1040,  0.0163, -0.0681,  0.0373]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight': tensor([[-0.1242, -0.1502,  0.0392,  ..., -0.0868,  0.1228,  0.2692],\n",
       "         [-0.0215, -0.0159,  0.1491,  ...,  0.1957,  0.2012, -0.3679],\n",
       "         [ 0.2948, -0.1643,  0.0809,  ...,  0.1668, -0.2539, -0.1399],\n",
       "         [ 0.1677, -0.0093, -0.0079,  ..., -0.0066,  0.3793, -0.2851]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.2347, -0.0909,  0.1227, -0.0340],\n",
       "         [ 0.1972,  0.0304, -0.1525, -0.1481],\n",
       "         [ 0.0525,  0.1141,  0.2847, -0.4320],\n",
       "         ...,\n",
       "         [-0.1260, -0.0536, -0.0853, -0.0963],\n",
       "         [-0.0042,  0.0121, -0.3392, -0.1403],\n",
       "         [ 0.1617, -0.2952,  0.2613, -0.0190]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.1221,  0.1422, -0.0975,  ..., -0.0360,  0.2163,  0.0348],\n",
       "         [ 0.2935,  0.1319,  0.0544,  ...,  0.1038, -0.0148,  0.0429],\n",
       "         [ 0.0238, -0.0251, -0.1716,  ...,  0.1387, -0.5176,  0.2565],\n",
       "         [-0.1160, -0.0239, -0.0326,  ..., -0.1883, -0.2387, -0.0920]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight': tensor([[-0.3120,  0.0111, -0.2698,  0.0046],\n",
       "         [-0.3009, -0.0375, -0.2244,  0.2205],\n",
       "         [-0.1557,  0.3911, -0.3273, -0.0092],\n",
       "         ...,\n",
       "         [ 0.0133,  0.3160,  0.0428,  0.1820],\n",
       "         [-0.1329, -0.0474,  0.2109, -0.0844],\n",
       "         [-0.0310,  0.0090, -0.2013,  0.4241]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0536,  0.0240, -0.0462,  ..., -0.0322,  0.1096, -0.2944],\n",
       "         [ 0.1125,  0.0244, -0.1109,  ...,  0.1557,  0.0094, -0.2690],\n",
       "         [-0.2498, -0.2215, -0.3425,  ..., -0.1629, -0.2223, -0.2232],\n",
       "         [-0.0749, -0.3175,  0.3265,  ..., -0.0122,  0.0486, -0.0795]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight': tensor([[-0.1514, -0.1573,  0.0163, -0.2570],\n",
       "         [ 0.1239,  0.1062, -0.0220,  0.2705],\n",
       "         [ 0.1222, -0.0006, -0.0685, -0.0451],\n",
       "         ...,\n",
       "         [-0.0031, -0.0482,  0.0646, -0.0275],\n",
       "         [ 0.0961, -0.3103,  0.1507, -0.2266],\n",
       "         [-0.1659, -0.0993,  0.1849, -0.1037]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.0157, -0.0579, -0.3003,  ..., -0.0174, -0.0731,  0.0674],\n",
       "         [-0.0739,  0.0581,  0.0596,  ..., -0.1898, -0.0453,  0.0936],\n",
       "         [-0.3060,  0.1367, -0.0114,  ...,  0.0259,  0.1275,  0.1447],\n",
       "         [-0.1625,  0.1391,  0.0427,  ..., -0.0616, -0.3960,  0.0264]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0357, -0.0758,  0.0867, -0.1317],\n",
       "         [ 0.1964, -0.0793, -0.4088, -0.0626],\n",
       "         [ 0.2392,  0.0138,  0.1341, -0.1848],\n",
       "         ...,\n",
       "         [ 0.1070, -0.0334,  0.0569, -0.2693],\n",
       "         [ 0.1307,  0.0096, -0.0793, -0.0448],\n",
       "         [ 0.0403,  0.0740, -0.1137,  0.0326]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0017, -0.2901, -0.1838,  ...,  0.1060, -0.0299, -0.1108],\n",
       "         [-0.0708,  0.1510, -0.0064,  ..., -0.1495,  0.5095,  0.2165],\n",
       "         [-0.0201, -0.1669, -0.0921,  ..., -0.1021,  0.2449, -0.2479],\n",
       "         [-0.1070, -0.1446,  0.2744,  ...,  0.1543,  0.0424,  0.0041]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight': tensor([[ 1.5841e-01,  3.0506e-01, -2.5409e-05,  8.8497e-02],\n",
       "         [-5.8322e-02, -3.9891e-03,  3.7226e-02, -1.0903e-03],\n",
       "         [ 8.4097e-03,  1.3195e-01, -3.7152e-02, -4.0638e-01],\n",
       "         ...,\n",
       "         [-3.8306e-02, -2.9015e-02, -1.2549e-01, -4.3796e-02],\n",
       "         [-2.9339e-01, -9.9329e-02, -6.0502e-02,  1.7781e-01],\n",
       "         [-1.3267e-01,  4.0529e-01,  6.0854e-02, -7.1577e-02]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.2944, -0.0449, -0.0722,  ..., -0.0411,  0.0027,  0.0241],\n",
       "         [ 0.1366, -0.0603, -0.1009,  ...,  0.1586,  0.0845,  0.0043],\n",
       "         [-0.1337,  0.1337, -0.0074,  ..., -0.0534,  0.0223, -0.0487],\n",
       "         [ 0.0982, -0.0426, -0.0004,  ...,  0.0484,  0.0365,  0.2158]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight': tensor([[-0.1738, -0.2979,  0.2382, -0.1188],\n",
       "         [-0.1176, -0.0304, -0.0068, -0.1618],\n",
       "         [ 0.0308,  0.0154,  0.1254, -0.1668],\n",
       "         ...,\n",
       "         [-0.0073, -0.0554,  0.1570, -0.2107],\n",
       "         [ 0.0632,  0.0769,  0.3736,  0.1092],\n",
       "         [ 0.0078,  0.1442,  0.1417, -0.0129]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight': tensor([[-0.2967, -0.1517, -0.2501,  ...,  0.2510,  0.3391, -0.0661],\n",
       "         [-0.0032, -0.0358,  0.3085,  ..., -0.3059,  0.1444, -0.0295],\n",
       "         [ 0.3607,  0.0570,  0.3763,  ..., -0.0521, -0.2840,  0.0244],\n",
       "         [ 0.3553,  0.0361, -0.0043,  ..., -0.0522, -0.2478, -0.1668]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.0835,  0.0120, -0.2592, -0.1908],\n",
       "         [-0.2083,  0.1134,  0.0899,  0.1033],\n",
       "         [-0.1681, -0.1139,  0.0074,  0.2583],\n",
       "         ...,\n",
       "         [ 0.0489, -0.1029,  0.0703,  0.0786],\n",
       "         [ 0.1230, -0.2279,  0.0637,  0.0711],\n",
       "         [ 0.2154, -0.2675, -0.3390,  0.1963]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.1511,  0.0027, -0.0262,  ..., -0.0071, -0.1702,  0.1368],\n",
       "         [ 0.1328, -0.0696, -0.2924,  ..., -0.1687,  0.1280,  0.1178],\n",
       "         [-0.1301, -0.0454,  0.0133,  ..., -0.0237,  0.3701, -0.1357],\n",
       "         [-0.0178, -0.1267, -0.0307,  ..., -0.0733,  0.1158,  0.0947]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.0777, -0.0594,  0.1363, -0.2528],\n",
       "         [-0.1445, -0.3954, -0.0586,  0.3654],\n",
       "         [-0.0942,  0.0519,  0.1269,  0.0264],\n",
       "         ...,\n",
       "         [-0.1962, -0.1604, -0.2052,  0.3142],\n",
       "         [ 0.0358, -0.2116, -0.0381,  0.1095],\n",
       "         [ 0.2995,  0.1261, -0.0579,  0.1614]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.0103, -0.2087,  0.0166,  ...,  0.1397,  0.2295,  0.1111],\n",
       "         [ 0.2256, -0.5468, -0.2225,  ..., -0.1092,  0.1116,  0.1231],\n",
       "         [-0.2728,  0.1873, -0.0530,  ...,  0.0687, -0.2698, -0.2447],\n",
       "         [ 0.1078,  0.0063, -0.0090,  ...,  0.0472,  0.2155,  0.1567]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.0385,  0.2897, -0.0608,  0.1108],\n",
       "         [-0.0315,  0.3684, -0.0506, -0.1358],\n",
       "         [-0.0999, -0.0030,  0.1613, -0.1059],\n",
       "         ...,\n",
       "         [-0.0503, -0.2178,  0.3751,  0.1548],\n",
       "         [-0.1541,  0.1799, -0.1879,  0.0987],\n",
       "         [ 0.1577, -0.3050,  0.1587, -0.0150]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight': tensor([[ 0.1117, -0.0149,  0.0527,  ...,  0.1428,  0.2886,  0.1042],\n",
       "         [-0.3378, -0.0209, -0.0652,  ..., -0.2366, -0.0684, -0.0972],\n",
       "         [ 0.0553,  0.1599, -0.0025,  ...,  0.4402, -0.2529, -0.2152],\n",
       "         [ 0.1133,  0.0365,  0.2394,  ..., -0.0402,  0.3145, -0.2449]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.1728, -0.1344, -0.3548,  0.0099],\n",
       "         [ 0.0347, -0.1769, -0.0458, -0.1566],\n",
       "         [ 0.1847, -0.2786,  0.0155,  0.1186],\n",
       "         ...,\n",
       "         [ 0.2222, -0.2509,  0.0262,  0.0311],\n",
       "         [ 0.0105,  0.2186,  0.0427,  0.2047],\n",
       "         [ 0.2506,  0.2726,  0.0308, -0.0170]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight': tensor([[ 0.2427,  0.1291, -0.4186,  ..., -0.2646, -0.0582,  0.1597],\n",
       "         [ 0.0858, -0.0536, -0.3618,  ..., -0.1054, -0.0579, -0.0268],\n",
       "         [ 0.1293,  0.1739, -0.1650,  ...,  0.1577,  0.1139, -0.2736],\n",
       "         [-0.2138,  0.0324, -0.0486,  ..., -0.0286,  0.0635, -0.3681]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight': tensor([[-0.2845,  0.2717,  0.1973, -0.0091],\n",
       "         [ 0.1210,  0.0195,  0.2272, -0.0538],\n",
       "         [ 0.0975,  0.1599,  0.0368,  0.2502],\n",
       "         ...,\n",
       "         [ 0.1618, -0.0577, -0.2555,  0.0245],\n",
       "         [ 0.0047,  0.0667, -0.0563,  0.0835],\n",
       "         [-0.1721,  0.1459,  0.0903, -0.2198]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight': tensor([[-0.1369,  0.1010, -0.3004,  ...,  0.0908, -0.3185, -0.0562],\n",
       "         [ 0.3046, -0.0742,  0.0408,  ...,  0.1179, -0.3628,  0.0465],\n",
       "         [ 0.1497,  0.1963, -0.0895,  ..., -0.0500, -0.0248, -0.0092],\n",
       "         [-0.3562,  0.0092, -0.2338,  ...,  0.1983, -0.3053, -0.0395]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0243, -0.3313,  0.0594,  0.0235],\n",
       "         [ 0.5184,  0.1207,  0.0203,  0.1434],\n",
       "         [ 0.0104, -0.3086, -0.0443,  0.1415],\n",
       "         ...,\n",
       "         [-0.1273, -0.1293,  0.0241,  0.1184],\n",
       "         [-0.0116,  0.0485, -0.0327, -0.0192],\n",
       "         [-0.0843, -0.3020, -0.0087,  0.1880]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight': tensor([[-0.2477, -0.1321, -0.2552,  ...,  0.1043, -0.3734, -0.0049],\n",
       "         [-0.2463,  0.1388, -0.1431,  ...,  0.0879, -0.3082, -0.4294],\n",
       "         [ 0.1513, -0.2286,  0.3040,  ..., -0.2301,  0.0950,  0.1445],\n",
       "         [-0.0168,  0.0667, -0.0735,  ..., -0.0076,  0.0823, -0.1446]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight': tensor([[ 1.5317e-01,  1.9087e-01,  1.0754e-01,  1.9324e-02],\n",
       "         [ 3.4215e-02,  1.8132e-01, -3.5341e-02, -3.5540e-02],\n",
       "         [-9.8628e-03, -1.3741e-02,  2.8603e-01,  1.0170e-01],\n",
       "         ...,\n",
       "         [-9.1784e-02,  4.9038e-02, -8.2119e-02, -1.4852e-01],\n",
       "         [ 2.7927e-02,  8.2390e-02, -4.6430e-01, -8.0560e-02],\n",
       "         [-4.4228e-01,  1.6941e-01, -5.4926e-02, -9.0603e-05]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight': tensor([[-0.0188, -0.0818, -0.2599,  ..., -0.3244, -0.5576,  0.0460],\n",
       "         [ 0.0727, -0.0163,  0.0308,  ..., -0.0306, -0.0403,  0.2100],\n",
       "         [-0.0267, -0.2543, -0.1244,  ...,  0.0843, -0.0144,  0.1162],\n",
       "         [ 0.2283, -0.0591,  0.0835,  ...,  0.0802,  0.0797, -0.0465]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight': tensor([[ 0.1825, -0.0063,  0.1650, -0.1990],\n",
       "         [ 0.1477,  0.2555,  0.1986, -0.1415],\n",
       "         [-0.1149, -0.1808,  0.0094, -0.2746],\n",
       "         ...,\n",
       "         [ 0.0428, -0.1515, -0.0797, -0.0665],\n",
       "         [-0.2934, -0.1938,  0.0070,  0.0889],\n",
       "         [-0.2190,  0.2564,  0.1160,  0.2318]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0551,  0.1450,  0.0547,  ..., -0.1775, -0.2612,  0.0111],\n",
       "         [ 0.0758, -0.1802, -0.0047,  ..., -0.1237,  0.2529, -0.1445],\n",
       "         [-0.4516, -0.2038, -0.2158,  ..., -0.0223, -0.0406,  0.1328],\n",
       "         [-0.1543, -0.1174,  0.2986,  ...,  0.0837,  0.0667,  0.2020]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight': tensor([[-0.1312,  0.0152,  0.1422,  0.2104],\n",
       "         [ 0.0365, -0.1668,  0.0336,  0.0005],\n",
       "         [ 0.1685, -0.1931, -0.0386, -0.1154],\n",
       "         ...,\n",
       "         [ 0.0168, -0.0116,  0.0200,  0.1892],\n",
       "         [ 0.1720, -0.0837, -0.3547, -0.2761],\n",
       "         [ 0.1597,  0.0719, -0.0211,  0.1932]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight': tensor([[-0.3483,  0.1300, -0.2245,  ..., -0.0549, -0.1249, -0.0022],\n",
       "         [-0.0186,  0.1200, -0.1366,  ..., -0.0995, -0.2141, -0.2760],\n",
       "         [ 0.2039,  0.0191, -0.1299,  ..., -0.1208, -0.1579, -0.0079],\n",
       "         [ 0.3203,  0.1963, -0.0807,  ...,  0.0167,  0.0955,  0.0841]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0149,  0.1786, -0.0621,  0.0295],\n",
       "         [ 0.2152,  0.2506, -0.2599, -0.1885],\n",
       "         [-0.1005, -0.2990, -0.0705,  0.0432],\n",
       "         ...,\n",
       "         [-0.3359,  0.1427, -0.1680,  0.1974],\n",
       "         [ 0.2715, -0.1994,  0.1410, -0.0909],\n",
       "         [ 0.0527,  0.0774,  0.1670, -0.0568]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight': tensor([[-0.0029,  0.2586, -0.1362,  ...,  0.1738,  0.3047,  0.0977],\n",
       "         [ 0.2349, -0.1574,  0.1308,  ...,  0.0946,  0.4486,  0.1944],\n",
       "         [-0.1275, -0.1791, -0.0203,  ..., -0.0529,  0.1214, -0.2669],\n",
       "         [-0.1427,  0.2016,  0.1660,  ..., -0.3021, -0.1623, -0.1897]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight': tensor([[ 0.0119,  0.3261, -0.0120, -0.0579],\n",
       "         [-0.0603,  0.1479, -0.2544, -0.0670],\n",
       "         [-0.2092,  0.2948,  0.0418, -0.0924],\n",
       "         ...,\n",
       "         [-0.1452,  0.1897, -0.1566, -0.1147],\n",
       "         [-0.0138,  0.0670, -0.0311, -0.0809],\n",
       "         [-0.0174, -0.0072,  0.0253, -0.2771]], device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight': tensor([[-0.3520, -0.0577, -0.0370,  ..., -0.2784, -0.0338, -0.1518],\n",
       "         [-0.3031,  0.1100,  0.1262,  ...,  0.1841,  0.4522, -0.1324],\n",
       "         [ 0.2345,  0.2292, -0.3029,  ...,  0.2678, -0.0224, -0.0841],\n",
       "         [ 0.2267, -0.1827, -0.2602,  ...,  0.0221, -0.1358, -0.1863]],\n",
       "        device='cuda:0'),\n",
       " 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight': tensor([[-0.0859,  0.2623,  0.1108, -0.0907],\n",
       "         [-0.1710,  0.1411, -0.2688, -0.2122],\n",
       "         [ 0.1049, -0.1856,  0.0467,  0.0467],\n",
       "         ...,\n",
       "         [ 0.1472, -0.0096,  0.1548, -0.4993],\n",
       "         [ 0.2963, -0.2851, -0.0818, -0.0417],\n",
       "         [ 0.0094, -0.1587,  0.2055, -0.3393]], device='cuda:0')}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_posterior_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234530c-a2c0-4698-b7f2-8a9e390585de",
   "metadata": {},
   "source": [
    "### Task 2: QA+QG EVCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e0bd22e-e909-48d8-ac98-23e524827601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "import torch\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO\n",
    "\n",
    "def run_lora_evcl_2(\n",
    "    num_epochs: int = 3,\n",
    "    batch_size: int = 2,\n",
    "    learning_rate: float = 1e-5,\n",
    "    logging_steps: int = 100,\n",
    "    eval_steps: int = 200,\n",
    "    save_steps: int = 500,\n",
    "    output_dir: str = \"finetuned-weights-LoRA-EVCL-2\",\n",
    "    prev_fisher_info: dict = None,            \n",
    "    prev_posterior_means: dict = None,        \n",
    "    ewc_lambda: float = 0.0,                  \n",
    "    synthetic_data_loader=None,               # Synthetic data from Task 1\n",
    "    combined_loader=None,                     # Data loader for Task 2\n",
    "    eval_loader=None,                         # Evaluation data loader\n",
    "    tokenizer=None,\n",
    "    model=None\n",
    "):\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Ensure all parameters require gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False  # Freeze non-LoRA parameters\n",
    "\n",
    "    def bayesian_guide(input_ids, attention_mask, labels):\n",
    "        # Define variational distributions over the LoRA parameters\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, 'lora_A'):\n",
    "                for key in module.lora_A:\n",
    "                    param_name = f\"{name}.lora_A.{key}\"\n",
    "                    lora_A_param = module.lora_A[key].weight\n",
    "                    device = lora_A_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_A_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_A_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_A_param.dim())\n",
    "                    )\n",
    "            if hasattr(module, 'lora_B'):\n",
    "                for key in module.lora_B:\n",
    "                    param_name = f\"{name}.lora_B.{key}\"\n",
    "                    lora_B_param = module.lora_B[key].weight\n",
    "                    device = lora_B_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_B_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_B_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_B_param.dim())\n",
    "                    )\n",
    "                        \n",
    "    def bayesian_model(input_ids, attention_mask, labels):\n",
    "        # Do NOT register the entire model with Pyro\n",
    "        # pyro.module(\"model\", model)  # Removed\n",
    "\n",
    "        # Define a function to sample and substitute LoRA parameters\n",
    "        def model_with_sampled_lora():\n",
    "            # Sample LoRA parameters and set them in the model\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, 'lora_A'):\n",
    "                    for key in module.lora_A:\n",
    "                        param_name = f\"{name}.lora_A.{key}\"\n",
    "                        lora_A_module = module.lora_A[key]\n",
    "                        device = lora_A_module.weight.device\n",
    "\n",
    "                        # Use posterior mean from Task 1 as prior mean\n",
    "                        prior_mean = prev_posterior_means.get(param_name, lora_A_module.weight.detach().clone()).to(device)\n",
    "                        prior_std = (0.1 * torch.ones_like(lora_A_module.weight)).to(device)\n",
    "\n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                prior_mean,\n",
    "                                prior_std\n",
    "                            ).to_event(lora_A_module.weight.dim())\n",
    "                        )\n",
    "\n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_A_module.weight.copy_(sampled_weight)\n",
    "\n",
    "                if hasattr(module, 'lora_B'):\n",
    "                    for key in module.lora_B:\n",
    "                        param_name = f\"{name}.lora_B.{key}\"\n",
    "                        lora_B_module = module.lora_B[key]\n",
    "                        device = lora_B_module.weight.device\n",
    "\n",
    "                        # Use posterior mean from Task 1 as prior mean\n",
    "                        prior_mean = prev_posterior_means.get(param_name, lora_B_module.weight.detach().clone()).to(device)\n",
    "                        prior_std = (0.1 * torch.ones_like(lora_B_module.weight)).to(device)\n",
    "\n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                prior_mean,\n",
    "                                prior_std\n",
    "                            ).to_event(lora_B_module.weight.dim())\n",
    "                        )\n",
    "\n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_B_module.weight.copy_(sampled_weight)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Add EWC penalty if previous Fisher info and posterior means are provided\n",
    "            if prev_fisher_info is not None and prev_posterior_means is not None and ewc_lambda > 0.0:\n",
    "                ewc_penalty = 0.0\n",
    "                for name, param in model.named_parameters():\n",
    "                    if 'lora' in name and name in prev_fisher_info:\n",
    "                        fisher = prev_fisher_info[name].to(DEVICE)\n",
    "                        prev_mean = prev_posterior_means[name].to(DEVICE)\n",
    "                        ewc_penalty += (fisher * (param - prev_mean) ** 2).sum()\n",
    "                loss += ewc_lambda * ewc_penalty\n",
    "\n",
    "            return loss\n",
    "\n",
    "        # Use the modified model with sampled LoRA parameters\n",
    "        return model_with_sampled_lora()\n",
    "\n",
    "    # Set up SVI\n",
    "    pyro.clear_param_store()\n",
    "    optim = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "    elbo = TraceMeanField_ELBO()\n",
    "    svi = SVI(bayesian_model, bayesian_guide, optim, loss=elbo)\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training on new task with EWC and synthetic data from previous task...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for num_batches, batch in enumerate(combined_loader, 1):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            loss = svi.step(input_ids, attention_mask, labels)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Logging\n",
    "            if num_batches % logging_steps == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch {epoch + 1}, Step {num_batches}, Loss: {avg_loss}\")\n",
    "\n",
    "            # Evaluation\n",
    "            if num_batches % eval_steps == 0:\n",
    "                evaluate_model(model, eval_loader)\n",
    "\n",
    "            # Save checkpoints\n",
    "            if num_batches % save_steps == 0:\n",
    "                save_trained_model(model, tokenizer, output_dir)\n",
    "\n",
    "        avg_epoch_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    # Save the final trained model after the task\n",
    "    save_trained_model(model, tokenizer, output_dir)\n",
    "    pyro.get_param_store().save('pyro_param_store_task2.pt')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pyro.distributions as dist\n",
    "# import pyro.poutine as poutine\n",
    "\n",
    "# def run_lora_evcl_2(\n",
    "#     num_epochs: int = 3,\n",
    "#     batch_size: int = 2,\n",
    "#     learning_rate: float = 1e-5,\n",
    "#     logging_steps: int = 100,\n",
    "#     eval_steps: int = 200,\n",
    "#     save_steps: int = 500,\n",
    "#     output_dir: str = \"finetuned-weights-LoRA-EVCL-2\",\n",
    "#     prev_fisher_info: dict = None,            \n",
    "#     prev_posterior_means: dict = None,        \n",
    "#     ewc_lambda: float = 0.0,                  \n",
    "#     synthetic_data_loader=None,               # Synthetic data from Task 1\n",
    "#     combined_loader=None,                        # Data loader for Task 2\n",
    "#     eval_loader=None,                         # Evaluation data loader\n",
    "#     tokenizer=None,\n",
    "#     model=None\n",
    "# ):\n",
    "\n",
    "\n",
    "#     def bayesian_guide(input_ids, attention_mask, labels):\n",
    "#         # Define variational distributions over the LoRA parameters\n",
    "#         for name, module in model.named_modules():\n",
    "#             if hasattr(module, 'lora_A'):\n",
    "#                 for key in module.lora_A:\n",
    "#                     param_name = f\"{name}.lora_A.{key}\"\n",
    "#                     lora_A_param = module.lora_A[key].weight\n",
    "#                     device = lora_A_param.device\n",
    "#                     loc = pyro.param(\n",
    "#                         f\"{param_name}_loc\",\n",
    "#                         lora_A_param.clone().to(device)\n",
    "#                     )\n",
    "#                     scale = pyro.param(\n",
    "#                         f\"{param_name}_scale\",\n",
    "#                         (0.1 * torch.ones_like(lora_A_param)).to(device),\n",
    "#                         constraint=dist.constraints.positive\n",
    "#                     )\n",
    "#                     pyro.sample(\n",
    "#                         param_name,\n",
    "#                         dist.Normal(loc, scale).to_event(lora_A_param.dim())\n",
    "#                     )\n",
    "#             if hasattr(module, 'lora_B'):\n",
    "#                 for key in module.lora_B:\n",
    "#                     param_name = f\"{name}.lora_B.{key}\"\n",
    "#                     lora_B_param = module.lora_B[key].weight\n",
    "#                     device = lora_B_param.device\n",
    "#                     loc = pyro.param(\n",
    "#                         f\"{param_name}_loc\",\n",
    "#                         lora_B_param.clone().to(device)\n",
    "#                     )\n",
    "#                     scale = pyro.param(\n",
    "#                         f\"{param_name}_scale\",\n",
    "#                         (0.1 * torch.ones_like(lora_B_param)).to(device),\n",
    "#                         constraint=dist.constraints.positive\n",
    "#                     )\n",
    "#                     pyro.sample(\n",
    "#                         param_name,\n",
    "#                         dist.Normal(loc, scale).to_event(lora_B_param.dim())\n",
    "#                     )\n",
    "                    \n",
    "#     def bayesian_model(input_ids, attention_mask, labels):\n",
    "#         # Define a function to sample and substitute LoRA parameters\n",
    "#         def model_with_sampled_lora():\n",
    "#             # Sample LoRA parameters and set them in the model\n",
    "#             for name, module in model.named_modules():\n",
    "#                 if hasattr(module, 'lora_A'):\n",
    "#                     for key in module.lora_A:\n",
    "#                         param_name = f\"{name}.lora_A.{key}\"\n",
    "#                         lora_A_module = module.lora_A[key]\n",
    "#                         device = lora_A_module.weight.device\n",
    "    \n",
    "#                         # Use posterior mean from Task 1 as prior mean\n",
    "#                         prior_mean = prev_posterior_means.get(param_name, lora_A_module.weight.detach().clone())\n",
    "#                         prior_std = (0.1 * torch.ones_like(lora_A_module.weight)).to(device)\n",
    "    \n",
    "#                         # Sample from the prior\n",
    "#                         sampled_weight = pyro.sample(\n",
    "#                             param_name,\n",
    "#                             dist.Normal(\n",
    "#                                 prior_mean.to(device),\n",
    "#                                 prior_std\n",
    "#                             ).to_event(lora_A_module.weight.dim())\n",
    "#                         )\n",
    "    \n",
    "#                         # Assign the sampled weight to the module\n",
    "#                         with torch.no_grad():\n",
    "#                             lora_A_module.weight.copy_(sampled_weight)\n",
    "    \n",
    "#                 if hasattr(module, 'lora_B'):\n",
    "#                     for key in module.lora_B:\n",
    "#                         param_name = f\"{name}.lora_B.{key}\"\n",
    "#                         lora_B_module = module.lora_B[key]\n",
    "#                         device = lora_B_module.weight.device\n",
    "    \n",
    "#                         # Use posterior mean from Task 1 as prior mean\n",
    "#                         prior_mean = prev_posterior_means.get(param_name, lora_B_module.weight.detach().clone())\n",
    "#                         prior_std = (0.1 * torch.ones_like(lora_B_module.weight)).to(device)\n",
    "    \n",
    "#                         # Sample from the prior\n",
    "#                         sampled_weight = pyro.sample(\n",
    "#                             param_name,\n",
    "#                             dist.Normal(\n",
    "#                                 prior_mean.to(device),\n",
    "#                                 prior_std\n",
    "#                             ).to_event(lora_B_module.weight.dim())\n",
    "#                         )\n",
    "    \n",
    "#                         # Assign the sampled weight to the module\n",
    "#                         with torch.no_grad():\n",
    "#                             lora_B_module.weight.copy_(sampled_weight)\n",
    "    \n",
    "#             # Forward pass\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#             loss = outputs.loss\n",
    "    \n",
    "#             # Add EWC penalty if previous Fisher info and posterior means are provided\n",
    "#             if prev_fisher_info is not None and prev_posterior_means is not None and ewc_lambda > 0.0:\n",
    "#                 ewc_penalty = 0.0\n",
    "#                 for name, param in model.named_parameters():\n",
    "#                     if 'lora' in name and name in prev_fisher_info:\n",
    "#                         fisher = prev_fisher_info[name].to(DEVICE)\n",
    "#                         prev_mean = prev_posterior_means[name].to(DEVICE)\n",
    "#                         ewc_penalty += (fisher * (param - prev_mean) ** 2).sum()\n",
    "#                 loss += ewc_lambda * ewc_penalty\n",
    "    \n",
    "#             return loss\n",
    "    \n",
    "#         # Use the modified model with sampled LoRA parameters\n",
    "#         return model_with_sampled_lora()\n",
    "\n",
    "#     # Set up SVI\n",
    "#     pyro.clear_param_store()\n",
    "#     optim = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "#     elbo = TraceMeanField_ELBO()\n",
    "#     svi = SVI(bayesian_model, bayesian_guide, optim, loss=elbo)\n",
    "    \n",
    "#     # Training loop\n",
    "#     print(\"Training on new task with EWC and synthetic data from previous task...\")\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0.0\n",
    "#         num_batches = 0\n",
    "#         for num_batches, batch in enumerate(combined_loader, 1):\n",
    "#             input_ids = batch['input_ids'].to(DEVICE)\n",
    "#             attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "#             labels = batch['labels'].to(DEVICE)\n",
    "    \n",
    "#             loss = svi.step(input_ids, attention_mask, labels)\n",
    "#             total_loss += loss\n",
    "    \n",
    "#             # Logging\n",
    "#             if num_batches % logging_steps == 0:\n",
    "#                 avg_loss = total_loss / num_batches\n",
    "#                 print(f\"Epoch {epoch + 1}, Step {num_batches}, Loss: {avg_loss}\")\n",
    "    \n",
    "#             # Evaluation\n",
    "#             if num_batches % eval_steps == 0:\n",
    "#                 evaluate_model(model, eval_loader)\n",
    "    \n",
    "#             # Save checkpoints\n",
    "#             if num_batches % save_steps == 0:\n",
    "#                 save_trained_model(model, tokenizer, output_dir)\n",
    "    \n",
    "#         avg_epoch_loss = total_loss / num_batches\n",
    "#         print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_epoch_loss}\")\n",
    "    \n",
    "#     # Save the final trained model after the task\n",
    "#     save_trained_model(model, tokenizer, output_dir)\n",
    "#     pyro.get_param_store().save('pyro_param_store_task2.pt')\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0dcdfc7d-7765-4548-aeef-6c7032d5961c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5969fa606e648d49c3ffa8a68b8bfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Latest_Weights/QA_QG_ Weights')\n",
    "target_file = \"task074_squad1.1_question_generation.json\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "instances = json_data['Instances'][0:2500]\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"output\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"attention_mask\"] = model_inputs.get(\"attention_mask\", None)\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization and set format\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Split dataset into train and eval\n",
    "train_size = int(0.9 * len(tokenized_datasets))\n",
    "train_dataset = tokenized_datasets.select(range(train_size))\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8  \n",
    "train_loader_2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader_2 = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1370b9d7-8f04-4a2c-a48f-f6f0b8500c56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: json_repair in /opt/conda/lib/python3.11/site-packages (0.30.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install json_repair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd52b9-8cc9-4709-aa6d-7342da74ecb5",
   "metadata": {},
   "source": [
    "#### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65bd9af2-9f5c-45b4-b1ba-7e8ea4c2ee50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d0e5308eec4bd680292e326112c0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json_repair \n",
    "os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Synthethic_Data_Generation')\n",
    "target_file = \"qa.train.final_sampled.jsonl\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json_repair.loads(f.read())\n",
    "\n",
    "instances = json_data\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_size = int(1.0 * len(tokenized_datasets))\n",
    "synthetic_train_dataset = tokenized_datasets.select(range(train_size))\n",
    "batch_size = 8  \n",
    "synthetic_loader_1 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98e0db01-be03-419b-abde-11df8e635b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Synthethic_Data_Generation\n",
      "/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8ad22aa-217f-4d30-9ef2-c882368a26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "# Combine datasets\n",
    "if synthetic_loader_1 is not None:\n",
    "    combined_dataset = ConcatDataset([train_loader_2.dataset, synthetic_loader_1.dataset])\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
    "else:\n",
    "    combined_loader = train_loader_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5211d71-2108-4c2e-b790-758068ced9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on new task with EWC and synthetic data from previous task...\n",
      "Epoch 1, Step 100, Loss: 843409.60125\n",
      "Epoch 1, Step 200, Loss: 847681.3821875\n",
      "Evaluation Loss: 14.6276\n",
      "Epoch 1, Step 300, Loss: 849061.0189583333\n",
      "Epoch 1, Step 400, Loss: 849795.40359375\n",
      "Evaluation Loss: 13.9444\n",
      "Epoch 1, Step 500, Loss: 850243.5205\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Task2\n",
      "Epoch 1 completed. Average Loss: 850428.323490231\n",
      "Epoch 2, Step 100, Loss: 851847.599375\n",
      "Epoch 2, Step 200, Loss: 851883.819375\n",
      "Evaluation Loss: 13.2723\n",
      "Epoch 2, Step 300, Loss: 851961.7602083334\n",
      "Epoch 2, Step 400, Loss: 851951.30484375\n",
      "Evaluation Loss: 13.7862\n",
      "Epoch 2, Step 500, Loss: 851994.86275\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Task2\n",
      "Epoch 2 completed. Average Loss: 851996.2115896981\n"
     ]
    }
   ],
   "source": [
    "ewc_lambda = 100.0\n",
    "model_task_2=run_lora_evcl_2(\n",
    "    num_epochs=3,\n",
    "    batch_size=2,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=100,\n",
    "    eval_steps=200,\n",
    "    save_steps=500,\n",
    "    output_dir=\"finetuned-weights-LoRA-EVCL-Task2\",\n",
    "    prev_fisher_info=fisher_info,\n",
    "    prev_posterior_means=prev_posterior_means,\n",
    "    ewc_lambda=ewc_lambda,\n",
    "    synthetic_data_loader=synthetic_loader_1,\n",
    "    combined_loader=combined_loader,\n",
    "    eval_loader=eval_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6457d3f-72ee-4eca-9d75-84b53d953adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89919ef9-216e-44d2-a83a-c2458951d58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844525ac-9677-4cc2-846d-bae8ebb87667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
