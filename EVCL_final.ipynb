{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3df9f10-1efb-4437-b04e-e6c2020be1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import tyxe\n",
    "\n",
    "import random\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO, Trace_ELBO\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, TensorDataset\n",
    "\n",
    "from datasets import load_dataset  # Added to load SuperNI dataset\n",
    "\n",
    "from typing import Optional, List\n",
    "from model.mle_prior import MLEPrior\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaff032f-ae3f-4be6-a016-d5ee7237f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current Device Index: 0\n",
      "Current Device Name: NVIDIA A100-SXM4-80GB\n",
      "Number of GPUs: 1\n",
      "Device 0: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "current_device = torch.cuda.current_device()\n",
    "print(\"Current Device Index:\", current_device)\n",
    "\n",
    "device_name = torch.cuda.get_device_name(current_device)\n",
    "print(\"Current Device Name:\", device_name)\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "for device_id in range(num_gpus):\n",
    "    print(f\"Device {device_id}: {torch.cuda.get_device_name(device_id)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3192f-96f4-44da-9eb4-aea2856aab4f",
   "metadata": {},
   "source": [
    "### Task1 -QA LoRA+EVCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d43227-5365-4537-a585-1f370b504d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_info(\n",
    "    model, \n",
    "    data_loader, \n",
    "    prev_fisher_info=None, \n",
    "    ewc_gamma=1.0, \n",
    "    num_epochs=1, \n",
    "    head_modules=None, \n",
    "    n_samples=None\n",
    "):\n",
    "\n",
    "    fisher = {}\n",
    "    \n",
    "    # Initialize Fisher matrix for LoRA parameters, excluding head modules if provided\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name and (head_modules is None or not any(name.startswith(head) for head in head_modules)):\n",
    "            fisher[name] = torch.zeros_like(param).to(DEVICE)\n",
    "    \n",
    "    # Save the model's current training state and set to eval\n",
    "    old_training_state = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    batch_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            if n_samples is not None and batch_count >= n_samples:\n",
    "                break\n",
    "\n",
    "            print(f\"Processing batch {batch_count + 1}\")\n",
    "            model.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            try:\n",
    "                # with autocast(device_type='cuda'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "            # scaler.scale(loss).backward()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {batch_count + 1}: {e}\")\n",
    "                break\n",
    "\n",
    "            # Accumulate Fisher information for LoRA parameters\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'lora' in name and param.grad is not None and (head_modules is None or not any(name.startswith(head) for head in head_modules)):\n",
    "                    fisher[name] += param.grad.data ** 2\n",
    "\n",
    "            print(f\"Completed batch {batch_count + 1}\")\n",
    "            batch_count += 1\n",
    "\n",
    "    # Normalize Fisher information by the number of processed batches or samples\n",
    "    normalization_factor = batch_count if n_samples is None else min(batch_count, n_samples)\n",
    "    for name in fisher:\n",
    "        fisher[name] = fisher[name] / normalization_factor\n",
    "\n",
    "    # Integrate previous Fisher information with EWC scaling\n",
    "    if prev_fisher_info is not None:\n",
    "        for name in fisher:\n",
    "            if name in prev_fisher_info:\n",
    "                fisher[name] += ewc_gamma * prev_fisher_info[name]\n",
    "\n",
    "    # Restore the model's original training state\n",
    "    model.train(old_training_state)\n",
    "    \n",
    "    return fisher\n",
    "\n",
    "# Function to get variational posterior means\n",
    "def get_variational_posterior_means(model):\n",
    "    posterior_means = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'lora_A'):\n",
    "            # print('yes')\n",
    "            for key in module.lora_A:\n",
    "                param_name = f\"{name}.lora_A.{key}\"\n",
    "                loc_name = f\"{param_name}_loc\"\n",
    "                if loc_name in pyro.get_param_store():\n",
    "                    lora_A_loc = pyro.param(loc_name).detach().clone()\n",
    "                    # Add '.weight' to the parameter name\n",
    "                    posterior_means[f\"{param_name}.weight\"] = lora_A_loc\n",
    "        if hasattr(module, 'lora_B'):\n",
    "            # print('yes')\n",
    "            for key in module.lora_B:\n",
    "                param_name = f\"{name}.lora_B.{key}\"\n",
    "                loc_name = f\"{param_name}_loc\"\n",
    "                if loc_name in pyro.get_param_store():\n",
    "                    lora_B_loc = pyro.param(loc_name).detach().clone()\n",
    "                    # Add '.weight' to the parameter name\n",
    "                    posterior_means[f\"{param_name}.weight\"] = lora_B_loc\n",
    "    return posterior_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4265f3da-cf93-4c14-b872-82c7f2817cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import LoraLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc12ed2-8b42-4e1e-87f5-ed00d1eb2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import init_empty_weights\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "\n",
    "\n",
    "def initialize_lora():\n",
    "    login(\"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\")\n",
    "    # Set environment variable to manage memory fragmentation\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    \n",
    "     \n",
    "    # Specify directories and the path to the zip file\n",
    "    offload_dir = os.path.expanduser(\"llama_offload_evcl/\")\n",
    "     \n",
    "    os.makedirs(offload_dir, exist_ok=True)\n",
    "     \n",
    "    # Extract only the specified JSON file from the zip archive\n",
    "    os.chdir('/home/pranav24/cs-546-project/SSR/Latest_Weights/QA_Weights')\n",
    "    target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "     \n",
    "    # Load tokenizer from Hugging Face\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "    # Load the model with accelerate's offloading and device map auto-setup\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B\",\n",
    "            device_map=\"auto\",\n",
    "            # max_memory=max_memory,\n",
    "            offload_folder=offload_dir,\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True\n",
    "        )\n",
    "     \n",
    "    # Configure LoRA with reduced rank\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    #printing the trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if 'lora' in name:\n",
    "    #         print(name)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62ff472-4fa4-47c2-b2bf-f39fd4fbfbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622c9ef651e0497c96a6ebdbac5d5373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading base model...\")\n",
    "model,tokenizer=initialize_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57385901-11a1-491b-abb3-1b924f4d6cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f2904a2558493aa12c4bcaaa5324e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav24/cs-546-project/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import init_empty_weights\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "os.chdir('/home/pranav24/cs-546-project/SSR/Latest_Weights/QA_Weights')\n",
    "target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "instances = json_data['Instances'][0:2500]\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"output\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"attention_mask\"] = model_inputs.get(\"attention_mask\", None)\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization and set format\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Split dataset into train and eval\n",
    "train_size = int(0.8 * len(tokenized_datasets))\n",
    "train_dataset = tokenized_datasets.select(range(train_size))\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6015f59-7b73-46b6-940a-9f1ec22c3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(model, tokenizer, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93cecb98-c125-4a32-a87f-05e4addea699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            outputs = model(input_ids, labels=labels, attention_mask=attention_mask)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ae06200-7093-45ff-bec3-abc1e93834af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from torch.optim import AdamW\n",
    "import torch.cuda.amp as amp\n",
    "from transformers import get_scheduler\n",
    "from pyro.optim import ExponentialLR\n",
    "evaluation_loss=[]\n",
    "\n",
    "\n",
    "def run_lora_evcl_1(\n",
    "    train_loader,\n",
    "    eval_loader,\n",
    "    num_epochs: int = 100,\n",
    "    model: str = \"meta-llama/Meta-Llama-3-8B\",\n",
    "    batch_size: int = 2,\n",
    "    learning_rate: float = 1e-5,\n",
    "    logging_steps: int = 100,\n",
    "    eval_steps: int = 200,\n",
    "    save_steps: int = 500,\n",
    "    output_dir: str = \"finetuned-weights-LoRA-EVCL\",\n",
    "    load_pyro: bool = False\n",
    "):\n",
    "\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False  # Freeze non-LoRA parameters\n",
    "\n",
    "    def bayesian_guide(input_ids, attention_mask, labels):\n",
    "        # Define variational distributions over the LoRA parameters\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, 'lora_A'):\n",
    "                for key in module.lora_A:\n",
    "                    param_name = f\"{name}.lora_A.{key}\"\n",
    "                    lora_A_param = module.lora_A[key].weight\n",
    "                    device = lora_A_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_A_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_A_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_A_param.dim())\n",
    "                    )\n",
    "            if hasattr(module, 'lora_B'):\n",
    "                for key in module.lora_B:\n",
    "                    param_name = f\"{name}.lora_B.{key}\"\n",
    "                    lora_B_param = module.lora_B[key].weight\n",
    "                    device = lora_B_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_B_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_B_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_B_param.dim())\n",
    "                    )\n",
    "                    \n",
    "    def bayesian_model(input_ids, attention_mask, labels):\n",
    "        # Define a function to sample and substitute LoRA parameters\n",
    "        def model_with_sampled_lora():\n",
    "            # Sample LoRA parameters and set them in the model\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, 'lora_A'):\n",
    "                    for key in module.lora_A:\n",
    "                        param_name = f\"{name}.lora_A.{key}\"\n",
    "                        lora_A_module = module.lora_A[key]\n",
    "                        device = lora_A_module.weight.device\n",
    "    \n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                lora_A_module.weight.detach().to(device),\n",
    "                                (0.1 * torch.ones_like(lora_A_module.weight)).to(device)\n",
    "                            ).to_event(lora_A_module.weight.dim())\n",
    "                        )\n",
    "    \n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_A_module.weight.copy_(sampled_weight)\n",
    "    \n",
    "                if hasattr(module, 'lora_B'):\n",
    "                    for key in module.lora_B:\n",
    "                        param_name = f\"{name}.lora_B.{key}\"\n",
    "                        lora_B_module = module.lora_B[key]\n",
    "                        device = lora_B_module.weight.device\n",
    "    \n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                lora_B_module.weight.detach().to(device),\n",
    "                                (0.1 * torch.ones_like(lora_B_module.weight)).to(device)\n",
    "                            ).to_event(lora_B_module.weight.dim())\n",
    "                        )\n",
    "    \n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_B_module.weight.copy_(sampled_weight)\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            return loss\n",
    "    \n",
    "        # Use the modified model with sampled LoRA parameters\n",
    "        return model_with_sampled_lora()\n",
    "\n",
    "\n",
    "    # Set up SVI\n",
    "    if load_pyro:\n",
    "        pyro.get_param_store().load('pyro_param_store_task1.pt')\n",
    "    else:\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "    optim = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "    optim = pyro.optim.PyroOptim(AdamW, {\"lr\": learning_rate, \"weight_decay\": 1e-5})\n",
    "  \n",
    "    scheduler = ExponentialLR({'optimizer': AdamW, 'optim_args': {'lr': learning_rate}, 'gamma': 0.1})\n",
    "    elbo = TraceMeanField_ELBO()\n",
    "    svi = SVI(bayesian_model, bayesian_guide, scheduler, loss=elbo)\n",
    "    # pt_optim = optim.pt_optim_constructor([p for p in model.parameters() if p.requires_grad])\n",
    "    # scheduler = get_scheduler(\n",
    "    # name=\"linear\",\n",
    "    # optimizer=pt_optim,\n",
    "    # num_warmup_steps=100,\n",
    "    # num_training_steps=len(train_loader) * num_epochs,\n",
    "    # )\n",
    "\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    # Training loop for Task 1\n",
    "    print(f\"Training on Task 1...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for num_batches, batch in enumerate(train_loader, 1):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            loss = svi.step(input_ids, attention_mask, labels)\n",
    "            total_loss += loss\n",
    "\n",
    "            # scaler.scale(loss).backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            # scaler.step(pt_optim)  \n",
    "            # scaler.update()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "            # Logging\n",
    "            if num_batches % logging_steps == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch {epoch}, Step {num_batches}, Loss: {avg_loss}\")\n",
    "\n",
    "            # Evaluation\n",
    "            if num_batches % eval_steps == 0:\n",
    "                eval_loss=evaluate_model(model, eval_loader)\n",
    "                evaluation_loss.append(eval_loss)\n",
    "                \n",
    "\n",
    "            # Save checkpoints\n",
    "            if num_batches % save_steps == 0:\n",
    "                save_trained_model(model, tokenizer, output_dir)\n",
    "\n",
    "        avg_epoch_loss = total_loss / num_batches\n",
    "        print(f\"Task 1 Epoch {epoch} completed. Average Loss: {avg_epoch_loss}\")\n",
    "\n",
    "        if epoch%10 ==0:\n",
    "            save_trained_model(model, tokenizer, output_dir)\n",
    "            pyro.get_param_store().save('pyro_param_store_task1.pt')\n",
    "            \n",
    "        if eval_loss<=3.5:\n",
    "            save_trained_model(model, tokenizer, output_dir)\n",
    "            pyro.get_param_store().save('pyro_param_store_task1.pt')\n",
    "            return model\n",
    "            \n",
    "    \n",
    "    save_trained_model(model, tokenizer, output_dir)\n",
    "    pyro.get_param_store().save('pyro_param_store_task1.pt') \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a88408ab-cd21-43bf-a8bb-6c9f4e8e3b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pranav24/cs-546-project\n",
      "/home/pranav24/cs-546-project\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('/home/pranav24/cs-546-project/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed87466-51c2-43f4-bccc-e0710e0b8fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Task 1...\n",
      "Epoch 0, Step 100, Loss: 1703962.13\n",
      "Epoch 0, Step 200, Loss: 1703930.1125\n",
      "Evaluation Loss: 14.0683\n",
      "Task 1 Epoch 0 completed. Average Loss: 1703947.596\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 1, Step 100, Loss: 1703755.5625\n",
      "Epoch 1, Step 200, Loss: 1703843.083125\n",
      "Evaluation Loss: 14.1231\n",
      "Task 1 Epoch 1 completed. Average Loss: 1703858.146\n",
      "Epoch 2, Step 100, Loss: 1703978.3625\n",
      "Epoch 2, Step 200, Loss: 1704032.42375\n",
      "Evaluation Loss: 12.1895\n",
      "Task 1 Epoch 2 completed. Average Loss: 1703991.258\n",
      "Epoch 3, Step 100, Loss: 1703981.325\n",
      "Epoch 3, Step 200, Loss: 1703828.42\n",
      "Evaluation Loss: 14.6717\n",
      "Task 1 Epoch 3 completed. Average Loss: 1703885.968\n",
      "Epoch 4, Step 100, Loss: 1704147.6575\n",
      "Epoch 4, Step 200, Loss: 1704068.41375\n",
      "Evaluation Loss: 14.3743\n",
      "Task 1 Epoch 4 completed. Average Loss: 1704012.1725\n",
      "Epoch 5, Step 100, Loss: 1703965.28375\n",
      "Epoch 5, Step 200, Loss: 1703895.67625\n",
      "Evaluation Loss: 15.9031\n",
      "Task 1 Epoch 5 completed. Average Loss: 1703914.948\n",
      "Epoch 6, Step 100, Loss: 1704067.2125\n",
      "Epoch 6, Step 200, Loss: 1703995.335\n",
      "Evaluation Loss: 16.0973\n",
      "Task 1 Epoch 6 completed. Average Loss: 1703934.1935\n",
      "Epoch 7, Step 100, Loss: 1703927.28125\n",
      "Epoch 7, Step 200, Loss: 1703909.299375\n",
      "Evaluation Loss: 15.7204\n",
      "Task 1 Epoch 7 completed. Average Loss: 1703891.1115\n",
      "Epoch 8, Step 100, Loss: 1703887.8525\n",
      "Epoch 8, Step 200, Loss: 1703986.865\n",
      "Evaluation Loss: 15.9694\n",
      "Task 1 Epoch 8 completed. Average Loss: 1704034.6145\n",
      "Epoch 9, Step 100, Loss: 1703913.90125\n",
      "Epoch 9, Step 200, Loss: 1703987.6275\n",
      "Evaluation Loss: 15.4856\n",
      "Task 1 Epoch 9 completed. Average Loss: 1704018.67\n",
      "Epoch 10, Step 100, Loss: 1703888.8775\n",
      "Epoch 10, Step 200, Loss: 1703939.455\n",
      "Evaluation Loss: 10.0263\n",
      "Task 1 Epoch 10 completed. Average Loss: 1703946.522\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 11, Step 100, Loss: 1703711.11625\n",
      "Epoch 11, Step 200, Loss: 1703933.91375\n",
      "Evaluation Loss: 13.5000\n",
      "Task 1 Epoch 11 completed. Average Loss: 1703937.668\n",
      "Epoch 12, Step 100, Loss: 1704053.67\n",
      "Epoch 12, Step 200, Loss: 1704037.2425\n",
      "Evaluation Loss: 13.9032\n",
      "Task 1 Epoch 12 completed. Average Loss: 1704016.3185\n",
      "Epoch 13, Step 100, Loss: 1703842.415\n",
      "Epoch 13, Step 200, Loss: 1703840.153125\n",
      "Evaluation Loss: 11.2139\n",
      "Task 1 Epoch 13 completed. Average Loss: 1703877.1015\n",
      "Epoch 14, Step 100, Loss: 1703993.595\n",
      "Epoch 14, Step 200, Loss: 1703898.67875\n",
      "Evaluation Loss: 12.6575\n",
      "Task 1 Epoch 14 completed. Average Loss: 1703902.9295\n",
      "Epoch 15, Step 100, Loss: 1703928.32375\n",
      "Epoch 15, Step 200, Loss: 1703960.124375\n",
      "Evaluation Loss: 13.9196\n",
      "Task 1 Epoch 15 completed. Average Loss: 1703976.0805\n",
      "Epoch 16, Step 100, Loss: 1704086.0425\n",
      "Epoch 16, Step 200, Loss: 1704106.633125\n",
      "Evaluation Loss: 15.7896\n",
      "Task 1 Epoch 16 completed. Average Loss: 1704030.1815\n",
      "Epoch 17, Step 100, Loss: 1703772.00625\n",
      "Epoch 17, Step 200, Loss: 1703754.225\n",
      "Evaluation Loss: 15.6669\n",
      "Task 1 Epoch 17 completed. Average Loss: 1703830.7415\n",
      "Epoch 18, Step 100, Loss: 1704019.265\n",
      "Epoch 18, Step 200, Loss: 1703972.666875\n",
      "Evaluation Loss: 17.0969\n",
      "Task 1 Epoch 18 completed. Average Loss: 1703938.995\n",
      "Epoch 19, Step 100, Loss: 1703918.2475\n",
      "Epoch 19, Step 200, Loss: 1704015.870625\n",
      "Evaluation Loss: 13.4567\n",
      "Task 1 Epoch 19 completed. Average Loss: 1704032.085\n",
      "Epoch 20, Step 100, Loss: 1703797.305\n",
      "Epoch 20, Step 200, Loss: 1703795.56125\n",
      "Evaluation Loss: 11.1904\n",
      "Task 1 Epoch 20 completed. Average Loss: 1703822.237\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 21, Step 100, Loss: 1704064.715\n",
      "Epoch 21, Step 200, Loss: 1703937.24125\n",
      "Evaluation Loss: 12.1977\n",
      "Task 1 Epoch 21 completed. Average Loss: 1703923.7005\n",
      "Epoch 22, Step 100, Loss: 1703880.85\n",
      "Epoch 22, Step 200, Loss: 1703786.45\n",
      "Evaluation Loss: 15.6467\n",
      "Task 1 Epoch 22 completed. Average Loss: 1703863.6415\n",
      "Epoch 23, Step 100, Loss: 1703949.765\n",
      "Epoch 23, Step 200, Loss: 1703985.373125\n",
      "Evaluation Loss: 11.3299\n",
      "Task 1 Epoch 23 completed. Average Loss: 1703965.4085\n",
      "Epoch 24, Step 100, Loss: 1703845.095\n",
      "Epoch 24, Step 200, Loss: 1703956.02875\n",
      "Evaluation Loss: 13.9901\n",
      "Task 1 Epoch 24 completed. Average Loss: 1703941.6805\n",
      "Epoch 25, Step 100, Loss: 1703619.2675\n",
      "Epoch 25, Step 200, Loss: 1703883.8575\n",
      "Evaluation Loss: 14.0786\n",
      "Task 1 Epoch 25 completed. Average Loss: 1703890.305\n",
      "Epoch 26, Step 100, Loss: 1704134.735\n",
      "Epoch 26, Step 200, Loss: 1703995.8\n",
      "Evaluation Loss: 16.7401\n",
      "Task 1 Epoch 26 completed. Average Loss: 1704008.178\n",
      "Epoch 27, Step 100, Loss: 1703927.28375\n",
      "Epoch 27, Step 200, Loss: 1703849.083125\n",
      "Evaluation Loss: 14.7741\n",
      "Task 1 Epoch 27 completed. Average Loss: 1703799.248\n",
      "Epoch 28, Step 100, Loss: 1704067.085\n",
      "Epoch 28, Step 200, Loss: 1704009.403125\n",
      "Evaluation Loss: 13.8663\n",
      "Task 1 Epoch 28 completed. Average Loss: 1704014.089\n",
      "Epoch 29, Step 100, Loss: 1703907.17625\n",
      "Epoch 29, Step 200, Loss: 1703813.445625\n",
      "Evaluation Loss: 13.4670\n",
      "Task 1 Epoch 29 completed. Average Loss: 1703871.9795\n",
      "Epoch 30, Step 100, Loss: 1703691.1425\n",
      "Epoch 30, Step 200, Loss: 1703822.956875\n",
      "Evaluation Loss: 9.5746\n",
      "Task 1 Epoch 30 completed. Average Loss: 1703819.55\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 31, Step 100, Loss: 1703845.92\n",
      "Epoch 31, Step 200, Loss: 1703834.740625\n",
      "Evaluation Loss: 14.5152\n",
      "Task 1 Epoch 31 completed. Average Loss: 1703828.925\n",
      "Epoch 32, Step 100, Loss: 1703767.2\n",
      "Epoch 32, Step 200, Loss: 1703859.323125\n",
      "Evaluation Loss: 14.2623\n",
      "Task 1 Epoch 32 completed. Average Loss: 1703850.113\n",
      "Epoch 33, Step 100, Loss: 1703860.86375\n",
      "Epoch 33, Step 200, Loss: 1703947.55875\n",
      "Evaluation Loss: 14.1724\n",
      "Task 1 Epoch 33 completed. Average Loss: 1703941.135\n",
      "Epoch 34, Step 100, Loss: 1703915.0\n",
      "Epoch 34, Step 200, Loss: 1703841.120625\n",
      "Evaluation Loss: 12.9754\n",
      "Task 1 Epoch 34 completed. Average Loss: 1703828.4505\n",
      "Epoch 35, Step 100, Loss: 1703887.165\n",
      "Epoch 35, Step 200, Loss: 1703918.560625\n",
      "Evaluation Loss: 10.4282\n",
      "Task 1 Epoch 35 completed. Average Loss: 1703897.4515\n",
      "Epoch 36, Step 100, Loss: 1704004.52875\n",
      "Epoch 36, Step 200, Loss: 1703972.821875\n",
      "Evaluation Loss: 13.4509\n",
      "Task 1 Epoch 36 completed. Average Loss: 1703943.196\n",
      "Epoch 37, Step 100, Loss: 1703893.88625\n",
      "Epoch 37, Step 200, Loss: 1703887.8525\n",
      "Evaluation Loss: 11.5545\n",
      "Task 1 Epoch 37 completed. Average Loss: 1703863.89\n",
      "Epoch 38, Step 100, Loss: 1703852.89125\n",
      "Epoch 38, Step 200, Loss: 1703825.543125\n",
      "Evaluation Loss: 14.4884\n",
      "Task 1 Epoch 38 completed. Average Loss: 1703882.573\n",
      "Epoch 39, Step 100, Loss: 1703847.5075\n",
      "Epoch 39, Step 200, Loss: 1703919.65875\n",
      "Evaluation Loss: 12.4354\n",
      "Task 1 Epoch 39 completed. Average Loss: 1703875.218\n",
      "Epoch 40, Step 100, Loss: 1704203.0625\n",
      "Epoch 40, Step 200, Loss: 1704112.490625\n",
      "Evaluation Loss: 16.1117\n",
      "Task 1 Epoch 40 completed. Average Loss: 1704090.7355\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 41, Step 100, Loss: 1704166.42125\n",
      "Epoch 41, Step 200, Loss: 1704124.135625\n",
      "Evaluation Loss: 16.2369\n",
      "Task 1 Epoch 41 completed. Average Loss: 1704102.1805\n",
      "Epoch 42, Step 100, Loss: 1703934.9525\n",
      "Epoch 42, Step 200, Loss: 1703973.94625\n",
      "Evaluation Loss: 14.1630\n",
      "Task 1 Epoch 42 completed. Average Loss: 1703956.674\n",
      "Epoch 43, Step 100, Loss: 1703988.22375\n",
      "Epoch 43, Step 200, Loss: 1703988.451875\n",
      "Evaluation Loss: 12.7035\n",
      "Task 1 Epoch 43 completed. Average Loss: 1703954.807\n",
      "Epoch 44, Step 100, Loss: 1703911.4425\n",
      "Epoch 44, Step 200, Loss: 1703907.32375\n",
      "Evaluation Loss: 15.4845\n",
      "Task 1 Epoch 44 completed. Average Loss: 1703947.0805\n",
      "Epoch 45, Step 100, Loss: 1703990.52625\n",
      "Epoch 45, Step 200, Loss: 1703915.903125\n",
      "Evaluation Loss: 16.2040\n",
      "Task 1 Epoch 45 completed. Average Loss: 1703899.9485\n",
      "Epoch 46, Step 100, Loss: 1704075.01\n",
      "Epoch 46, Step 200, Loss: 1703996.801875\n",
      "Evaluation Loss: 13.5427\n",
      "Task 1 Epoch 46 completed. Average Loss: 1703982.94\n",
      "Epoch 47, Step 100, Loss: 1704006.71625\n",
      "Epoch 47, Step 200, Loss: 1703946.680625\n",
      "Evaluation Loss: 12.9132\n",
      "Task 1 Epoch 47 completed. Average Loss: 1703903.3345\n",
      "Epoch 48, Step 100, Loss: 1704002.56625\n",
      "Epoch 48, Step 200, Loss: 1703932.643125\n",
      "Evaluation Loss: 12.4283\n",
      "Task 1 Epoch 48 completed. Average Loss: 1703911.5495\n",
      "Epoch 49, Step 100, Loss: 1703918.59125\n",
      "Epoch 49, Step 200, Loss: 1703893.14\n",
      "Evaluation Loss: 15.3448\n",
      "Task 1 Epoch 49 completed. Average Loss: 1703894.04\n",
      "Epoch 50, Step 100, Loss: 1703809.0525\n",
      "Epoch 50, Step 200, Loss: 1703949.993125\n",
      "Evaluation Loss: 13.2214\n",
      "Task 1 Epoch 50 completed. Average Loss: 1703945.405\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 51, Step 100, Loss: 1703991.9225\n",
      "Epoch 51, Step 200, Loss: 1703963.0075\n",
      "Evaluation Loss: 15.6848\n",
      "Task 1 Epoch 51 completed. Average Loss: 1703932.0055\n",
      "Epoch 52, Step 100, Loss: 1703871.96875\n",
      "Epoch 52, Step 200, Loss: 1703905.35625\n",
      "Evaluation Loss: 13.2916\n",
      "Task 1 Epoch 52 completed. Average Loss: 1703941.2865\n",
      "Epoch 53, Step 100, Loss: 1703852.29\n",
      "Epoch 53, Step 200, Loss: 1703924.195625\n",
      "Evaluation Loss: 14.7785\n",
      "Task 1 Epoch 53 completed. Average Loss: 1703943.1455\n",
      "Epoch 54, Step 100, Loss: 1703797.37375\n",
      "Epoch 54, Step 200, Loss: 1703786.134375\n",
      "Evaluation Loss: 11.5345\n",
      "Task 1 Epoch 54 completed. Average Loss: 1703788.9255\n",
      "Epoch 55, Step 100, Loss: 1703930.72375\n",
      "Epoch 55, Step 200, Loss: 1703842.493125\n",
      "Evaluation Loss: 9.7123\n",
      "Task 1 Epoch 55 completed. Average Loss: 1703867.255\n",
      "Epoch 56, Step 100, Loss: 1704099.175\n",
      "Epoch 56, Step 200, Loss: 1704045.759375\n",
      "Evaluation Loss: 18.6519\n",
      "Task 1 Epoch 56 completed. Average Loss: 1704056.9735\n",
      "Epoch 57, Step 100, Loss: 1703967.1475\n",
      "Epoch 57, Step 200, Loss: 1704064.8425\n",
      "Evaluation Loss: 16.4277\n",
      "Task 1 Epoch 57 completed. Average Loss: 1704065.394\n",
      "Epoch 58, Step 100, Loss: 1704024.05\n",
      "Epoch 58, Step 200, Loss: 1703986.16125\n",
      "Evaluation Loss: 13.6008\n",
      "Task 1 Epoch 58 completed. Average Loss: 1703979.652\n",
      "Epoch 59, Step 100, Loss: 1704159.61125\n",
      "Epoch 59, Step 200, Loss: 1704056.468125\n",
      "Evaluation Loss: 14.8238\n",
      "Task 1 Epoch 59 completed. Average Loss: 1704071.1175\n",
      "Epoch 60, Step 100, Loss: 1703704.71375\n",
      "Epoch 60, Step 200, Loss: 1703843.743125\n",
      "Evaluation Loss: 19.6668\n",
      "Task 1 Epoch 60 completed. Average Loss: 1703929.211\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 61, Step 100, Loss: 1703704.84375\n",
      "Epoch 61, Step 200, Loss: 1703875.596875\n",
      "Evaluation Loss: 12.6000\n",
      "Task 1 Epoch 61 completed. Average Loss: 1703902.4355\n",
      "Epoch 62, Step 100, Loss: 1703805.09875\n",
      "Epoch 62, Step 200, Loss: 1703954.300625\n",
      "Evaluation Loss: 11.7124\n",
      "Task 1 Epoch 62 completed. Average Loss: 1703993.4615\n",
      "Epoch 63, Step 100, Loss: 1703934.13375\n",
      "Epoch 63, Step 200, Loss: 1703926.12875\n",
      "Evaluation Loss: 14.9063\n",
      "Task 1 Epoch 63 completed. Average Loss: 1703919.7865\n",
      "Epoch 64, Step 100, Loss: 1703941.45\n",
      "Epoch 64, Step 200, Loss: 1703933.449375\n",
      "Evaluation Loss: 12.7553\n",
      "Task 1 Epoch 64 completed. Average Loss: 1703902.427\n",
      "Epoch 65, Step 100, Loss: 1703921.22375\n",
      "Epoch 65, Step 200, Loss: 1703976.258125\n",
      "Evaluation Loss: 16.4484\n",
      "Task 1 Epoch 65 completed. Average Loss: 1703944.6425\n",
      "Epoch 66, Step 100, Loss: 1703853.66125\n",
      "Epoch 66, Step 200, Loss: 1703978.390625\n",
      "Evaluation Loss: 13.9504\n",
      "Task 1 Epoch 66 completed. Average Loss: 1703971.796\n",
      "Epoch 67, Step 100, Loss: 1703843.18\n",
      "Epoch 67, Step 200, Loss: 1703894.478125\n",
      "Evaluation Loss: 13.2224\n",
      "Task 1 Epoch 67 completed. Average Loss: 1703889.851\n",
      "Epoch 68, Step 100, Loss: 1703673.04875\n",
      "Epoch 68, Step 200, Loss: 1703724.46875\n",
      "Evaluation Loss: 11.8724\n",
      "Task 1 Epoch 68 completed. Average Loss: 1703766.1075\n",
      "Epoch 69, Step 100, Loss: 1703964.20625\n",
      "Epoch 69, Step 200, Loss: 1703973.94375\n",
      "Evaluation Loss: 17.7663\n",
      "Task 1 Epoch 69 completed. Average Loss: 1704003.9275\n",
      "Epoch 70, Step 100, Loss: 1703976.5075\n",
      "Epoch 70, Step 200, Loss: 1703899.706875\n",
      "Evaluation Loss: 14.7189\n",
      "Task 1 Epoch 70 completed. Average Loss: 1703904.8435\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 71, Step 100, Loss: 1703957.2275\n",
      "Epoch 71, Step 200, Loss: 1703970.450625\n",
      "Evaluation Loss: 14.0049\n",
      "Task 1 Epoch 71 completed. Average Loss: 1703921.139\n",
      "Epoch 72, Step 100, Loss: 1703886.3175\n",
      "Epoch 72, Step 200, Loss: 1703999.805625\n",
      "Evaluation Loss: 12.6738\n",
      "Task 1 Epoch 72 completed. Average Loss: 1703988.898\n",
      "Epoch 73, Step 100, Loss: 1704090.21875\n",
      "Epoch 73, Step 200, Loss: 1703870.509375\n",
      "Evaluation Loss: 13.5533\n",
      "Task 1 Epoch 73 completed. Average Loss: 1703874.239\n",
      "Epoch 74, Step 100, Loss: 1704004.825\n",
      "Epoch 74, Step 200, Loss: 1703847.8175\n",
      "Evaluation Loss: 12.9010\n",
      "Task 1 Epoch 74 completed. Average Loss: 1703913.948\n",
      "Epoch 75, Step 100, Loss: 1704043.31375\n",
      "Epoch 75, Step 200, Loss: 1703994.74125\n",
      "Evaluation Loss: 12.0562\n",
      "Task 1 Epoch 75 completed. Average Loss: 1704032.7855\n",
      "Epoch 76, Step 100, Loss: 1704144.96625\n",
      "Epoch 76, Step 200, Loss: 1704013.971875\n",
      "Evaluation Loss: 12.3672\n",
      "Task 1 Epoch 76 completed. Average Loss: 1703975.743\n",
      "Epoch 77, Step 100, Loss: 1704197.1\n",
      "Epoch 77, Step 200, Loss: 1704014.301875\n",
      "Evaluation Loss: 15.0706\n",
      "Task 1 Epoch 77 completed. Average Loss: 1704016.038\n",
      "Epoch 78, Step 100, Loss: 1703857.12875\n",
      "Epoch 78, Step 200, Loss: 1703953.171875\n",
      "Evaluation Loss: 13.4054\n",
      "Task 1 Epoch 78 completed. Average Loss: 1703925.895\n",
      "Epoch 79, Step 100, Loss: 1704083.31\n",
      "Epoch 79, Step 200, Loss: 1704011.229375\n",
      "Evaluation Loss: 14.9473\n",
      "Task 1 Epoch 79 completed. Average Loss: 1703968.277\n",
      "Epoch 80, Step 100, Loss: 1704168.8925\n",
      "Epoch 80, Step 200, Loss: 1703944.7675\n",
      "Evaluation Loss: 13.1573\n",
      "Task 1 Epoch 80 completed. Average Loss: 1704016.9085\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 81, Step 100, Loss: 1703974.72875\n",
      "Epoch 81, Step 200, Loss: 1704020.48625\n",
      "Evaluation Loss: 11.7487\n",
      "Task 1 Epoch 81 completed. Average Loss: 1704022.421\n",
      "Epoch 82, Step 100, Loss: 1704130.775\n",
      "Epoch 82, Step 200, Loss: 1703911.77875\n",
      "Evaluation Loss: 12.6033\n",
      "Task 1 Epoch 82 completed. Average Loss: 1703908.0315\n",
      "Epoch 83, Step 100, Loss: 1703886.92875\n",
      "Epoch 83, Step 200, Loss: 1703942.84375\n",
      "Evaluation Loss: 12.0741\n",
      "Task 1 Epoch 83 completed. Average Loss: 1703947.255\n",
      "Epoch 84, Step 100, Loss: 1703828.36625\n",
      "Epoch 84, Step 200, Loss: 1703821.169375\n",
      "Evaluation Loss: 15.2866\n",
      "Task 1 Epoch 84 completed. Average Loss: 1703860.4645\n",
      "Epoch 85, Step 100, Loss: 1703780.35875\n",
      "Epoch 85, Step 200, Loss: 1703842.66375\n",
      "Evaluation Loss: 13.5344\n",
      "Task 1 Epoch 85 completed. Average Loss: 1703853.599\n",
      "Epoch 86, Step 100, Loss: 1704021.63875\n",
      "Epoch 86, Step 200, Loss: 1703950.49625\n",
      "Evaluation Loss: 14.6321\n",
      "Task 1 Epoch 86 completed. Average Loss: 1703945.228\n",
      "Epoch 87, Step 100, Loss: 1703983.0025\n",
      "Epoch 87, Step 200, Loss: 1704006.029375\n",
      "Evaluation Loss: 15.0482\n",
      "Task 1 Epoch 87 completed. Average Loss: 1704013.1605\n",
      "Epoch 88, Step 100, Loss: 1703842.17\n",
      "Epoch 88, Step 200, Loss: 1703896.609375\n",
      "Evaluation Loss: 15.1743\n",
      "Task 1 Epoch 88 completed. Average Loss: 1703894.478\n",
      "Epoch 89, Step 100, Loss: 1703898.5075\n",
      "Epoch 89, Step 200, Loss: 1703887.303125\n",
      "Evaluation Loss: 11.7351\n",
      "Task 1 Epoch 89 completed. Average Loss: 1703912.992\n",
      "Epoch 90, Step 100, Loss: 1703943.20875\n",
      "Epoch 90, Step 200, Loss: 1703893.109375\n",
      "Evaluation Loss: 14.7885\n",
      "Task 1 Epoch 90 completed. Average Loss: 1703890.1715\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 91, Step 100, Loss: 1704085.92\n",
      "Epoch 91, Step 200, Loss: 1703974.081875\n",
      "Evaluation Loss: 16.2176\n",
      "Task 1 Epoch 91 completed. Average Loss: 1703945.3085\n",
      "Epoch 92, Step 100, Loss: 1704126.25\n",
      "Epoch 92, Step 200, Loss: 1703995.041875\n",
      "Evaluation Loss: 12.4507\n",
      "Task 1 Epoch 92 completed. Average Loss: 1703995.398\n",
      "Epoch 93, Step 100, Loss: 1704078.06875\n",
      "Epoch 93, Step 200, Loss: 1704103.67375\n",
      "Evaluation Loss: 13.9287\n",
      "Task 1 Epoch 93 completed. Average Loss: 1704087.3995\n",
      "Epoch 94, Step 100, Loss: 1703858.65875\n",
      "Epoch 94, Step 200, Loss: 1703940.711875\n",
      "Evaluation Loss: 14.2004\n",
      "Task 1 Epoch 94 completed. Average Loss: 1703944.3085\n",
      "Epoch 95, Step 100, Loss: 1703567.8975\n",
      "Epoch 95, Step 200, Loss: 1703788.6675\n",
      "Evaluation Loss: 14.5142\n",
      "Task 1 Epoch 95 completed. Average Loss: 1703820.499\n",
      "Epoch 96, Step 100, Loss: 1703783.0925\n",
      "Epoch 96, Step 200, Loss: 1703823.825\n",
      "Evaluation Loss: 14.0564\n",
      "Task 1 Epoch 96 completed. Average Loss: 1703867.8005\n",
      "Epoch 97, Step 100, Loss: 1703981.16625\n",
      "Epoch 97, Step 200, Loss: 1703952.769375\n",
      "Evaluation Loss: 9.5172\n",
      "Task 1 Epoch 97 completed. Average Loss: 1703950.8275\n",
      "Epoch 98, Step 100, Loss: 1704080.42625\n",
      "Epoch 98, Step 200, Loss: 1704068.76125\n",
      "Evaluation Loss: 13.6264\n",
      "Task 1 Epoch 98 completed. Average Loss: 1704048.1415\n",
      "Epoch 99, Step 100, Loss: 1703828.0925\n",
      "Epoch 99, Step 200, Loss: 1703822.443125\n",
      "Evaluation Loss: 14.2491\n",
      "Task 1 Epoch 99 completed. Average Loss: 1703836.2395\n",
      "Epoch 100, Step 100, Loss: 1703663.5575\n",
      "Epoch 100, Step 200, Loss: 1703715.343125\n",
      "Evaluation Loss: 9.9417\n",
      "Task 1 Epoch 100 completed. Average Loss: 1703672.802\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 101, Step 100, Loss: 1703777.695\n",
      "Epoch 101, Step 200, Loss: 1703805.90125\n",
      "Evaluation Loss: 13.7509\n",
      "Task 1 Epoch 101 completed. Average Loss: 1703701.566\n",
      "Epoch 102, Step 100, Loss: 1703958.41375\n",
      "Epoch 102, Step 200, Loss: 1703964.75875\n",
      "Evaluation Loss: 13.1728\n",
      "Task 1 Epoch 102 completed. Average Loss: 1703969.5105\n",
      "Epoch 103, Step 100, Loss: 1703900.07125\n",
      "Epoch 103, Step 200, Loss: 1703998.546875\n",
      "Evaluation Loss: 14.1512\n",
      "Task 1 Epoch 103 completed. Average Loss: 1704047.838\n",
      "Epoch 104, Step 100, Loss: 1704119.88\n",
      "Epoch 104, Step 200, Loss: 1703883.646875\n",
      "Evaluation Loss: 13.4455\n",
      "Task 1 Epoch 104 completed. Average Loss: 1703803.471\n",
      "Epoch 105, Step 100, Loss: 1703735.73125\n",
      "Epoch 105, Step 200, Loss: 1703791.15125\n",
      "Evaluation Loss: 12.8350\n",
      "Task 1 Epoch 105 completed. Average Loss: 1703849.668\n",
      "Epoch 106, Step 100, Loss: 1703914.49375\n",
      "Epoch 106, Step 200, Loss: 1703962.98125\n",
      "Evaluation Loss: 13.5661\n",
      "Task 1 Epoch 106 completed. Average Loss: 1703910.0175\n",
      "Epoch 107, Step 100, Loss: 1703837.275\n",
      "Epoch 107, Step 200, Loss: 1703972.93375\n",
      "Evaluation Loss: 11.0504\n",
      "Task 1 Epoch 107 completed. Average Loss: 1704051.712\n",
      "Epoch 108, Step 100, Loss: 1703932.965\n",
      "Epoch 108, Step 200, Loss: 1703946.059375\n",
      "Evaluation Loss: 19.2338\n",
      "Task 1 Epoch 108 completed. Average Loss: 1703948.833\n",
      "Epoch 109, Step 100, Loss: 1703923.75125\n",
      "Epoch 109, Step 200, Loss: 1703931.77875\n",
      "Evaluation Loss: 10.5856\n",
      "Task 1 Epoch 109 completed. Average Loss: 1703934.5265\n",
      "Epoch 110, Step 100, Loss: 1704059.35125\n",
      "Epoch 110, Step 200, Loss: 1703932.04125\n",
      "Evaluation Loss: 11.1141\n",
      "Task 1 Epoch 110 completed. Average Loss: 1703938.8655\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 111, Step 100, Loss: 1704073.705\n",
      "Epoch 111, Step 200, Loss: 1704065.58\n",
      "Evaluation Loss: 13.0972\n",
      "Task 1 Epoch 111 completed. Average Loss: 1704043.58\n",
      "Epoch 112, Step 100, Loss: 1703931.24625\n",
      "Epoch 112, Step 200, Loss: 1703908.351875\n",
      "Evaluation Loss: 15.6351\n",
      "Task 1 Epoch 112 completed. Average Loss: 1703929.264\n",
      "Epoch 113, Step 100, Loss: 1703886.60875\n",
      "Epoch 113, Step 200, Loss: 1703893.333125\n",
      "Evaluation Loss: 9.8948\n",
      "Task 1 Epoch 113 completed. Average Loss: 1703897.2865\n",
      "Epoch 114, Step 100, Loss: 1704035.28375\n",
      "Epoch 114, Step 200, Loss: 1703986.085\n",
      "Evaluation Loss: 14.5587\n",
      "Task 1 Epoch 114 completed. Average Loss: 1703986.403\n",
      "Epoch 115, Step 100, Loss: 1704330.7625\n",
      "Epoch 115, Step 200, Loss: 1704106.605625\n",
      "Evaluation Loss: 14.6594\n",
      "Task 1 Epoch 115 completed. Average Loss: 1704088.1575\n",
      "Epoch 116, Step 100, Loss: 1703900.26125\n",
      "Epoch 116, Step 200, Loss: 1703846.8775\n",
      "Evaluation Loss: 15.6909\n",
      "Task 1 Epoch 116 completed. Average Loss: 1703859.7835\n",
      "Epoch 117, Step 100, Loss: 1703914.69375\n",
      "Epoch 117, Step 200, Loss: 1703790.19875\n",
      "Evaluation Loss: 13.7494\n",
      "Task 1 Epoch 117 completed. Average Loss: 1703765.976\n",
      "Epoch 118, Step 100, Loss: 1704462.46\n",
      "Epoch 118, Step 200, Loss: 1704244.581875\n",
      "Evaluation Loss: 17.6109\n",
      "Task 1 Epoch 118 completed. Average Loss: 1704191.941\n",
      "Epoch 119, Step 100, Loss: 1703877.28875\n",
      "Epoch 119, Step 200, Loss: 1703948.301875\n",
      "Evaluation Loss: 11.4245\n",
      "Task 1 Epoch 119 completed. Average Loss: 1703876.3615\n",
      "Epoch 120, Step 100, Loss: 1704039.97125\n",
      "Epoch 120, Step 200, Loss: 1703939.01375\n",
      "Evaluation Loss: 13.5877\n",
      "Task 1 Epoch 120 completed. Average Loss: 1703928.852\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 121, Step 100, Loss: 1704050.15625\n",
      "Epoch 121, Step 200, Loss: 1704054.3325\n",
      "Evaluation Loss: 13.4460\n",
      "Task 1 Epoch 121 completed. Average Loss: 1704034.2785\n",
      "Epoch 122, Step 100, Loss: 1703895.165\n",
      "Epoch 122, Step 200, Loss: 1703858.110625\n",
      "Evaluation Loss: 15.1219\n",
      "Task 1 Epoch 122 completed. Average Loss: 1703923.82\n",
      "Epoch 123, Step 100, Loss: 1703999.02625\n",
      "Epoch 123, Step 200, Loss: 1703895.3675\n",
      "Evaluation Loss: 11.6663\n",
      "Task 1 Epoch 123 completed. Average Loss: 1703919.8755\n",
      "Epoch 124, Step 100, Loss: 1703770.03375\n",
      "Epoch 124, Step 200, Loss: 1703763.933125\n",
      "Evaluation Loss: 11.8374\n",
      "Task 1 Epoch 124 completed. Average Loss: 1703820.5535\n",
      "Epoch 125, Step 100, Loss: 1703649.01875\n",
      "Epoch 125, Step 200, Loss: 1703783.01375\n",
      "Evaluation Loss: 11.6288\n",
      "Task 1 Epoch 125 completed. Average Loss: 1703848.354\n",
      "Epoch 126, Step 100, Loss: 1703783.825\n",
      "Epoch 126, Step 200, Loss: 1703819.958125\n",
      "Evaluation Loss: 13.6733\n",
      "Task 1 Epoch 126 completed. Average Loss: 1703871.1075\n",
      "Epoch 127, Step 100, Loss: 1703895.71\n",
      "Epoch 127, Step 200, Loss: 1703865.365\n",
      "Evaluation Loss: 11.1196\n",
      "Task 1 Epoch 127 completed. Average Loss: 1703839.034\n",
      "Epoch 128, Step 100, Loss: 1704071.4325\n",
      "Epoch 128, Step 200, Loss: 1704135.586875\n",
      "Evaluation Loss: 14.4730\n",
      "Task 1 Epoch 128 completed. Average Loss: 1704065.875\n",
      "Epoch 129, Step 100, Loss: 1703704.0775\n",
      "Epoch 129, Step 200, Loss: 1703821.503125\n",
      "Evaluation Loss: 14.7276\n",
      "Task 1 Epoch 129 completed. Average Loss: 1703788.435\n",
      "Epoch 130, Step 100, Loss: 1704032.95625\n",
      "Epoch 130, Step 200, Loss: 1703928.518125\n",
      "Evaluation Loss: 12.0936\n",
      "Task 1 Epoch 130 completed. Average Loss: 1703957.578\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 131, Step 100, Loss: 1703785.63125\n",
      "Epoch 131, Step 200, Loss: 1703875.995\n",
      "Evaluation Loss: 13.8394\n",
      "Task 1 Epoch 131 completed. Average Loss: 1703921.7125\n",
      "Epoch 132, Step 100, Loss: 1703777.16875\n",
      "Epoch 132, Step 200, Loss: 1703724.2075\n",
      "Evaluation Loss: 14.3539\n",
      "Task 1 Epoch 132 completed. Average Loss: 1703742.1415\n",
      "Epoch 133, Step 100, Loss: 1704098.06\n",
      "Epoch 133, Step 200, Loss: 1703930.21625\n",
      "Evaluation Loss: 9.0795\n",
      "Task 1 Epoch 133 completed. Average Loss: 1703917.899\n",
      "Epoch 134, Step 100, Loss: 1703915.48375\n",
      "Epoch 134, Step 200, Loss: 1703920.980625\n",
      "Evaluation Loss: 10.4606\n",
      "Task 1 Epoch 134 completed. Average Loss: 1703907.1975\n",
      "Epoch 135, Step 100, Loss: 1704057.97375\n",
      "Epoch 135, Step 200, Loss: 1703942.863125\n",
      "Evaluation Loss: 14.8507\n",
      "Task 1 Epoch 135 completed. Average Loss: 1703926.0625\n",
      "Epoch 136, Step 100, Loss: 1703854.01875\n",
      "Epoch 136, Step 200, Loss: 1703819.40875\n",
      "Evaluation Loss: 13.8920\n",
      "Task 1 Epoch 136 completed. Average Loss: 1703826.0735\n",
      "Epoch 137, Step 100, Loss: 1703923.14\n",
      "Epoch 137, Step 200, Loss: 1703936.436875\n",
      "Evaluation Loss: 13.4750\n",
      "Task 1 Epoch 137 completed. Average Loss: 1703888.385\n",
      "Epoch 138, Step 100, Loss: 1703775.2625\n",
      "Epoch 138, Step 200, Loss: 1703840.45875\n",
      "Evaluation Loss: 14.5660\n",
      "Task 1 Epoch 138 completed. Average Loss: 1703832.8465\n",
      "Epoch 139, Step 100, Loss: 1703820.27125\n",
      "Epoch 139, Step 200, Loss: 1703890.251875\n",
      "Evaluation Loss: 18.5676\n",
      "Task 1 Epoch 139 completed. Average Loss: 1703893.5915\n",
      "Epoch 140, Step 100, Loss: 1704093.3775\n",
      "Epoch 140, Step 200, Loss: 1703953.999375\n",
      "Evaluation Loss: 13.9957\n",
      "Task 1 Epoch 140 completed. Average Loss: 1703921.6815\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Final-Task1\n",
      "Epoch 141, Step 100, Loss: 1703917.31625\n",
      "Epoch 141, Step 200, Loss: 1704009.0925\n",
      "Evaluation Loss: 16.8502\n",
      "Task 1 Epoch 141 completed. Average Loss: 1704043.1495\n",
      "Epoch 142, Step 100, Loss: 1703612.2025\n",
      "Epoch 142, Step 200, Loss: 1703926.880625\n",
      "Evaluation Loss: 18.7494\n",
      "Task 1 Epoch 142 completed. Average Loss: 1703943.714\n",
      "Epoch 143, Step 100, Loss: 1703886.9725\n",
      "Epoch 143, Step 200, Loss: 1703921.045625\n",
      "Evaluation Loss: 12.7043\n",
      "Task 1 Epoch 143 completed. Average Loss: 1703944.5085\n",
      "Epoch 144, Step 100, Loss: 1703788.685\n",
      "Epoch 144, Step 200, Loss: 1703863.014375\n",
      "Evaluation Loss: 11.6327\n",
      "Task 1 Epoch 144 completed. Average Loss: 1703908.277\n",
      "Epoch 145, Step 100, Loss: 1703788.415\n",
      "Epoch 145, Step 200, Loss: 1703803.354375\n",
      "Evaluation Loss: 12.0547\n",
      "Task 1 Epoch 145 completed. Average Loss: 1703893.5555\n",
      "Epoch 146, Step 100, Loss: 1703929.7275\n",
      "Epoch 146, Step 200, Loss: 1704011.035625\n",
      "Evaluation Loss: 14.3346\n",
      "Task 1 Epoch 146 completed. Average Loss: 1704025.5385\n",
      "Epoch 147, Step 100, Loss: 1703979.79\n",
      "Epoch 147, Step 200, Loss: 1703947.098125\n",
      "Evaluation Loss: 12.3405\n",
      "Task 1 Epoch 147 completed. Average Loss: 1703859.0315\n",
      "Epoch 148, Step 100, Loss: 1703867.2025\n",
      "Epoch 148, Step 200, Loss: 1703976.16125\n",
      "Evaluation Loss: 14.2611\n",
      "Task 1 Epoch 148 completed. Average Loss: 1703978.4605\n",
      "Epoch 149, Step 100, Loss: 1703742.3425\n",
      "Epoch 149, Step 200, Loss: 1703829.496875\n",
      "Evaluation Loss: 14.4877\n",
      "Task 1 Epoch 149 completed. Average Loss: 1703893.9295\n",
      "Epoch 150, Step 100, Loss: 1703836.27875\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     model=run_lora_evcl_1(\n",
    "#         train_loader=train_loader,\n",
    "#         eval_loader=eval_loader,\n",
    "#         num_epochs=200,\n",
    "#         model=model,\n",
    "#         batch_size=8,\n",
    "#         learning_rate=2e-4,\n",
    "#         logging_steps=100,\n",
    "#         eval_steps=200,\n",
    "#         save_steps=500,\n",
    "#         output_dir=\"finetuned-weights-LoRA-EVCL-Task1\",\n",
    "#     )\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model=run_lora_evcl_1(\n",
    "        train_loader=train_loader,\n",
    "        eval_loader=eval_loader,\n",
    "        num_epochs=185,\n",
    "        model=model,\n",
    "        batch_size=8,\n",
    "        learning_rate=1e-5,\n",
    "        # learning_rate=2e-4,\n",
    "        logging_steps=100,\n",
    "        eval_steps=200,\n",
    "        save_steps=500,\n",
    "        output_dir=\"finetuned-weights-LoRA-EVCL-Final-Task1\",\n",
    "        load_pyro=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64aee245-7e13-4698-8b75-7d72f09421c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456b0e18377e4256a2953950a1e3284c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\",\n",
    "            device_map=\"auto\",\n",
    "            offload_folder='/home/pranav24/cs-546-project/llama_offload_evcl',\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "lora_model_path = \"/home/pranav24/cs-546-project/finetuned-weights-LoRA-EVCL-Final-Task1\"\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a70a2-af6d-4078-98bf-473da225ec18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a69c3-3a02-4521-a882-a55b1fcca627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "prev_fisher_info = None\n",
    "prev_params = None\n",
    "ewc_gamma = 1.0  \n",
    "\n",
    "fisher_info = compute_fisher_info(\n",
    "    model=model,\n",
    "    data_loader=train_loader,\n",
    "    prev_fisher_info=prev_fisher_info,\n",
    "    ewc_gamma=ewc_gamma,\n",
    "    num_epochs=1,  \n",
    "    head_modules=None,  \n",
    "    n_samples=None  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7e157fa-3d0a-4f37-a264-8c6ada0f30aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.0005135599058121443\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 0.0010042006615549326\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.029310958459973335\n",
      "Layer: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.07284019887447357\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.00012028848868794739\n",
      "Layer: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.44399270741269e-05\n",
      "Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0160883367061615\n",
      "Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.07432784140110016\n",
      "Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 0.00014660028682556003\n",
      "Layer: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.724917976593133e-05\n",
      "Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.01853732392191887\n",
      "Layer: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.027849949896335602\n",
      "Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 6.263380055315793e-05\n",
      "Layer: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 6.651174771832302e-05\n",
      "Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0032166452147066593\n",
      "Layer: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.02938860096037388\n",
      "Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 6.102528004703345e-06\n",
      "Layer: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 4.9254340410698205e-06\n",
      "Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.007425458170473576\n",
      "Layer: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.007124226540327072\n",
      "Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.1080824151576962e-05\n",
      "Layer: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 9.383667929796502e-06\n",
      "Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00208555837161839\n",
      "Layer: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.018010050058364868\n",
      "Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.381498117960291e-06\n",
      "Layer: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.8912448871997185e-06\n",
      "Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0034343204461038113\n",
      "Layer: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.007514249533414841\n",
      "Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.3519834940088913e-05\n",
      "Layer: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 9.293102266383357e-06\n",
      "Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0011356242466717958\n",
      "Layer: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.009284822270274162\n",
      "Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.762624485010747e-06\n",
      "Layer: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 8.811045518086758e-06\n",
      "Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0010721504222601652\n",
      "Layer: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.005346088670194149\n",
      "Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.1054527223896002e-06\n",
      "Layer: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 7.679725968046114e-06\n",
      "Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.002220537979155779\n",
      "Layer: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.005603664554655552\n",
      "Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.111691850994248e-06\n",
      "Layer: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.4383977941179182e-05\n",
      "Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0019892742857337\n",
      "Layer: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.004897716920822859\n",
      "Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.2640726936297142e-06\n",
      "Layer: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 5.0797430048987735e-06\n",
      "Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.001047244993969798\n",
      "Layer: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.004034414421766996\n",
      "Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.3561094621982193e-06\n",
      "Layer: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.514296056688181e-06\n",
      "Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00047469406854361296\n",
      "Layer: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.004460825119167566\n",
      "Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 9.425190000911243e-06\n",
      "Layer: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.5808325264952146e-06\n",
      "Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0010196338407695293\n",
      "Layer: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.005350570194423199\n",
      "Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.789291895373026e-06\n",
      "Layer: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 8.83506629634212e-07\n",
      "Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0011822390370070934\n",
      "Layer: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.005570509470999241\n",
      "Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.169946810681722e-06\n",
      "Layer: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 7.385384606095613e-07\n",
      "Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0010968780843541026\n",
      "Layer: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0025079380720853806\n",
      "Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 4.907239599560853e-06\n",
      "Layer: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.715106575124082e-06\n",
      "Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0007925725658424199\n",
      "Layer: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.007638889830559492\n",
      "Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.9054822359976242e-06\n",
      "Layer: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.621270484974957e-06\n",
      "Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0005891437176615\n",
      "Layer: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0031002433970570564\n",
      "Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.618912671754515e-07\n",
      "Layer: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.7630733282203437e-06\n",
      "Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.002080557169392705\n",
      "Layer: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0024559572339057922\n",
      "Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.3366947086979053e-06\n",
      "Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.705000497371657e-06\n",
      "Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0013511718716472387\n",
      "Layer: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0040166983380913734\n",
      "Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 9.67640971794026e-07\n",
      "Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.3631024557980709e-06\n",
      "Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0007465659291483462\n",
      "Layer: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.006023302674293518\n",
      "Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.729576126763277e-07\n",
      "Layer: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.705068311570358e-07\n",
      "Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0012518700677901506\n",
      "Layer: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0031773275695741177\n",
      "Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.9731429524181294e-07\n",
      "Layer: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.0596622814773582e-06\n",
      "Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0013591318856924772\n",
      "Layer: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.002549517899751663\n",
      "Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 7.208799956970324e-07\n",
      "Layer: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 5.329301302481326e-07\n",
      "Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0006280883681029081\n",
      "Layer: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.002614453434944153\n",
      "Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 6.808315902162576e-07\n",
      "Layer: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.498294406905188e-07\n",
      "Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0004691544163506478\n",
      "Layer: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0018144214991480112\n",
      "Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.131150384026114e-06\n",
      "Layer: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 9.289548756896693e-07\n",
      "Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.000891244737431407\n",
      "Layer: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0050424253568053246\n",
      "Layer: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.1073492462164722e-06\n",
      "Layer: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.163129011023557e-06\n",
      "Layer: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00037781090941280127\n",
      "Layer: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.002978071803227067\n",
      "Layer: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 3.840800218313234e-06\n",
      "Layer: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.5072357604803983e-06\n",
      "Layer: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0007891942514106631\n",
      "Layer: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.002095266943797469\n",
      "Layer: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 5.903692681386019e-07\n",
      "Layer: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 1.1036710247935844e-06\n",
      "Layer: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0005908538587391376\n",
      "Layer: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0025230818428099155\n",
      "Layer: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 1.476827605983999e-06\n",
      "Layer: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 2.676709755178308e-06\n",
      "Layer: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.000971137429587543\n",
      "Layer: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0068094367161393166\n",
      "Layer: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 6.30104750598548e-07\n",
      "Layer: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 4.097591954632662e-07\n",
      "Layer: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.0018235808238387108\n",
      "Layer: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.004527417477220297\n",
      "Layer: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight, Fisher Info Mean: 2.529903611048212e-07\n",
      "Layer: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight, Fisher Info Mean: 3.493404108212417e-07\n",
      "Layer: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight, Fisher Info Mean: 0.00044795440044254065\n",
      "Layer: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight, Fisher Info Mean: 0.0023827587720006704\n"
     ]
    }
   ],
   "source": [
    "for name, fisher_matrix in fisher_info.items():\n",
    "    print(f\"Layer: {name}, Fisher Info Mean: {fisher_matrix.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d1b4cfa-ef40-47e7-9f88-1940c51e0c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "prev_posterior_means = get_variational_posterior_means(model)\n",
    "torch.save(prev_posterior_means, f'posterior_means_task_{1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf1d6a1f-9a1c-4f64-923b-3a02a008e407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_posterior_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234530c-a2c0-4698-b7f2-8a9e390585de",
   "metadata": {},
   "source": [
    "### Task 2: QA+QG EVCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e0bd22e-e909-48d8-ac98-23e524827601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "import torch\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO\n",
    "\n",
    "def run_lora_evcl_2(\n",
    "    num_epochs: int = 3,\n",
    "    batch_size: int = 2,\n",
    "    learning_rate: float = 1e-5,\n",
    "    logging_steps: int = 100,\n",
    "    eval_steps: int = 200,\n",
    "    save_steps: int = 500,\n",
    "    output_dir: str = \"finetuned-weights-LoRA-EVCL-2\",\n",
    "    prev_fisher_info: dict = None,            \n",
    "    prev_posterior_means: dict = None,        \n",
    "    ewc_lambda: float = 0.0,                  \n",
    "    synthetic_data_loader=None,               # Synthetic data from Task 1\n",
    "    combined_loader=None,                     # Data loader for Task 2\n",
    "    eval_loader=None,                         # Evaluation data loader\n",
    "    tokenizer=None,\n",
    "    model=None\n",
    "):\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Ensure all parameters require gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False  # Freeze non-LoRA parameters\n",
    "\n",
    "    def bayesian_guide(input_ids, attention_mask, labels):\n",
    "        # Define variational distributions over the LoRA parameters\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, 'lora_A'):\n",
    "                for key in module.lora_A:\n",
    "                    param_name = f\"{name}.lora_A.{key}\"\n",
    "                    lora_A_param = module.lora_A[key].weight\n",
    "                    device = lora_A_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_A_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_A_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_A_param.dim())\n",
    "                    )\n",
    "            if hasattr(module, 'lora_B'):\n",
    "                for key in module.lora_B:\n",
    "                    param_name = f\"{name}.lora_B.{key}\"\n",
    "                    lora_B_param = module.lora_B[key].weight\n",
    "                    device = lora_B_param.device\n",
    "\n",
    "                    # Ensure initial values are leaf tensors with requires_grad=True\n",
    "                    loc_init = lora_B_param.detach().clone().to(device).requires_grad_()\n",
    "                    scale_init = (0.1 * torch.ones_like(lora_B_param)).to(device).requires_grad_()\n",
    "\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        loc_init\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        scale_init,\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_B_param.dim())\n",
    "                    )\n",
    "                        \n",
    "    def bayesian_model(input_ids, attention_mask, labels):\n",
    "        # pyro.module(\"model\", model)  # Removed\n",
    "\n",
    "        # Define a function to sample and substitute LoRA parameters\n",
    "        def model_with_sampled_lora():\n",
    "            # Sample LoRA parameters and set them in the model\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, 'lora_A'):\n",
    "                    for key in module.lora_A:\n",
    "                        param_name = f\"{name}.lora_A.{key}\"\n",
    "                        lora_A_module = module.lora_A[key]\n",
    "                        device = lora_A_module.weight.device\n",
    "\n",
    "                        # Use posterior mean from Task 1 as prior mean\n",
    "                        prior_mean = prev_posterior_means.get(param_name, lora_A_module.weight.detach().clone()).to(device)\n",
    "                        prior_std = (0.1 * torch.ones_like(lora_A_module.weight)).to(device)\n",
    "\n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                prior_mean,\n",
    "                                prior_std\n",
    "                            ).to_event(lora_A_module.weight.dim())\n",
    "                        )\n",
    "\n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_A_module.weight.copy_(sampled_weight)\n",
    "\n",
    "                if hasattr(module, 'lora_B'):\n",
    "                    for key in module.lora_B:\n",
    "                        param_name = f\"{name}.lora_B.{key}\"\n",
    "                        lora_B_module = module.lora_B[key]\n",
    "                        device = lora_B_module.weight.device\n",
    "\n",
    "                        # Use posterior mean from Task 1 as prior mean\n",
    "                        prior_mean = prev_posterior_means.get(param_name, lora_B_module.weight.detach().clone()).to(device)\n",
    "                        prior_std = (0.1 * torch.ones_like(lora_B_module.weight)).to(device)\n",
    "\n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                prior_mean,\n",
    "                                prior_std\n",
    "                            ).to_event(lora_B_module.weight.dim())\n",
    "                        )\n",
    "\n",
    "                        # Assign the sampled weight to the module\n",
    "                        with torch.no_grad():\n",
    "                            lora_B_module.weight.copy_(sampled_weight)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Add EWC penalty if previous Fisher info and posterior means are provided\n",
    "            if prev_fisher_info is not None and prev_posterior_means is not None and ewc_lambda > 0.0:\n",
    "                ewc_penalty = 0.0\n",
    "                for name, param in model.named_parameters():\n",
    "                    if 'lora' in name and name in prev_fisher_info:\n",
    "                        fisher = prev_fisher_info[name].to(DEVICE)\n",
    "                        prev_mean = prev_posterior_means[name].to(DEVICE)\n",
    "                        ewc_penalty += (fisher * (param - prev_mean) ** 2).sum()\n",
    "                loss += ewc_lambda * ewc_penalty\n",
    "\n",
    "            return loss\n",
    "\n",
    "        # Use the modified model with sampled LoRA parameters\n",
    "        return model_with_sampled_lora()\n",
    "\n",
    "    # Set up SVI\n",
    "    pyro.clear_param_store()\n",
    "    optim = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "    elbo = TraceMeanField_ELBO()\n",
    "    svi = SVI(bayesian_model, bayesian_guide, optim, loss=elbo)\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training on new task with EWC and synthetic data from previous task...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for num_batches, batch in enumerate(combined_loader, 1):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            loss = svi.step(input_ids, attention_mask, labels)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Logging\n",
    "            if num_batches % logging_steps == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch {epoch + 1}, Step {num_batches}, Loss: {avg_loss}\")\n",
    "\n",
    "            # Evaluation\n",
    "            if num_batches % eval_steps == 0:\n",
    "                evaluate_model(model, eval_loader)\n",
    "\n",
    "            # Save checkpoints\n",
    "            if num_batches % save_steps == 0:\n",
    "                save_trained_model(model, tokenizer, output_dir)\n",
    "\n",
    "        avg_epoch_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    # Save the final trained model after the task\n",
    "    save_trained_model(model, tokenizer, output_dir)\n",
    "    pyro.get_param_store().save('pyro_param_store_task2.pt')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dcdfc7d-7765-4548-aeef-6c7032d5961c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1bc610257e4318b23428dab1820dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Latest_Weights/QA_QG_ Weights')\n",
    "target_file = \"task074_squad1.1_question_generation.json\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "instances = json_data['Instances'][0:2500]\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"output\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"attention_mask\"] = model_inputs.get(\"attention_mask\", None)\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization and set format\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Split dataset into train and eval\n",
    "train_size = int(0.9 * len(tokenized_datasets))\n",
    "train_dataset = tokenized_datasets.select(range(train_size))\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8  \n",
    "train_loader_2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader_2 = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1370b9d7-8f04-4a2c-a48f-f6f0b8500c56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: json_repair in /opt/conda/lib/python3.11/site-packages (0.30.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install json_repair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd52b9-8cc9-4709-aa6d-7342da74ecb5",
   "metadata": {},
   "source": [
    "#### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65bd9af2-9f5c-45b4-b1ba-7e8ea4c2ee50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9f622ae83946cbbb15db925d58838c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json_repair \n",
    "os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Synthethic_Data_Generation')\n",
    "target_file = \"qa.train.final_sampled.jsonl\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json_repair.loads(f.read())\n",
    "\n",
    "instances = json_data\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_size = int(1.0 * len(tokenized_datasets))\n",
    "synthetic_train_dataset = tokenized_datasets.select(range(train_size))\n",
    "batch_size = 8  \n",
    "synthetic_loader_1 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98e0db01-be03-419b-abde-11df8e635b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Synthethic_Data_Generation\n",
      "/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8ad22aa-217f-4d30-9ef2-c882368a26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "# Combine datasets\n",
    "if synthetic_loader_1 is not None:\n",
    "    combined_dataset = ConcatDataset([train_loader_2.dataset, synthetic_loader_1.dataset])\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
    "else:\n",
    "    combined_loader = train_loader_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5211d71-2108-4c2e-b790-758068ced9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on new task with EWC and synthetic data from previous task...\n",
      "Epoch 1, Step 100, Loss: 843486.794375\n",
      "Epoch 1, Step 200, Loss: 847730.5909375\n",
      "Evaluation Loss: 12.8290\n",
      "Epoch 1, Step 300, Loss: 849076.033125\n",
      "Epoch 1, Step 400, Loss: 849807.50875\n",
      "Evaluation Loss: 15.4223\n",
      "Epoch 1, Step 500, Loss: 850279.091625\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Task2\n",
      "Epoch 1 completed. Average Loss: 850449.5449600355\n",
      "Epoch 2, Step 100, Loss: 851994.78625\n",
      "Epoch 2, Step 200, Loss: 852064.1734375\n",
      "Evaluation Loss: 10.8797\n",
      "Epoch 2, Step 300, Loss: 852052.5872916667\n",
      "Epoch 2, Step 400, Loss: 852069.47703125\n",
      "Evaluation Loss: 14.5681\n",
      "Epoch 2, Step 500, Loss: 852065.829125\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Task2\n",
      "Epoch 2 completed. Average Loss: 852064.1171181173\n",
      "Epoch 3, Step 100, Loss: 851905.533125\n",
      "Epoch 3, Step 200, Loss: 851910.8978125\n",
      "Evaluation Loss: 15.9732\n",
      "Epoch 3, Step 300, Loss: 851894.6825\n",
      "Epoch 3, Step 400, Loss: 851918.4740625\n",
      "Evaluation Loss: 15.9056\n",
      "Epoch 3, Step 500, Loss: 851909.012125\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Task2\n",
      "Epoch 3 completed. Average Loss: 851946.198490231\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL-Task2\n"
     ]
    }
   ],
   "source": [
    "ewc_lambda = 100.0\n",
    "model_task_2=run_lora_evcl_2(\n",
    "    num_epochs=3,\n",
    "    batch_size=2,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=100,\n",
    "    eval_steps=200,\n",
    "    save_steps=500,\n",
    "    output_dir=\"finetuned-weights-LoRA-EVCL-Task2\",\n",
    "    prev_fisher_info=fisher_info,\n",
    "    prev_posterior_means=prev_posterior_means,\n",
    "    ewc_lambda=ewc_lambda,\n",
    "    synthetic_data_loader=synthetic_loader_1,\n",
    "    combined_loader=combined_loader,\n",
    "    eval_loader=eval_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6457d3f-72ee-4eca-9d75-84b53d953adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89919ef9-216e-44d2-a83a-c2458951d58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844525ac-9677-4cc2-846d-bae8ebb87667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_546)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
