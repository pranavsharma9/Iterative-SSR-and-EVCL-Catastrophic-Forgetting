{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3df9f10-1efb-4437-b04e-e6c2020be1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import tyxe\n",
    "\n",
    "import random\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO, Trace_ELBO\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, TensorDataset\n",
    "\n",
    "from datasets import load_dataset  # Added to load SuperNI dataset\n",
    "\n",
    "from typing import Optional, List\n",
    "from model.mle_prior import MLEPrior\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaff032f-ae3f-4be6-a016-d5ee7237f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current Device Index: 0\n",
      "Current Device Name: NVIDIA A100-SXM4-80GB\n",
      "Number of GPUs: 1\n",
      "Device 0: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get the current device index\n",
    "current_device = torch.cuda.current_device()\n",
    "print(\"Current Device Index:\", current_device)\n",
    "\n",
    "# Get the name of the current device\n",
    "device_name = torch.cuda.get_device_name(current_device)\n",
    "print(\"Current Device Name:\", device_name)\n",
    "\n",
    "# Get the number of GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "# List all GPUs\n",
    "for device_id in range(num_gpus):\n",
    "    print(f\"Device {device_id}: {torch.cuda.get_device_name(device_id)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80eb69d9-1465-4f9c-baee-5b562fdd840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_nlp_datasets(tokenizer, batch_size, num_tasks, start_task=1):\n",
    "#     train_loaders = []\n",
    "#     test_loaders = []\n",
    "\n",
    "#     # Load the SuperNI dataset\n",
    "#     # You can specify the split and tasks you need\n",
    "#     superni_dataset = load_dataset('super_glue', 'ni')  # Adjust if necessary\n",
    "\n",
    "#     # Assuming tasks are numbered starting from 1\n",
    "#     for task_index in range(start_task, num_tasks + 1):\n",
    "#         if task_index == 1:\n",
    "#             # Load QA task from SuperNI\n",
    "#             train_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qa', split='train')\n",
    "#             test_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qa', split='validation')\n",
    "#         elif task_index == 2:\n",
    "#             # Load QG task from SuperNI\n",
    "#             train_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qg', split='train')\n",
    "#             test_dataset = load_superni_task_dataset(superni_dataset, tokenizer, task_type='qg', split='validation')\n",
    "#         else:\n",
    "#             # Load additional tasks if needed\n",
    "#             pass\n",
    "\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#         test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         train_loaders.append(train_loader)\n",
    "#         test_loaders.append(test_loader)\n",
    "\n",
    "#     return train_loaders, test_loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211d73a0-e0a1-4260-9876-5c755ea94ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# def load_superni_task_dataset(tokenizer, task_type='qa', split='train'):\n",
    "#     # Filter the dataset for the specific task type\n",
    "#     # SuperNI tasks are identified by their task names or IDs\n",
    "#     # For example, you can filter tasks that contain 'question answering' or 'question generation'\n",
    "\n",
    "#     # Example of filtering:\n",
    "#     if task_type == 'qa':\n",
    "#         path='/home/pranav24/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/QA_FineTuned'\n",
    "#         os.chdir(path)\n",
    "#         target_file = r\"task024_cosmosqa_answer_generation.json\"\n",
    "#         with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "#             json_data = json.load(f)\n",
    "\n",
    "#         dataset = json_data['Instances'][0:2223]\n",
    "        \n",
    "#         # task_filter = lambda ex: 'question answering' in ex['Task']\n",
    "#     elif task_type == 'qg':\n",
    "#         # task_filter = lambda ex: 'question generation' in ex['Task']\n",
    "#         path='/home/pranav24/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/QG_FineTuned/QG_FineTuned'\n",
    "#         os.chdir(path)\n",
    "#         target_file = r\"task074_squad1.1_question_generation.json\"\n",
    "#         with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "#             json_data = json.load(f)\n",
    "\n",
    "#         dataset = json_data['Instances'][0:2223]\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "\n",
    "\n",
    "#     def preprocess_function(examples):\n",
    "#         # For SuperNI, inputs and outputs are in 'Input' and 'Output' fields\n",
    "#         inputs = examples['Input']\n",
    "#         targets = examples['Output']\n",
    "    \n",
    "#         # Tokenize inputs and targets\n",
    "#         model_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=512)\n",
    "#         with tokenizer.as_target_tokenizer():\n",
    "#             labels = tokenizer(targets, truncation=True, padding='max_length', max_length=512)\n",
    "    \n",
    "#         model_inputs['labels'] = labels['input_ids']\n",
    "#         return model_inputs\n",
    "    \n",
    "#     dataset = dataset.map(preprocess_function, batched=True)\n",
    "#     dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "#     return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a9ca12-dd23-43c9-9b4c-fe2d6798a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_loader_for_task1(tokenizer, batch_size):\n",
    "#     # Load the SuperNI dataset\n",
    "\n",
    "#     # Load QA task\n",
    "#     train_dataset = load_superni_task_dataset(tokenizer, task_type='qa', split='train')\n",
    "#     test_dataset = load_superni_task_dataset(tokenizer, task_type='qa', split='validation')\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3192f-96f4-44da-9eb4-aea2856aab4f",
   "metadata": {},
   "source": [
    "### Task1 -QA LoRA+EVCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "006361fb-8a46-4d83-9eab-78ca4ee72eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrainingConfig:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         output_dir,\n",
    "#         num_train_epochs,\n",
    "#         per_device_train_batch_size,\n",
    "#         gradient_accumulation_steps,\n",
    "#         learning_rate,\n",
    "#         logging_steps,\n",
    "#         eval_steps,\n",
    "#         save_steps,\n",
    "#         save_total_limit,\n",
    "#         fp16,\n",
    "#     ):\n",
    "#         self.output_dir = output_dir\n",
    "#         self.num_train_epochs = num_train_epochs\n",
    "#         self.per_device_train_batch_size = per_device_train_batch_size\n",
    "#         self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.logging_steps = logging_steps\n",
    "#         self.eval_steps = eval_steps\n",
    "#         self.save_steps = save_steps\n",
    "#         self.save_total_limit = save_total_limit\n",
    "#         self.fp16 = fp16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18d43227-5365-4537-a585-1f370b504d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_info(\n",
    "    model, \n",
    "    data_loader, \n",
    "    prev_fisher_info=None, \n",
    "    ewc_gamma=1.0, \n",
    "    num_epochs=1, \n",
    "    head_modules=None, \n",
    "    n_samples=None\n",
    "):\n",
    "\n",
    "    fisher = {}\n",
    "    \n",
    "    # Initialize Fisher matrix for LoRA parameters, excluding head modules if provided\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name and (head_modules is None or not any(name.startswith(head) for head in head_modules)):\n",
    "            fisher[name] = torch.zeros_like(param).to(DEVICE)\n",
    "    \n",
    "    # Save the model's current training state and set to eval\n",
    "    old_training_state = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    batch_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            if n_samples is not None and batch_count >= n_samples:\n",
    "                break\n",
    "\n",
    "            print(f\"Processing batch {batch_count + 1}\")\n",
    "            model.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            try:\n",
    "                # with autocast(device_type='cuda'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "            # scaler.scale(loss).backward()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {batch_count + 1}: {e}\")\n",
    "                break\n",
    "\n",
    "            # Accumulate Fisher information for LoRA parameters\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'lora' in name and param.grad is not None and (head_modules is None or not any(name.startswith(head) for head in head_modules)):\n",
    "                    fisher[name] += param.grad.data ** 2\n",
    "\n",
    "            print(f\"Completed batch {batch_count + 1}\")\n",
    "            batch_count += 1\n",
    "\n",
    "    # Normalize Fisher information by the number of processed batches or samples\n",
    "    normalization_factor = batch_count if n_samples is None else min(batch_count, n_samples)\n",
    "    for name in fisher:\n",
    "        fisher[name] = fisher[name] / normalization_factor\n",
    "\n",
    "    # Integrate previous Fisher information with EWC scaling\n",
    "    if prev_fisher_info is not None:\n",
    "        for name in fisher:\n",
    "            if name in prev_fisher_info:\n",
    "                fisher[name] += ewc_gamma * prev_fisher_info[name]\n",
    "\n",
    "    # Restore the model's original training state\n",
    "    model.train(old_training_state)\n",
    "    \n",
    "    return fisher\n",
    "\n",
    "# Function to get variational posterior means\n",
    "def get_variational_posterior_means():\n",
    "    posterior_means = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'lora_A'):\n",
    "            lora_A_loc = pyro.param(f\"{name}.lora_A_loc\").detach().clone()\n",
    "            posterior_means[f\"{name}.lora_A\"] = lora_A_loc\n",
    "        if hasattr(module, 'lora_B'):\n",
    "            lora_B_loc = pyro.param(f\"{name}.lora_B_loc\").detach().clone()\n",
    "            posterior_means[f\"{name}.lora_B\"] = lora_B_loc\n",
    "    return posterior_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4265f3da-cf93-4c14-b872-82c7f2817cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import LoraLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc12ed2-8b42-4e1e-87f5-ed00d1eb2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import init_empty_weights\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "\n",
    "\n",
    "def initialize_lora():\n",
    "    login(\"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\")\n",
    "    # Set environment variable to manage memory fragmentation\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    \n",
    "     \n",
    "    # Specify directories and the path to the zip file\n",
    "    offload_dir = os.path.expanduser(\"llama_offload_evcl/\")\n",
    "     \n",
    "    os.makedirs(offload_dir, exist_ok=True)\n",
    "     \n",
    "    # Extract only the specified JSON file from the zip archive\n",
    "    os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Latest_Weights/QA_Weights')\n",
    "    target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "     \n",
    "    # Load tokenizer from Hugging Face\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "    # Load the model with accelerate's offloading and device map auto-setup\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B\",\n",
    "            device_map=\"auto\",\n",
    "            # max_memory=max_memory,\n",
    "            offload_folder=offload_dir,\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True\n",
    "        )\n",
    "     \n",
    "    # Configure LoRA with reduced rank\n",
    "    lora_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    #printing the trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if 'lora' in name:\n",
    "    #         print(name)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62ff472-4fa4-47c2-b2bf-f39fd4fbfbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b46f39987a4449b1edbb2b5cb586dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,703,936 || all params: 8,031,965,184 || trainable%: 0.0212\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading base model...\")\n",
    "model,tokenizer=initialize_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57385901-11a1-491b-abb3-1b924f4d6cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558f2a548b2e4c3caaa524944dce2063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/SSR/Latest_Weights/QA_Weights')\n",
    "target_file = \"task024_cosmosqa_answer_generation.json\"\n",
    "\n",
    "with open(target_file, 'r', encoding='utf-8-sig') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "instances = json_data['Instances'][0:2223]\n",
    "input_texts = [str(instance['input']) for instance in instances]\n",
    "output_texts = [str(instance['output'][0]) if instance['output'] else \"\" for instance in instances]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "ds = Dataset.from_dict({'input': input_texts, 'output': output_texts})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"output\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"attention_mask\"] = model_inputs.get(\"attention_mask\", None)\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization and set format\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Split dataset into train and eval\n",
    "train_size = int(0.9 * len(tokenized_datasets))\n",
    "train_dataset = tokenized_datasets.select(range(train_size))\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6015f59-7b73-46b6-940a-9f1ec22c3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(model, tokenizer, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93cecb98-c125-4a32-a87f-05e4addea699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            outputs = model(input_ids, labels=labels, attention_mask=attention_mask)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Evaluation Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ae06200-7093-45ff-bec3-abc1e93834af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "def run_lora_evcl_1(\n",
    "    num_epochs: int = 3,\n",
    "    base_model_name: str = \"meta-llama/Meta-Llama-3-8B\",\n",
    "    batch_size: int = 2,\n",
    "    learning_rate: float = 1e-5,\n",
    "    logging_steps: int = 100,\n",
    "    eval_steps: int = 200,\n",
    "    save_steps: int = 500,\n",
    "    output_dir: str = \"finetuned-weights-LoRA-EVCL\",\n",
    "):\n",
    "\n",
    "\n",
    "    def bayesian_guide(input_ids, attention_mask, labels):\n",
    "        # Define variational distributions over the LoRA parameters\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, 'lora_A'):\n",
    "                for key in module.lora_A:\n",
    "                    param_name = f\"{name}.lora_A.{key}\"\n",
    "                    lora_A_param = module.lora_A[key].weight\n",
    "                    device = lora_A_param.device\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        lora_A_param.clone().detach().to(device)\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        (0.1 * torch.ones_like(lora_A_param)).to(device),\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_A_param.dim())\n",
    "                    )\n",
    "            if hasattr(module, 'lora_B'):\n",
    "                for key in module.lora_B:\n",
    "                    param_name = f\"{name}.lora_B.{key}\"\n",
    "                    lora_B_param = module.lora_B[key].weight\n",
    "                    device = lora_B_param.device\n",
    "                    loc = pyro.param(\n",
    "                        f\"{param_name}_loc\",\n",
    "                        lora_B_param.clone().detach().to(device)\n",
    "                    )\n",
    "                    scale = pyro.param(\n",
    "                        f\"{param_name}_scale\",\n",
    "                        (0.1 * torch.ones_like(lora_B_param)).to(device),\n",
    "                        constraint=dist.constraints.positive\n",
    "                    )\n",
    "                    pyro.sample(\n",
    "                        param_name,\n",
    "                        dist.Normal(loc, scale).to_event(lora_B_param.dim())\n",
    "                    )\n",
    "                    \n",
    "    def bayesian_model(input_ids, attention_mask, labels):\n",
    "        # Define a function to sample and substitute LoRA parameters\n",
    "        def model_with_sampled_lora():\n",
    "            # Sample LoRA parameters and set them in the model\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, 'lora_A'):\n",
    "                    for key in module.lora_A:\n",
    "                        param_name = f\"{name}.lora_A.{key}\"\n",
    "                        lora_A_module = module.lora_A[key]\n",
    "                        device = lora_A_module.weight.device\n",
    "    \n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                lora_A_module.weight.detach().to(device),\n",
    "                                (0.1 * torch.ones_like(lora_A_module.weight)).to(device)\n",
    "                            ).to_event(lora_A_module.weight.dim())\n",
    "                        )\n",
    "    \n",
    "                        # Assign the sampled weight to the module\n",
    "                        # lora_A_module.weight = torch.nn.Parameter(sampled_weight)\n",
    "                        lora_A_module.weight.data.copy_(sampled_weight)\n",
    "    \n",
    "                if hasattr(module, 'lora_B'):\n",
    "                    for key in module.lora_B:\n",
    "                        param_name = f\"{name}.lora_B.{key}\"\n",
    "                        lora_B_module = module.lora_B[key]\n",
    "                        device = lora_B_module.weight.device\n",
    "    \n",
    "                        # Sample from the prior\n",
    "                        sampled_weight = pyro.sample(\n",
    "                            param_name,\n",
    "                            dist.Normal(\n",
    "                                lora_B_module.weight.detach().to(device),\n",
    "                                (0.1 * torch.ones_like(lora_B_module.weight)).to(device)\n",
    "                            ).to_event(lora_B_module.weight.dim())\n",
    "                        )\n",
    "    \n",
    "                        # Assign the sampled weight to the module\n",
    "                        # lora_B_module.weight = torch.nn.Parameter(sampled_weight)\n",
    "                        lora_B_module.weight.data.copy_(sampled_weight)\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            return loss\n",
    "    \n",
    "        # Use the modified model with sampled LoRA parameters\n",
    "        return model_with_sampled_lora()\n",
    "\n",
    "\n",
    "    # Set up SVI\n",
    "    pyro.clear_param_store()\n",
    "    optim = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "    elbo = TraceMeanField_ELBO()\n",
    "    svi = SVI(bayesian_model, bayesian_guide, optim, loss=elbo)\n",
    "\n",
    "    \n",
    "    # Training loop for Task 1\n",
    "    print(f\"Training on Task 1...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for num_batches, batch in enumerate(train_loader, 1):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            loss = svi.step(input_ids, attention_mask, labels)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Logging\n",
    "            if num_batches % logging_steps == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch {epoch}, Step {num_batches}, Loss: {avg_loss}\")\n",
    "\n",
    "            # Evaluation\n",
    "            if num_batches % eval_steps == 0:\n",
    "                evaluate_model(model, eval_loader)\n",
    "\n",
    "            # Save checkpoints\n",
    "            if num_batches % save_steps == 0:\n",
    "                save_trained_model(model, tokenizer, output_dir)\n",
    "\n",
    "        avg_epoch_loss = total_loss / num_batches\n",
    "        print(f\"Task 1 Epoch {epoch} completed. Average Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    # Save the final trained model after Task 1\n",
    "    save_trained_model(model, tokenizer, output_dir)\n",
    "    return model\n",
    "\n",
    "    # After Task 1, compute FIM and save posterior means\n",
    "    # fisher_info = compute_fisher_info(model, train_loader)\n",
    "    # prev_posterior_means = get_variational_posterior_means()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a88408ab-cd21-43bf-a8bb-6c9f4e8e3b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting\n",
      "/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ed87466-51c2-43f4-bccc-e0710e0b8fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Task 1...\n",
      "Epoch 0, Step 100, Loss: 852121.69\n",
      "Epoch 0, Step 200, Loss: 852002.860625\n",
      "Evaluation Loss: 9.9737\n",
      "Task 1 Epoch 0 completed. Average Loss: 852012.5255\n",
      "Epoch 1, Step 100, Loss: 851845.203125\n",
      "Epoch 1, Step 200, Loss: 851937.7140625\n",
      "Evaluation Loss: 15.9449\n",
      "Task 1 Epoch 1 completed. Average Loss: 851954.0025\n",
      "Epoch 2, Step 100, Loss: 851915.589375\n",
      "Epoch 2, Step 200, Loss: 851983.643125\n",
      "Evaluation Loss: 14.9926\n",
      "Task 1 Epoch 2 completed. Average Loss: 851933.47325\n",
      "Model and tokenizer saved to finetuned-weights-LoRA-EVCL\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model=run_lora_evcl_1(\n",
    "        num_epochs=3,\n",
    "        base_model_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "        batch_size=2,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=100,\n",
    "        eval_steps=200,\n",
    "        save_steps=500,\n",
    "        output_dir=\"finetuned-weights-LoRA-EVCL\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64aee245-7e13-4698-8b75-7d72f09421c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c447e6245fbd4b27b02fab42c9105c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\",\n",
    "            device_map=\"auto\",\n",
    "            offload_folder='/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/llama_offload_evcl',\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "lora_model_path = \"/home/kakde2/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/finetuned-weights-LoRA-EVCL\"\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "602a70a2-af6d-4078-98bf-473da225ec18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Verify that LoRA parameters require gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a69c3-3a02-4521-a882-a55b1fcca627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/1\n",
      "Processing batch 1\n",
      "Completed batch 1\n",
      "Processing batch 2\n",
      "Completed batch 2\n",
      "Processing batch 3\n",
      "Completed batch 3\n",
      "Processing batch 4\n",
      "Completed batch 4\n",
      "Processing batch 5\n",
      "Completed batch 5\n",
      "Processing batch 6\n",
      "Completed batch 6\n",
      "Processing batch 7\n",
      "Completed batch 7\n",
      "Processing batch 8\n",
      "Completed batch 8\n",
      "Processing batch 9\n",
      "Completed batch 9\n",
      "Processing batch 10\n",
      "Completed batch 10\n",
      "Processing batch 11\n",
      "Completed batch 11\n",
      "Processing batch 12\n",
      "Completed batch 12\n",
      "Processing batch 13\n",
      "Completed batch 13\n",
      "Processing batch 14\n",
      "Completed batch 14\n",
      "Processing batch 15\n",
      "Completed batch 15\n",
      "Processing batch 16\n",
      "Completed batch 16\n",
      "Processing batch 17\n",
      "Completed batch 17\n",
      "Processing batch 18\n",
      "Completed batch 18\n",
      "Processing batch 19\n",
      "Completed batch 19\n",
      "Processing batch 20\n",
      "Completed batch 20\n",
      "Processing batch 21\n",
      "Completed batch 21\n",
      "Processing batch 22\n",
      "Completed batch 22\n",
      "Processing batch 23\n",
      "Completed batch 23\n",
      "Processing batch 24\n",
      "Completed batch 24\n",
      "Processing batch 25\n",
      "Completed batch 25\n",
      "Processing batch 26\n",
      "Completed batch 26\n",
      "Processing batch 27\n",
      "Completed batch 27\n",
      "Processing batch 28\n",
      "Completed batch 28\n",
      "Processing batch 29\n",
      "Completed batch 29\n",
      "Processing batch 30\n",
      "Completed batch 30\n",
      "Processing batch 31\n",
      "Completed batch 31\n",
      "Processing batch 32\n",
      "Completed batch 32\n",
      "Processing batch 33\n",
      "Completed batch 33\n",
      "Processing batch 34\n",
      "Completed batch 34\n",
      "Processing batch 35\n",
      "Completed batch 35\n",
      "Processing batch 36\n",
      "Completed batch 36\n",
      "Processing batch 37\n",
      "Completed batch 37\n",
      "Processing batch 38\n",
      "Completed batch 38\n",
      "Processing batch 39\n",
      "Completed batch 39\n",
      "Processing batch 40\n",
      "Completed batch 40\n",
      "Processing batch 41\n",
      "Completed batch 41\n",
      "Processing batch 42\n",
      "Completed batch 42\n",
      "Processing batch 43\n",
      "Completed batch 43\n",
      "Processing batch 44\n",
      "Completed batch 44\n",
      "Processing batch 45\n",
      "Completed batch 45\n",
      "Processing batch 46\n",
      "Completed batch 46\n",
      "Processing batch 47\n",
      "Completed batch 47\n",
      "Processing batch 48\n",
      "Completed batch 48\n",
      "Processing batch 49\n",
      "Completed batch 49\n",
      "Processing batch 50\n",
      "Completed batch 50\n",
      "Processing batch 51\n",
      "Completed batch 51\n",
      "Processing batch 52\n",
      "Completed batch 52\n",
      "Processing batch 53\n",
      "Completed batch 53\n",
      "Processing batch 54\n",
      "Completed batch 54\n",
      "Processing batch 55\n",
      "Completed batch 55\n",
      "Processing batch 56\n",
      "Completed batch 56\n",
      "Processing batch 57\n",
      "Completed batch 57\n",
      "Processing batch 58\n",
      "Completed batch 58\n",
      "Processing batch 59\n",
      "Completed batch 59\n",
      "Processing batch 60\n",
      "Completed batch 60\n",
      "Processing batch 61\n",
      "Completed batch 61\n",
      "Processing batch 62\n",
      "Completed batch 62\n",
      "Processing batch 63\n",
      "Completed batch 63\n",
      "Processing batch 64\n",
      "Completed batch 64\n",
      "Processing batch 65\n",
      "Completed batch 65\n",
      "Processing batch 66\n",
      "Completed batch 66\n",
      "Processing batch 67\n",
      "Completed batch 67\n",
      "Processing batch 68\n",
      "Completed batch 68\n",
      "Processing batch 69\n",
      "Completed batch 69\n",
      "Processing batch 70\n",
      "Completed batch 70\n",
      "Processing batch 71\n",
      "Completed batch 71\n",
      "Processing batch 72\n",
      "Completed batch 72\n",
      "Processing batch 73\n",
      "Completed batch 73\n",
      "Processing batch 74\n",
      "Completed batch 74\n",
      "Processing batch 75\n",
      "Completed batch 75\n",
      "Processing batch 76\n",
      "Completed batch 76\n",
      "Processing batch 77\n",
      "Completed batch 77\n",
      "Processing batch 78\n",
      "Completed batch 78\n",
      "Processing batch 79\n",
      "Completed batch 79\n",
      "Processing batch 80\n",
      "Completed batch 80\n",
      "Processing batch 81\n",
      "Completed batch 81\n",
      "Processing batch 82\n",
      "Completed batch 82\n",
      "Processing batch 83\n",
      "Completed batch 83\n",
      "Processing batch 84\n",
      "Completed batch 84\n",
      "Processing batch 85\n",
      "Completed batch 85\n",
      "Processing batch 86\n",
      "Completed batch 86\n",
      "Processing batch 87\n",
      "Completed batch 87\n",
      "Processing batch 88\n",
      "Completed batch 88\n",
      "Processing batch 89\n",
      "Completed batch 89\n",
      "Processing batch 90\n",
      "Completed batch 90\n",
      "Processing batch 91\n",
      "Completed batch 91\n",
      "Processing batch 92\n",
      "Completed batch 92\n",
      "Processing batch 93\n",
      "Completed batch 93\n",
      "Processing batch 94\n",
      "Completed batch 94\n",
      "Processing batch 95\n",
      "Completed batch 95\n",
      "Processing batch 96\n",
      "Completed batch 96\n",
      "Processing batch 97\n",
      "Completed batch 97\n",
      "Processing batch 98\n",
      "Completed batch 98\n",
      "Processing batch 99\n",
      "Completed batch 99\n",
      "Processing batch 100\n",
      "Completed batch 100\n",
      "Processing batch 101\n",
      "Completed batch 101\n",
      "Processing batch 102\n",
      "Completed batch 102\n",
      "Processing batch 103\n",
      "Completed batch 103\n",
      "Processing batch 104\n",
      "Completed batch 104\n",
      "Processing batch 105\n",
      "Completed batch 105\n",
      "Processing batch 106\n",
      "Completed batch 106\n",
      "Processing batch 107\n",
      "Completed batch 107\n",
      "Processing batch 108\n",
      "Completed batch 108\n",
      "Processing batch 109\n",
      "Completed batch 109\n",
      "Processing batch 110\n",
      "Completed batch 110\n",
      "Processing batch 111\n",
      "Completed batch 111\n",
      "Processing batch 112\n",
      "Completed batch 112\n",
      "Processing batch 113\n",
      "Completed batch 113\n",
      "Processing batch 114\n",
      "Completed batch 114\n",
      "Processing batch 115\n",
      "Completed batch 115\n",
      "Processing batch 116\n",
      "Completed batch 116\n",
      "Processing batch 117\n",
      "Completed batch 117\n",
      "Processing batch 118\n",
      "Completed batch 118\n",
      "Processing batch 119\n",
      "Completed batch 119\n",
      "Processing batch 120\n",
      "Completed batch 120\n",
      "Processing batch 121\n",
      "Completed batch 121\n",
      "Processing batch 122\n",
      "Completed batch 122\n",
      "Processing batch 123\n",
      "Completed batch 123\n",
      "Processing batch 124\n",
      "Completed batch 124\n",
      "Processing batch 125\n",
      "Completed batch 125\n",
      "Processing batch 126\n",
      "Completed batch 126\n",
      "Processing batch 127\n",
      "Completed batch 127\n",
      "Processing batch 128\n",
      "Completed batch 128\n",
      "Processing batch 129\n",
      "Completed batch 129\n",
      "Processing batch 130\n",
      "Completed batch 130\n",
      "Processing batch 131\n",
      "Completed batch 131\n",
      "Processing batch 132\n",
      "Completed batch 132\n",
      "Processing batch 133\n",
      "Completed batch 133\n",
      "Processing batch 134\n",
      "Completed batch 134\n",
      "Processing batch 135\n",
      "Completed batch 135\n",
      "Processing batch 136\n",
      "Completed batch 136\n",
      "Processing batch 137\n",
      "Completed batch 137\n",
      "Processing batch 138\n",
      "Completed batch 138\n",
      "Processing batch 139\n",
      "Completed batch 139\n",
      "Processing batch 140\n",
      "Completed batch 140\n",
      "Processing batch 141\n",
      "Completed batch 141\n",
      "Processing batch 142\n",
      "Completed batch 142\n",
      "Processing batch 143\n",
      "Completed batch 143\n",
      "Processing batch 144\n",
      "Completed batch 144\n",
      "Processing batch 145\n",
      "Completed batch 145\n",
      "Processing batch 146\n",
      "Completed batch 146\n",
      "Processing batch 147\n",
      "Completed batch 147\n",
      "Processing batch 148\n",
      "Completed batch 148\n",
      "Processing batch 149\n",
      "Completed batch 149\n",
      "Processing batch 150\n",
      "Completed batch 150\n",
      "Processing batch 151\n",
      "Completed batch 151\n",
      "Processing batch 152\n",
      "Completed batch 152\n",
      "Processing batch 153\n",
      "Completed batch 153\n",
      "Processing batch 154\n",
      "Completed batch 154\n",
      "Processing batch 155\n",
      "Completed batch 155\n",
      "Processing batch 156\n",
      "Completed batch 156\n",
      "Processing batch 157\n",
      "Completed batch 157\n",
      "Processing batch 158\n",
      "Completed batch 158\n",
      "Processing batch 159\n",
      "Completed batch 159\n",
      "Processing batch 160\n",
      "Completed batch 160\n",
      "Processing batch 161\n",
      "Completed batch 161\n",
      "Processing batch 162\n",
      "Completed batch 162\n",
      "Processing batch 163\n",
      "Completed batch 163\n",
      "Processing batch 164\n",
      "Completed batch 164\n",
      "Processing batch 165\n",
      "Completed batch 165\n",
      "Processing batch 166\n",
      "Completed batch 166\n",
      "Processing batch 167\n",
      "Completed batch 167\n",
      "Processing batch 168\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "prev_fisher_info = None\n",
    "prev_params = None\n",
    "ewc_gamma = 1.0  \n",
    "\n",
    "fisher_info = compute_fisher_info(\n",
    "    model=model,\n",
    "    data_loader=train_loader,\n",
    "    prev_fisher_info=prev_fisher_info,\n",
    "    ewc_gamma=ewc_gamma,\n",
    "    num_epochs=1,  \n",
    "    head_modules=None,  \n",
    "    n_samples=None  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e157fa-3d0a-4f37-a264-8c6ada0f30aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b4cfa-ef40-47e7-9f88-1940c51e0c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d6a1f-9a1c-4f64-923b-3a02a008e407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd1fe1-6e3a-4ae9-86e0-94d150a5e0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0bd22e-e909-48d8-ac98-23e524827601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcdfc7d-7765-4548-aeef-6c7032d5961c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd9af2-9f5c-45b4-b1ba-7e8ea4c2ee50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0db01-be03-419b-abde-11df8e635b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad22aa-217f-4d30-9ef2-c882368a26f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5211d71-2108-4c2e-b790-758068ced9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6457d3f-72ee-4eca-9d75-84b53d953adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89919ef9-216e-44d2-a83a-c2458951d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_variational_approx(bnn, train_loader, curr_coreset, num_epochs, callback, ewc_lambda, fisher_info=None, prev_params=None):\n",
    "#     if curr_coreset:\n",
    "#         # Create a dataset from the coreset\n",
    "#         print('Coreset is true')\n",
    "#         coreset_input_ids = torch.stack([item[0] for item in curr_coreset])\n",
    "#         coreset_labels = torch.stack([item[1] for item in curr_coreset])\n",
    "#         coreset_dataset = TensorDataset(coreset_input_ids, coreset_labels)\n",
    "\n",
    "#         # Combine coreset and current task data\n",
    "#         combined_dataset = ConcatDataset([train_loader.dataset, coreset_dataset])\n",
    "#         data_loader = DataLoader(combined_dataset, batch_size=train_loader.batch_size, shuffle=True)\n",
    "#     else:\n",
    "#         print('coreset not true')\n",
    "#         data_loader = train_loader\n",
    "\n",
    "#     optim = pyro.optim.Adam({\"lr\": 1e-5})  # Adjust learning rate as needed\n",
    "\n",
    "#     with tyxe.poutine.local_reparameterization():\n",
    "#         bnn.fit(data_loader, optim, num_epochs, device=DEVICE, callback=callback, ewc_lambda=ewc_lambda, fisher_info=fisher_info, prev_params=prev_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c77ef6a0-a2a7-4c38-be77-fab57110e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer\n",
    "\n",
    "# class EWCTrainer(Trainer):\n",
    "#     def __init__(self, ewc_lambda, fisher_info, prev_params, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.ewc_lambda = ewc_lambda\n",
    "#         self.fisher_info = fisher_info\n",
    "#         self.prev_params = prev_params\n",
    "\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         # Standard loss\n",
    "#         outputs = model(**inputs)\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         # EWC loss\n",
    "#         if self.fisher_info is not None and self.prev_params is not None:\n",
    "#             ewc_loss = 0\n",
    "#             for name, param in model.named_parameters():\n",
    "#                 if 'lora' in name and name in self.fisher_info:\n",
    "#                     ewc_loss += (self.fisher_info[name] * (param - self.prev_params[name]) ** 2).sum()\n",
    "#             ewc_loss = 0.5 * self.ewc_lambda * ewc_loss\n",
    "#             loss += ewc_loss\n",
    "\n",
    "#         return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c480446-d4b9-4423-8728-7654876ee118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyro.nn as pynn\n",
    "# import torch.nn as nn\n",
    "# class BayesianLoRAModule(pynn.PyroModule):\n",
    "#     def __init__(self, model):\n",
    "#         super().__init__()\n",
    "#         self.model = model  # The base model remains unchanged\n",
    "\n",
    "#         # Replace LoRA parameters with PyroSample\n",
    "#         for name, module in self.model.named_modules():\n",
    "#             if isinstance(module, nn.Linear) and hasattr(module, 'lora_A'):\n",
    "#                 # Replace lora_A and lora_B with PyroSample\n",
    "#                 lora_A_name = f\"{name}.lora_A\"\n",
    "#                 lora_B_name = f\"{name}.lora_B\"\n",
    "\n",
    "#                 # Get the existing parameters\n",
    "#                 lora_A = getattr(module, 'lora_A')\n",
    "#                 lora_B = getattr(module, 'lora_B')\n",
    "\n",
    "#                 # Register PyroSample parameters\n",
    "#                 setattr(module, 'lora_A', pynn.PyroSample(dist.Normal(lora_A.data, 1.0).to_event(2)))\n",
    "#                 setattr(module, 'lora_B', pynn.PyroSample(dist.Normal(lora_B.data, 1.0).to_event(2)))\n",
    "\n",
    "#     def forward(self, *args, **kwargs):\n",
    "#         return self.model(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61acd4b7-fd02-40b3-8136-fd713a901ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bayesian_model_and_guide(model):\n",
    "#     bayesian_model = BayesianLoRAModule(model)\n",
    "\n",
    "#     # Define the guide\n",
    "#     def guide(*args, **kwargs):\n",
    "#         for name, module in bayesian_model.named_modules():\n",
    "#             if isinstance(module, nn.Linear) and hasattr(module, 'lora_A'):\n",
    "#                 # Define variational distributions for lora_A and lora_B\n",
    "#                 lora_A_loc = pyro.param(f\"{name}.lora_A_loc\", torch.zeros_like(module.lora_A.data))\n",
    "#                 lora_A_scale = pyro.param(f\"{name}.lora_A_scale\", torch.ones_like(module.lora_A.data), constraint=pyro.distributions.constraints.positive)\n",
    "#                 lora_B_loc = pyro.param(f\"{name}.lora_B_loc\", torch.zeros_like(module.lora_B.data))\n",
    "#                 lora_B_scale = pyro.param(f\"{name}.lora_B_scale\", torch.ones_like(module.lora_B.data), constraint=pyro.distributions.constraints.positive)\n",
    "#                 pyro.sample(f\"{name}.lora_A\", dist.Normal(lora_A_loc, lora_A_scale).to_event(2))\n",
    "#                 pyro.sample(f\"{name}.lora_B\", dist.Normal(lora_B_loc, lora_B_scale).to_event(2))\n",
    "#     return bayesian_model, guide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0159a88-7c03-406a-a7b9-c308562314ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get the current device index\n",
    "current_device = torch.cuda.current_device()\n",
    "print(\"Current Device Index:\", current_device)\n",
    "\n",
    "# Get the name of the current device\n",
    "device_name = torch.cuda.get_device_name(current_device)\n",
    "print(\"Current Device Name:\", device_name)\n",
    "\n",
    "# Get the number of GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "# List all GPUs\n",
    "for device_id in range(num_gpus):\n",
    "    print(f\"Device {device_id}: {torch.cuda.get_device_name(device_id)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed938bb-c6ce-4049-9431-77e6b5e338f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7904e-a05c-4272-8bcc-74bcc2c82097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2375fcc-a362-4f1a-8156-4bdb9e424063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed5fe3-5ade-44f9-b08a-197b881d4894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e713f74-5c6a-415a-a725-29d8a167ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evcl(\n",
    "    num_tasks: int = 2,  # Assuming tasks 1 (QA) and 2 (QG)\n",
    "    num_epochs: int = 3,\n",
    "    experiment_name: str = 'llama_evcl_superni',\n",
    "    base_model_name: str = \"meta-llama/Llama-2-7b-hf\",\n",
    "    lora_model_path: str = 'fine-tuned-llama-lora',\n",
    "    batch_size: int = 8,\n",
    "    coreset_size: int = 200,\n",
    "    coreset_method: str = 'random',\n",
    "    ewc_lambda: float = 100.0,\n",
    "    ewc_gamma: float = 1.0,\n",
    "    task_folder='QA_FineTuned'\n",
    "):\n",
    "    os.chdir(f'/home/pranav24/cs-546/Iterative-SSR-and-EVCL-Catastrophic-Forgetting/{task_folder}/finetuned-weights')\n",
    "    \n",
    "    print(\"Loading base model...\")\n",
    "    # Load the model already fine-tuned on the first task\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "    print(\"Applying LoRA adapter...\")\n",
    "    model = PeftModel.from_pretrained(model, lora_model_path)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare the prior using the fine-tuned model\n",
    "    prior = MLEPrior(model)\n",
    "    obs = tyxe.likelihoods.Categorical(-1)\n",
    "    guide = functools.partial(\n",
    "        tyxe.guides.AutoNormal,\n",
    "        init_scale=1e-4,\n",
    "        init_loc_fn=tyxe.guides.PretrainedInitializer.from_net(model, prefix=\"net\")\n",
    "    )\n",
    "\n",
    "    # Initialize Bayesian model\n",
    "    bnn = VariationalBNNWithEWC(model, prior, obs, guide)\n",
    "\n",
    "    # Load the first task's data\n",
    "    train_loader_task1, test_loader_task1 = get_data_loader_for_task1(tokenizer, batch_size)\n",
    "\n",
    "    # Generate the initial coreset from the first task's data\n",
    "    prev_coreset = update_coreset(prev_coreset=[], train_loader=train_loader_task1, coreset_size=coreset_size, selection_method=coreset_method)\n",
    "\n",
    "    # Compute the initial Fisher Information Matrix and previous parameters\n",
    "    prev_fisher_info = compute_fisher_info_llm(\n",
    "        bnn, prev_fisher_info=None, data_loader=train_loader_task1, n_samples=5000, ewc_gamma=ewc_gamma\n",
    "    )\n",
    "    prev_params = {\n",
    "        name: param.detach().clone()\n",
    "        for name, param in bnn.named_parameters()\n",
    "        if 'lora' in name\n",
    "    }\n",
    "\n",
    "    # Now proceed with tasks 2 and onwards\n",
    "    # Prepare tasks 2 to num_tasks\n",
    "    train_loaders, test_loaders = fetch_nlp_datasets(tokenizer, batch_size, num_tasks, start_task=2)\n",
    "\n",
    "    for task_index, train_loader in enumerate(train_loaders, 2):  # Start from task_index=2\n",
    "        print(f\"Training on Task {task_index}...\")\n",
    "\n",
    "        # Update coreset\n",
    "        if coreset_size > 0:\n",
    "            curr_coreset = update_coreset(prev_coreset, train_loader, coreset_size, coreset_method)\n",
    "            # curr_coreset = update_coreset(prev_coreset, train_loader_task1, coreset_size, coreset_method)\n",
    "            # curr_coreset=prev_coreset\n",
    "        else:\n",
    "            curr_coreset = []\n",
    "\n",
    "        # Training loop for current task\n",
    "        def callback(epoch, step, loss):\n",
    "            print(f\"Epoch {epoch}, Step {step}, Loss: {loss}\")\n",
    "\n",
    "        # Fine-tune with variational inference and EWC\n",
    "        update_variational_approx(\n",
    "            bnn, train_loader, curr_coreset, num_epochs, callback, ewc_lambda,\n",
    "            fisher_info=prev_fisher_info, prev_params=prev_params\n",
    "        )\n",
    "\n",
    "        # Compute Fisher Information Matrix for current task\n",
    "        fisher_info = compute_fisher_info_llm(\n",
    "            bnn, prev_fisher_info, train_loader, n_samples=5000, ewc_gamma=ewc_gamma\n",
    "        )\n",
    "\n",
    "        # Update prev_params and prev_fisher_info\n",
    "        prev_params = {\n",
    "            name: param.detach().clone()\n",
    "            for name, param in bnn.named_parameters()\n",
    "            if 'lora' in name\n",
    "        }\n",
    "        prev_fisher_info = fisher_info\n",
    "\n",
    "        # Update prior with posterior from current task\n",
    "        site_names = [site for site in tyxe.util.pyro_sample_sites(bnn) if 'lora' in site]\n",
    "        params_to_update = tyxe.priors.DictPrior({\n",
    "            site: list(bnn.net_guide.get_detached_distributions(site).values())[0]\n",
    "            for site in site_names\n",
    "        })\n",
    "        bnn.update_prior(params_to_update)\n",
    "\n",
    "        # Update prev_coreset\n",
    "        prev_coreset = curr_coreset\n",
    "\n",
    "        # Evaluate on all tasks up to current\n",
    "        for j, test_loader in enumerate([test_loader_task1] + test_loaders[:task_index - 2], 1):\n",
    "            print(f\"Evaluating Task {j}...\")\n",
    "            total_loss = 0.0\n",
    "            num_batches = 0\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    outputs = bnn.net(input_ids, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f\"Task {j} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd853c25-4eab-40f1-a52f-54cb97f9406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_evcl(\n",
    "        num_tasks=2,  # QA and QG tasks\n",
    "        num_epochs=3,\n",
    "        experiment_name='llama_evcl_superni',\n",
    "        base_model_name='meta-llama/Llama-2-7b-hf',\n",
    "        lora_model_path='path/to/your/lora/model',\n",
    "        batch_size=8,\n",
    "        coreset_size=200,  # Adjust as needed\n",
    "        ewc_lambda=100.0,\n",
    "        ewc_gamma=1.0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b94c2a-6f14-4c26-92b0-da100fa4aba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a628a465-6ee6-4673-ba01-0591db3ab9ea",
   "metadata": {},
   "source": [
    "How to Run the Process with SuperNI Dataset\n",
    "\n",
    "Step 1: Environment Setup\n",
    "(Same as previously described)\n",
    "\n",
    "Step 2: Preparing the SuperNI Dataset\n",
    "Install the datasets Library:\n",
    "Ensure you have the datasets library installed:\n",
    "\n",
    "pip install datasets\n",
    "Inspect the SuperNI Dataset:\n",
    "The SuperNI dataset can be loaded using:\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "superni_dataset = load_dataset('super_nat_instruct', 'v1_1')\n",
    "Note: Replace 'super_nat_instruct' and 'v1_1' with the correct dataset identifier if necessary.\n",
    "Identify QA and QG Tasks:\n",
    "SuperNI contains multiple tasks with task descriptions.\n",
    "You need to identify the task IDs or names corresponding to QA and QG.\n",
    "You can print out the tasks to find the ones you need:\n",
    "for task in superni_dataset['train']['Task']:\n",
    "    print(task)\n",
    "Adjust the load_superni_task_dataset Function:\n",
    "Modify the task_filter in load_superni_task_dataset to match the task identifiers for QA and QG.\n",
    "For example:\n",
    "if task_type == 'qa':\n",
    "    task_ids = ['task_id_for_qa1', 'task_id_for_qa2']  # Replace with actual task IDs\n",
    "    task_filter = lambda ex: ex['TaskID'] in task_ids\n",
    "elif task_type == 'qg':\n",
    "    task_ids = ['task_id_for_qg1', 'task_id_for_qg2']  # Replace with actual task IDs\n",
    "    task_filter = lambda ex: ex['TaskID'] in task_ids\n",
    "Adjust Data Preprocessing:\n",
    "Ensure that the Input and Output fields are correctly used.\n",
    "For some tasks, you might need to concatenate context and question.\n",
    "Step 3: Running the Code\n",
    "(Same as previously described)\n",
    "\n",
    "Step 4: Monitoring and Evaluation\n",
    "(Same as previously described)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844525ac-9677-4cc2-846d-bae8ebb87667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
